{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld(object):\n",
    "    def __init__(self):\n",
    "        \n",
    "        ### Attributes defining the Gridworld #######\n",
    "        # Shape of the gridworld\n",
    "        self.shape = (5,5)\n",
    "        \n",
    "        # Locations of the obstacles\n",
    "        self.obstacle_locs = [(1,1),(2,1),(2,3)]\n",
    "        \n",
    "        # Locations for the absorbing states\n",
    "        self.absorbing_locs = [(4,0),(4,1),(4,2),(4,3),(4,4)]\n",
    "        \n",
    "        # Rewards for each of the absorbing states \n",
    "        self.special_rewards = [-10, -10, -10, -10, 10] #corresponds to each of the absorbing_locs\n",
    "        \n",
    "        # Reward for all the other states\n",
    "        self.default_reward = 0\n",
    "        \n",
    "        # Starting location\n",
    "        self.starting_loc = (3,0)\n",
    "        \n",
    "        # Action names\n",
    "        self.action_names = ['N','E','S','W']\n",
    "        \n",
    "        # Number of actions\n",
    "        self.action_size = len(self.action_names)\n",
    "        \n",
    "        \n",
    "        # Randomizing action results: [1 0 0 0] to no Noise in the action results.\n",
    "        self.action_randomizing_array = [0.8, 0.1, 0.0 , 0.1]\n",
    "        \n",
    "        ############################################\n",
    "    \n",
    "    \n",
    "    \n",
    "        #### Internal State  ####\n",
    "        \n",
    "    \n",
    "        # Get attributes defining the world\n",
    "        state_size, T, R, absorbing, locs = self.build_grid_world()\n",
    "        \n",
    "        # Number of valid states in the gridworld (there are 22 of them)\n",
    "        self.state_size = state_size\n",
    "        \n",
    "        # Transition operator (3D tensor)\n",
    "        self.T = T\n",
    "        \n",
    "        # Reward function (3D tensor)\n",
    "        self.R = R\n",
    "        \n",
    "        # Absorbing states\n",
    "        self.absorbing = absorbing\n",
    "        \n",
    "        # The locations of the valid states\n",
    "        self.locs = locs\n",
    "        \n",
    "        # Number of the starting state\n",
    "        self.starting_state = self.loc_to_state(self.starting_loc, locs);\n",
    "        \n",
    "        # Locating the initial state\n",
    "        self.initial = np.zeros((1,len(locs)))\n",
    "        self.initial[0,self.starting_state] = 1\n",
    "        \n",
    "        \n",
    "        # Placing the walls on a bitmap\n",
    "        self.walls = np.zeros(self.shape);\n",
    "        for ob in self.obstacle_locs:\n",
    "            self.walls[ob]=1\n",
    "            \n",
    "        # Placing the absorbers on a grid for illustration\n",
    "        self.absorbers = np.zeros(self.shape)\n",
    "        for ab in self.absorbing_locs:\n",
    "            self.absorbers[ab] = -1\n",
    "        \n",
    "        # Placing the rewarders on a grid for illustration\n",
    "        self.rewarders = np.zeros(self.shape)\n",
    "        for i, rew in enumerate(self.absorbing_locs):\n",
    "            self.rewarders[rew] = self.special_rewards[i]\n",
    "        \n",
    "        #Illustrating the grid world\n",
    "        self.paint_maps()\n",
    "        ################################\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ####### Getters ###########\n",
    "    \n",
    "    def get_transition_matrix(self):\n",
    "        return self.T\n",
    "    \n",
    "    def get_reward_matrix(self):\n",
    "        return self.R\n",
    "    \n",
    "    \n",
    "    ########################\n",
    "    \n",
    "    ####### Methods #########\n",
    "    def policy_evaluation(self, policy, threshold, discount):\n",
    "        \n",
    "        # Make sure delta is bigger than the threshold to start with\n",
    "        delta= 2*threshold\n",
    "        \n",
    "        #Get the reward and transition matrices\n",
    "        R = self.get_reward_matrix()\n",
    "        T = self.get_transition_matrix()\n",
    "        \n",
    "        # The value is initialised at 0\n",
    "        V = np.zeros(policy.shape[0])\n",
    "        # Make a deep copy of the value array to hold the update during the evaluation\n",
    "        Vnew = np.copy(V)\n",
    "        \n",
    "        # While the Value has not yet converged do:\n",
    "        while delta>threshold:\n",
    "            for state_idx in range(policy.shape[0]):\n",
    "                # If it is one of the absorbing states, ignore\n",
    "                if(self.absorbing[0,state_idx]):\n",
    "                    continue   \n",
    "                \n",
    "                # Accumulator variable for the Value of a state\n",
    "                tmpV = 0\n",
    "                for action_idx in range(policy.shape[1]):\n",
    "                    # Accumulator variable for the State-Action Value\n",
    "                    tmpQ = 0\n",
    "                    for state_idx_prime in range(policy.shape[0]):\n",
    "                        tmpQ = tmpQ + T[state_idx_prime,state_idx,action_idx] * (R[state_idx_prime,state_idx, action_idx] + discount * V[state_idx_prime])\n",
    "                    \n",
    "                    tmpV += policy[state_idx,action_idx] * tmpQ\n",
    "                    \n",
    "                # Update the value of the state\n",
    "                Vnew[state_idx] = tmpV\n",
    "            \n",
    "            # After updating the values of all states, update the delta\n",
    "            delta =  max(abs(Vnew-V))\n",
    "            # and save the new value into the old\n",
    "            V=np.copy(Vnew)\n",
    "            \n",
    "        return V\n",
    "    \n",
    "    def draw_deterministic_policy(self, Policy):\n",
    "        # Draw a deterministic policy\n",
    "        # The policy needs to be a np array of 22 values between 0 and 3 with\n",
    "        # 0 -> N, 1->E, 2->S, 3->W\n",
    "        plt.figure()\n",
    "        \n",
    "        plt.imshow(self.walls+self.rewarders +self.absorbers)\n",
    "        plt.hold('on')\n",
    "        for state, action in enumerate(Policy):\n",
    "            if(self.absorbing[0,state]):\n",
    "                continue\n",
    "            arrows = [r\"$\\uparrow$\",r\"$\\rightarrow$\", r\"$\\downarrow$\", r\"$\\leftarrow$\"]\n",
    "            action_arrow = arrows[action]\n",
    "            location = self.locs[state]\n",
    "            plt.text(location[1], location[0], action_arrow, ha='center', va='center')\n",
    "    \n",
    "        plt.show()\n",
    "    ##########################\n",
    "    \n",
    "    \n",
    "    ########### Internal Helper Functions #####################\n",
    "    def paint_maps(self):\n",
    "        plt.figure()\n",
    "        plt.subplot(1,3,1)\n",
    "        plt.imshow(self.walls)\n",
    "        plt.subplot(1,3,2)\n",
    "        plt.imshow(self.absorbers)\n",
    "        plt.subplot(1,3,3)\n",
    "        plt.imshow(self.rewarders)\n",
    "        plt.show()\n",
    "        \n",
    "    def build_grid_world(self):\n",
    "        # Get the locations of all the valid states, the neighbours of each state (by state number),\n",
    "        # and the absorbing states (array of 0's with ones in the absorbing states)\n",
    "        locations, neighbours, absorbing = self.get_topology()\n",
    "        \n",
    "        # Get the number of states\n",
    "        S = len(locations)\n",
    "        \n",
    "        # Initialise the transition matrix\n",
    "        T = np.zeros((S,S,4))\n",
    "        \n",
    "        for action in range(4):\n",
    "            for effect in range(4):\n",
    "                \n",
    "                # Randomize the outcome of taking an action\n",
    "                outcome = (action+effect+1) % 4\n",
    "                if outcome == 0:\n",
    "                    outcome = 3\n",
    "                else:\n",
    "                    outcome -= 1\n",
    "    \n",
    "                # Fill the transition matrix\n",
    "                prob = self.action_randomizing_array[effect]\n",
    "                for prior_state in range(S):\n",
    "                    post_state = neighbours[prior_state, outcome]\n",
    "                    post_state = int(post_state)\n",
    "                    T[post_state,prior_state,action] = T[post_state,prior_state,action]+prob\n",
    "                    \n",
    "    \n",
    "        # Build the reward matrix\n",
    "        R = self.default_reward*np.ones((S,S,4))\n",
    "        for i, sr in enumerate(self.special_rewards):\n",
    "            post_state = self.loc_to_state(self.absorbing_locs[i],locations)\n",
    "            R[post_state,:,:]= sr\n",
    "        \n",
    "        return S, T,R,absorbing,locations\n",
    "    \n",
    "    def get_topology(self):\n",
    "        height = self.shape[0]\n",
    "        width = self.shape[1]\n",
    "        \n",
    "        index = 1 \n",
    "        locs = []\n",
    "        neighbour_locs = []\n",
    "        \n",
    "        for i in range(height):\n",
    "            for j in range(width):\n",
    "                # Get the locaiton of each state\n",
    "                loc = (i,j)\n",
    "                \n",
    "                #And append it to the valid state locations if it is a valid state (ie not absorbing)\n",
    "                if(self.is_location(loc)):\n",
    "                    locs.append(loc)\n",
    "                    \n",
    "                    # Get an array with the neighbours of each state, in terms of locations\n",
    "                    local_neighbours = [self.get_neighbour(loc,direction) for direction in ['nr','ea','so', 'we']]\n",
    "                    neighbour_locs.append(local_neighbours)\n",
    "                \n",
    "        # translate neighbour lists from locations to states\n",
    "        num_states = len(locs)\n",
    "        state_neighbours = np.zeros((num_states,4))\n",
    "        \n",
    "        for state in range(num_states):\n",
    "            for direction in range(4):\n",
    "                # Find neighbour location\n",
    "                nloc = neighbour_locs[state][direction]\n",
    "                \n",
    "                # Turn location into a state number\n",
    "                nstate = self.loc_to_state(nloc,locs)\n",
    "      \n",
    "                # Insert into neighbour matrix\n",
    "                state_neighbours[state,direction] = nstate;\n",
    "                \n",
    "    \n",
    "        # Translate absorbing locations into absorbing state indices\n",
    "        absorbing = np.zeros((1,num_states))\n",
    "        for a in self.absorbing_locs:\n",
    "            absorbing_state = self.loc_to_state(a,locs)\n",
    "            absorbing[0,absorbing_state] =1\n",
    "        \n",
    "        return locs, state_neighbours, absorbing \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def loc_to_state(self,loc,locs):\n",
    "        #takes list of locations and gives index corresponding to input loc\n",
    "        return locs.index(tuple(loc))\n",
    "\n",
    "\n",
    "    def is_location(self, loc):\n",
    "        # It is a valid location if it is in grid and not obstacle\n",
    "        if(loc[0]<0 or loc[1]<0 or loc[0]>self.shape[0]-1 or loc[1]>self.shape[1]-1):\n",
    "            return False\n",
    "        elif(loc in self.obstacle_locs):\n",
    "            return False\n",
    "        else:\n",
    "             return True\n",
    "            \n",
    "    def get_neighbour(self,loc,direction):\n",
    "        #Find the valid neighbours (ie that are in the grif and not obstacle)\n",
    "        i = loc[0]\n",
    "        j = loc[1]\n",
    "        \n",
    "        nr = (i-1,j)\n",
    "        ea = (i,j+1)\n",
    "        so = (i+1,j)\n",
    "        we = (i,j-1)\n",
    "        \n",
    "        # If the neighbour is a valid location, accept it, otherwise, stay put\n",
    "        if(direction == 'nr' and self.is_location(nr)):\n",
    "            return nr\n",
    "        elif(direction == 'ea' and self.is_location(ea)):\n",
    "            return ea\n",
    "        elif(direction == 'so' and self.is_location(so)):\n",
    "            return so\n",
    "        elif(direction == 'we' and self.is_location(we)):\n",
    "            return we\n",
    "        else:\n",
    "            #default is to return to the same location\n",
    "            return loc\n",
    "        \n",
    "###########################################         \n",
    "    \n",
    "                \n",
    "                        \n",
    "                \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Policy is : [[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "The value of that policy is : 0\n"
     ]
    }
   ],
   "source": [
    "grid = GridWorld()\n",
    "\n",
    "### Question 1 : Change the policy here:\n",
    "Policy= np.zeros((grid.state_size, grid.action_size))\n",
    "print(\"The Policy is : {}\".format(Policy))\n",
    "\n",
    "val = 0 #Change here!\n",
    "print(\"The value of that policy is : {}\".format(val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/ipykernel_launcher.py:149: MatplotlibDeprecationWarning: pyplot.hold is deprecated.\n",
      "    Future behavior will be consistent with the long-time default:\n",
      "    plot commands add elements without first clearing the\n",
      "    Axes and/or Figure.\n",
      "/usr/local/lib/python2.7/site-packages/matplotlib/__init__.py:910: MatplotlibDeprecationWarning: axes.hold is deprecated. Please remove it from your matplotlibrc and/or style files.\n",
      "  mplDeprecation)\n",
      "/usr/local/lib/python2.7/site-packages/matplotlib/rcsetup.py:156: MatplotlibDeprecationWarning: axes.hold is deprecated, will be removed in 3.0\n",
      "  mplDeprecation)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Using draw_deterministic_policy to illustrate some arbitracy policy.\n",
    "Policy2 = np.zeros(22).astype(int)\n",
    "Policy2[2] = 3\n",
    "Policy2[6] = 2\n",
    "Policy2[18] = 1\n",
    "grid.draw_deterministic_policy(Policy2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
