{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning\n",
    "\n",
    "\n",
    "- Q-Learning is an algorithm which attempts to learn a function or policy which takes an observation of the environment as input and returns an action as output. \n",
    "- Q-Learning does this by determining which action is best in the current state as well as all future states. \n",
    "- We call this function the action value function or Q(a,s), where Q is the value of taking action a in state s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discount=1\n",
    "learning = 0.01\n",
    "epsilon = 0.05\n",
    "\n",
    "\n",
    "def QLearning(env, learning, discount, epsilon, min_eps, episodes):\n",
    "    # Determine size of discretized state space\n",
    "    num_states = (env.observation_space.high - env.observation_space.low)*\\\n",
    "                    np.array([10, 100])\n",
    "    num_states = np.round(num_states, 0).astype(int) + 1\n",
    "    \n",
    "    \n",
    "    #num_states = 10\n",
    "    \n",
    "    \n",
    "    # Initialize Q table\n",
    "    Q = np.random.uniform(low = -1, high = 1, \n",
    "                          size = (num_states[0], num_states[1], \n",
    "                                  env.action_space.n))\n",
    "    \n",
    "    # Initialize variables to track rewards\n",
    "    reward_list = []\n",
    "    ave_reward_list = []\n",
    "    \n",
    "    # Calculate episodic reduction in epsilon\n",
    "    reduction = (epsilon - min_eps)/episodes\n",
    "    \n",
    "    # Run Q learning algorithm\n",
    "    for i in range(episodes):\n",
    "        # Initialize parameters\n",
    "        done = False\n",
    "        tot_reward, reward = 0,0\n",
    "        state = env.reset()\n",
    "        \n",
    "        # Discretize state\n",
    "        state_adj = (state - env.observation_space.low)*np.array([10, 100])\n",
    "        state_adj = np.round(state_adj, 0).astype(int)\n",
    "    \n",
    "        while done != True:   \n",
    "            # Render environment for last five episodes\n",
    "            if i >= (episodes - 20):\n",
    "                env.render()\n",
    "                \n",
    "            # Determine next action - epsilon greedy strategy\n",
    "            if np.random.random() < 1 - epsilon:\n",
    "                action = np.argmax(Q[state_adj[0], state_adj[1]]) \n",
    "            else:\n",
    "                action = np.random.randint(0, env.action_space.n)\n",
    "                \n",
    "            # Get next state and reward\n",
    "            state2, reward, done, info = env.step(action) \n",
    "            \n",
    "            # Discretize state2\n",
    "            state2_adj = (state2 - env.observation_space.low)*np.array([10, 100])\n",
    "            state2_adj = np.round(state2_adj, 0).astype(int)\n",
    "            \n",
    "            #Allow for terminal states\n",
    "            if done and state2[0] >= 0.5:\n",
    "                Q[state_adj[0], state_adj[1], action] = reward\n",
    "                \n",
    "            # Adjust Q value for current state\n",
    "            else:\n",
    "                delta = learning*(reward + \n",
    "                                 discount*np.max(Q[state2_adj[0], \n",
    "                                                   state2_adj[1]]) - \n",
    "                                 Q[state_adj[0], state_adj[1],action])\n",
    "                Q[state_adj[0], state_adj[1],action] += delta\n",
    "                                     \n",
    "            # Update variables\n",
    "            tot_reward += reward\n",
    "            state_adj = state2_adj\n",
    "        \n",
    "        # Decay epsilon\n",
    "        if epsilon > min_eps:\n",
    "            epsilon -= reduction\n",
    "        \n",
    "        # Track rewards\n",
    "        reward_list.append(tot_reward)\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            ave_reward = np.mean(reward_list)\n",
    "            ave_reward_list.append(ave_reward)\n",
    "            reward_list = []\n",
    "            \n",
    "        if (i+1) % 100 == 0:    \n",
    "            print('Episode {} Average Reward: {}'.format(i+1, ave_reward))\n",
    "            \n",
    "    env.close()\n",
    "    \n",
    "    return ave_reward_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "\"\"\"\n",
    "Qlearning is an off policy learning python implementation.\n",
    "This is a python implementation of the qlearning algorithm in the Sutton and\n",
    "Barto's book on RL. It's called SARSA because - (state, action, reward, state,\n",
    "action). The only difference between SARSA and Qlearning is that SARSA takes the\n",
    "next action based on the current policy while qlearning takes the action with\n",
    "maximum utility of next state.\n",
    "Using the simplest gym environment for brevity: https://gym.openai.com/envs/FrozenLake-v0/\n",
    "\"\"\"\n",
    "\n",
    "def init_q(s, a, type=\"ones\"):\n",
    "    \"\"\"\n",
    "    @param s the number of states\n",
    "    @param a the number of actions\n",
    "    @param type random, ones or zeros for the initialization\n",
    "    \"\"\"\n",
    "    if type == \"ones\":\n",
    "        return np.ones((s, a))\n",
    "    elif type == \"random\":\n",
    "        return np.random.random((s, a))\n",
    "    elif type == \"zeros\":\n",
    "        return np.zeros((s, a))\n",
    "\n",
    "\n",
    "def epsilon_greedy(Q, epsilon, n_actions, s, train=False):\n",
    "    \"\"\"\n",
    "    @param Q Q values state x action -> value\n",
    "    @param epsilon for exploration\n",
    "    @param s number of states\n",
    "    @param train if true then no random actions selected\n",
    "    \"\"\"\n",
    "    if train or np.random.rand() < epsilon:\n",
    "        action = np.argmax(Q[s, :])\n",
    "    else:\n",
    "        action = np.random.randint(0, n_actions)\n",
    "    return action\n",
    "\n",
    "def qlearning(alpha, gamma, epsilon, episodes, max_steps, n_tests, render = False, test=False):\n",
    "    \"\"\"\n",
    "    @param alpha learning rate\n",
    "    @param gamma decay factor\n",
    "    @param epsilon for exploration\n",
    "    @param max_steps for max step in each episode\n",
    "    @param n_tests number of test episodes\n",
    "    \"\"\"\n",
    "    env = gym.make('Taxi-v2')\n",
    "    n_states, n_actions = env.observation_space.n, env.action_space.n\n",
    "    Q = init_q(n_states, n_actions, type=\"ones\")\n",
    "    timestep_reward = []\n",
    "    for episode in range(episodes):\n",
    "        print(f\"Episode: {episode}\")\n",
    "        s = env.reset()\n",
    "        a = epsilon_greedy(Q, epsilon, n_actions, s)\n",
    "        t = 0\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        while t < max_steps:\n",
    "            if render:\n",
    "                env.render()\n",
    "            t += 1\n",
    "            s_, reward, done, info = env.step(a)\n",
    "            total_reward += reward\n",
    "            a_ = np.argmax(Q[s_, :])\n",
    "            if done:\n",
    "                Q[s, a] += alpha * ( reward  - Q[s, a] )\n",
    "            else:\n",
    "                Q[s, a] += alpha * ( reward + (gamma * Q[s_, a_]) - Q[s, a] )\n",
    "            s, a = s_, a_\n",
    "            if done:\n",
    "                if render:\n",
    "                    print(f\"This episode took {t} timesteps and reward: {total_reward}\")\n",
    "                timestep_reward.append(total_reward)\n",
    "                break\n",
    "    if render:\n",
    "        print(f\"Here are the Q values:\\n{Q}\\nTesting now:\")\n",
    "    if test:\n",
    "        test_agent(Q, env, n_tests, n_actions)\n",
    "    return timestep_reward\n",
    "\n",
    "def test_agent(Q, env, n_tests, n_actions, delay=1):\n",
    "    for test in range(n_tests):\n",
    "        print(f\"Test #{test}\")\n",
    "        s = env.reset()\n",
    "        done = False\n",
    "        epsilon = 0\n",
    "        while True:\n",
    "            time.sleep(delay)\n",
    "            env.render()\n",
    "            a = epsilon_greedy(Q, epsilon, n_actions, s, train=True)\n",
    "            print(f\"Chose action {a} for state {s}\")\n",
    "            s, reward, done, info = env.step(a)\n",
    "            if done:\n",
    "                if reward > 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import cm\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Easy21:\n",
    "\n",
    "    def __init__(self, max_length=1000):\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def reset(self):\n",
    "        self.player_first_card_val = np.random.choice(10) + 1\n",
    "        self.dealer_first_card_val = np.random.choice(10) + 1\n",
    "\n",
    "        self.player_sum = self.player_first_card_val\n",
    "        self.dealer_sum = self.dealer_first_card_val\n",
    "\n",
    "        self.state = [self.dealer_first_card_val, self.player_sum]\n",
    "\n",
    "        self.player_goes_bust = False\n",
    "        self.dealer_goes_bust = False\n",
    "\n",
    "        self.ret = 0\n",
    "        self.terminal = False\n",
    "        self.t = 0\n",
    "\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action):\n",
    "        # action 1: hit   0: stick\n",
    "        # color: 1: black   -1: red\n",
    "        r = 0\n",
    "\n",
    "        ### Player hits\n",
    "        if action == 1 and self.terminal == False:\n",
    "            self.player_card_val = np.random.choice(10) + 1\n",
    "            self.player_card_col = np.random.choice([-1, 1], p=[1./3., 2./3.])\n",
    "\n",
    "            self.player_sum += (self.player_card_val * self.player_card_col)\n",
    "            self.player_goes_bust = self.check_go_bust(self.player_sum)\n",
    "            \n",
    "            if self.player_goes_bust == 1:\n",
    "                    r = -1\n",
    "                    self.terminal = True\n",
    "            \n",
    "            self.t+=1\n",
    "        \n",
    "        ### Player sticks\n",
    "        else:\n",
    "            ##Dealer hits until sum >=17\n",
    "            while (not self.terminal) and self.dealer_sum < 17 and self.t <= self.max_length:\n",
    "                self.dealer_card_val = np.random.choice(10) + 1\n",
    "                self.dealer_card_col = np.random.choice([-1, 1], p=[1./3., 2./3.])\n",
    "\n",
    "                self.dealer_sum += (self.dealer_card_val * self.dealer_card_col)\n",
    "                self.dealer_goes_bust = self.check_go_bust(self.dealer_sum)\n",
    "            \n",
    "                ## Dealer goes bust\n",
    "                if self.dealer_goes_bust == 1:\n",
    "                    r = 1\n",
    "                    self.terminal = True\n",
    "                    break\n",
    "                    \n",
    "                self.t+=1\n",
    "                \n",
    "            if not self.terminal:\n",
    "                \n",
    "                if self.player_sum > self.dealer_sum: \n",
    "                    r = 1\n",
    "                    self.terminal = True\n",
    "                elif self.player_sum < self.dealer_sum: \n",
    "                    r = -1\n",
    "                    self.terminal = True\n",
    "                else:  \n",
    "                    r = 0\n",
    "                    self.terminal = True\n",
    "\n",
    "            \n",
    "        if self.terminal: return 'Terminal', r, self.terminal\n",
    "        else:\n",
    "            self.state[1] = self.player_sum\n",
    "            return self.state, r, self.terminal\n",
    "\n",
    "\n",
    "    def check_go_bust(self, Sum):\n",
    "        return bool(Sum > 21 or Sum < 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
