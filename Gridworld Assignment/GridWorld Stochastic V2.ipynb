{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters\n",
    "\n",
    "p = 0.3\n",
    "gamma = 0.3\n",
    "No noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hypothesis: no attribute pi, it is simply included in the transiition matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld(object):\n",
    "    def __init__(self):\n",
    "        \n",
    "        ### Attributes defining the Gridworld #######\n",
    "        \n",
    "        # Shape of the gridworld\n",
    "        self.shape = (4,4)\n",
    "        \n",
    "        # Locations of the obstacles\n",
    "        self.obstacle_locs = [(1,2),(2,0),(3,0),(3,1),(3,3)]\n",
    "        \n",
    "        # Locations for the absorbing states\n",
    "        self.absorbing_locs = [(0,1),(3,2)]\n",
    "        \n",
    "        # Rewards for each of the absorbing states \n",
    "        self.special_rewards = [10, -100] #corresponds to each of the absorbing_locs\n",
    "        \n",
    "        # Reward for all the other states\n",
    "        self.default_reward = -1\n",
    "        \n",
    "        # Starting location\n",
    "        self.starting_loc = (0,0)\n",
    "        \n",
    "        # Action names\n",
    "        self.action_names = ['N','E','S','W']\n",
    "        \n",
    "        # Number of actions\n",
    "        self.action_size = len(self.action_names)\n",
    "        \n",
    "        \n",
    "        # Randomizing action results: [1 0 0 0] to no Noise in the action results.\n",
    "        self.action_randomizing_array = [1, 0, 0 , 0]\n",
    "        \n",
    "        ############################################\n",
    "    \n",
    "    \n",
    "    \n",
    "        #### Internal State  ####\n",
    "        \n",
    "    \n",
    "        # Get attributes defining the world\n",
    "        state_size, T, R, absorbing, locs = self.build_grid_world()\n",
    "        \n",
    "        # Number of valid states in the gridworld (there are 11 of them)\n",
    "        self.state_size = state_size\n",
    "        \n",
    "        # Transition operator (3D tensor)\n",
    "        self.T = T\n",
    "        \n",
    "        # Reward function (3D tensor)\n",
    "        self.R = R\n",
    "        \n",
    "        \n",
    "        # Probability function for stochasticity (2D tensor)\n",
    "        #self.pi = pi\n",
    "        \n",
    "        # Absorbing states\n",
    "        self.absorbing = absorbing\n",
    "        \n",
    "        # The locations of the valid states\n",
    "        self.locs = locs\n",
    "        \n",
    "        # Number of the starting state\n",
    "        self.starting_state = self.loc_to_state(self.starting_loc, locs);\n",
    "        \n",
    "        # Locating the initial state\n",
    "        self.initial = np.zeros((1,len(locs)));\n",
    "        self.initial[0,self.starting_state] = 1\n",
    "        \n",
    "        \n",
    "        # Placing the walls on a bitmap\n",
    "        self.walls = np.zeros(self.shape);\n",
    "        for ob in self.obstacle_locs:\n",
    "            self.walls[ob]=1\n",
    "            \n",
    "        # Placing the absorbers on a grid for illustration\n",
    "        self.absorbers = np.zeros(self.shape)\n",
    "        for ab in self.absorbing_locs:\n",
    "            self.absorbers[ab] = -1\n",
    "        \n",
    "        # Placing the rewarders on a grid for illustration\n",
    "        self.rewarders = np.zeros(self.shape)\n",
    "        for i, rew in enumerate(self.absorbing_locs):\n",
    "            self.rewarders[rew] = self.special_rewards[i]\n",
    "        \n",
    "        #Illustrating the grid world\n",
    "        self.paint_maps()\n",
    "        ################################\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ####### Getters ###########\n",
    "    \n",
    "    def get_transition_matrix(self):\n",
    "        return self.T\n",
    "    \n",
    "    def get_reward_matrix(self):\n",
    "        return self.R\n",
    "    \n",
    "   # def get_probability_matrix(self):\n",
    "    #    return self.pi\n",
    "    \n",
    "    \n",
    "    ########################\n",
    "    \n",
    "    ####### Methods #########\n",
    "    \n",
    "    \n",
    "    def value_iteration(self, discount = 0.3, threshold = 0.0001):\n",
    "        V = np.zeros(self.state_size)\n",
    "        \n",
    "        T = self.get_transition_matrix()\n",
    "        R = self.get_reward_matrix()\n",
    "        \n",
    "        ####\n",
    "       # pi = self.get_probability_matrix()\n",
    "        \n",
    "        epochs = 0\n",
    "        while True:\n",
    "            epochs+=1\n",
    "            delta = 0\n",
    "\n",
    "            for state_idx in range(self.state_size):\n",
    "                if(self.absorbing[0,state_idx]):\n",
    "                    continue \n",
    "                    \n",
    "                v = V[state_idx]\n",
    "\n",
    "                Q = np.zeros(4)\n",
    "                for state_idx_prime in range(self.state_size):\n",
    "                    Q +=  T[state_idx_prime,state_idx,:] * (R[state_idx_prime,state_idx, :] + discount * V[state_idx_prime])\n",
    "                 \n",
    "                    \n",
    "                V[state_idx]= np.max(Q)\n",
    "                delta = max(delta,np.abs(v - V[state_idx]))\n",
    "                \n",
    "            if(delta<threshold):\n",
    "                optimal_policy = np.zeros((self.state_size, self.action_size))\n",
    "                for state_idx in range(self.state_size):\n",
    "                    Q = np.zeros(4)\n",
    "                    for state_idx_prime in range(self.state_size):\n",
    "                        Q += T[state_idx_prime,state_idx,:] * (R[state_idx_prime,state_idx, :] + discount * V[state_idx_prime])\n",
    "                    \n",
    "                    optimal_policy[state_idx, np.argmax(Q)]=1\n",
    "\n",
    "                \n",
    "                return optimal_policy,epochs\n",
    "\n",
    "\n",
    "    \n",
    "    def policy_iteration(self, discount=0.3, threshold = 0.0001):\n",
    "        policy= np.zeros((self.state_size, self.action_size))\n",
    "        policy[:,0] = 1\n",
    "        \n",
    "        T = self.get_transition_matrix()\n",
    "        R = self.get_reward_matrix()\n",
    "        \n",
    "        ########\n",
    "        #pi = self.get_probability_matrix()\n",
    "        \n",
    "        epochs =0\n",
    "        while True: \n",
    "            V, epochs_eval = self.policy_evaluation(policy, threshold, discount)\n",
    "            \n",
    "            epochs+=epochs_eval\n",
    "            #Policy iteration\n",
    "            policy_stable = True\n",
    "            \n",
    "            for state_idx in range(policy.shape[0]):\n",
    "                if(self.absorbing[0,state_idx]):\n",
    "                    continue \n",
    "                    \n",
    "                old_action = np.argmax(policy[state_idx,:])\n",
    "                \n",
    "                Q = np.zeros(4)\n",
    "                for state_idx_prime in range(policy.shape[0]):\n",
    "                    Q += T[state_idx_prime,state_idx,:] * (R[state_idx_prime,state_idx, :] + discount * V[state_idx_prime])\n",
    "                   \n",
    "                ####### Added P, 2D vector consisting of pi(s,a)\n",
    "                \n",
    "                new_policy = np.zeros(4)\n",
    "                new_policy[np.argmax(Q)]=1\n",
    "                policy[state_idx] = new_policy\n",
    "                \n",
    "                if(old_action !=np.argmax(policy[state_idx])):\n",
    "                    policy_stable = False\n",
    "            \n",
    "            if(policy_stable):\n",
    "                return V, policy,epochs\n",
    "                \n",
    "                \n",
    "                \n",
    "        \n",
    "    \n",
    "    def policy_evaluation(self, policy, threshold, discount):\n",
    "        \n",
    "        # Make sure delta is bigger than the threshold to start with\n",
    "        delta= 2*threshold\n",
    "        \n",
    "        #Get the reward and transition matrices\n",
    "        R = self.get_reward_matrix()\n",
    "        T = self.get_transition_matrix()\n",
    "        \n",
    "        \n",
    "        ###\n",
    "        #pi = self.get_probability_matrix()\n",
    "\n",
    "        \n",
    "        # The value is initialised at 0\n",
    "        V = np.zeros(policy.shape[0])\n",
    "        # Make a deep copy of the value array to hold the update during the evaluation\n",
    "        Vnew = np.copy(V)\n",
    "        \n",
    "        epoch = 0\n",
    "        # While the Value has not yet converged do:\n",
    "        while delta>threshold:\n",
    "            epoch += 1\n",
    "            for state_idx in range(policy.shape[0]):\n",
    "                # If it is one of the absorbing states, ignore\n",
    "                if(self.absorbing[0,state_idx]):\n",
    "                    continue   \n",
    "                \n",
    "                # Accumulator variable for the Value of a state\n",
    "                tmpV = 0\n",
    "                for action_idx in range(policy.shape[1]):\n",
    "                    # Accumulator variable for the State-Action Value\n",
    "                    tmpQ = 0\n",
    "                    for state_idx_prime in range(policy.shape[0]):\n",
    "                        tmpQ = tmpQ + T[state_idx_prime,state_idx,action_idx] * (R[state_idx_prime,state_idx, action_idx] + discount * V[state_idx_prime])\n",
    "                    \n",
    "                    tmpV += policy[state_idx,action_idx] * tmpQ\n",
    "                    \n",
    "                # Update the value of the state\n",
    "                Vnew[state_idx] = tmpV\n",
    "            \n",
    "            # After updating the values of all states, update the delta\n",
    "            delta =  max(abs(Vnew-V))\n",
    "            # and save the new value into the old\n",
    "            V=np.copy(Vnew)\n",
    "            \n",
    "        return V, epoch\n",
    "    \n",
    "    def draw_deterministic_policy(self, Policy):\n",
    "        # Draw a deterministic policy\n",
    "        # The policy needs to be a np array of 11 values between 0 and 3 with\n",
    "        # 0 -> N, 1->E, 2->S, 3->W\n",
    "        plt.figure()\n",
    "        plt.imshow(self.walls+ self.rewarders + self.absorbers)\n",
    "        plt.imshow(self.walls)\n",
    "        \n",
    "        #plt.hold('on')\n",
    "        for state, action in enumerate(Policy):\n",
    "            if(self.absorbing[0,state]):\n",
    "                continue\n",
    "            arrows = [r\"$\\uparrow$\",r\"$\\rightarrow$\", r\"$\\downarrow$\", r\"$\\leftarrow$\"]\n",
    "            action_arrow = arrows[action]\n",
    "            location = self.locs[state]\n",
    "            plt.text(location[1], location[0], action_arrow, ha='center', va='center', color='b', size='50')\n",
    "    \n",
    "        plt.show()\n",
    "    ##########################\n",
    "    \n",
    "    \n",
    "    ########### Internal Helper Functions #####################\n",
    "    def paint_maps(self):\n",
    "        plt.figure()\n",
    "        plt.subplot(1,3,1)\n",
    "        plt.imshow(self.walls)\n",
    "        plt.subplot(1,3,2)\n",
    "        plt.imshow(self.absorbers)\n",
    "        plt.subplot(1,3,3)\n",
    "        plt.imshow(self.rewarders) \n",
    "        plt.show()\n",
    "        \n",
    "    def build_grid_world(self):\n",
    "        # Get the locations of all the valid states, the neighbours of each state (by state number),\n",
    "        # and the absorbing states (array of 0's with ones in the absorbing states)\n",
    "        locations, neighbours, absorbing_states = self.get_topology()\n",
    "        \n",
    "        # Get the number of states\n",
    "        S = len(locations)\n",
    "        \n",
    "        # Initialise the transition matrix\n",
    "        T = np.zeros((S,S,4))\n",
    "        \n",
    "        ## Initialise the probability matrix\n",
    "       # pi = np.zeros((S,4))  ### add\n",
    "        \n",
    "        ##probability matrix is based on action-state pairs\n",
    "        ##if the action and outcome is the samethen p=0.3\n",
    "        p_constant = 0.3\n",
    "        \n",
    "        for action in range(4):\n",
    "            for effect in range(4):\n",
    "                \n",
    "                # Randomize the outcome of taking an action. is it random tho......\n",
    "                \n",
    "                # outcome = \n",
    "                \n",
    "                outcome = (action+effect+1) % 4\n",
    "                if outcome == 0:\n",
    "                    outcome = 3\n",
    "                else:\n",
    "                    outcome -= 1\n",
    "                    \n",
    "                    ##establish the probability is stochastic and different for each effect/action\n",
    "                if action == outcome : \n",
    "                    prob = p_constant     \n",
    "                else:\n",
    "                    prob = (1-p_constant)/3\n",
    "                    \n",
    "                # Fill the transition matrix\n",
    "               # prob = self.action_randomizing_array[effect]\n",
    "                for prior_state in range(S):\n",
    "                    post_state = neighbours[prior_state, outcome]\n",
    "                    post_state = int(post_state)\n",
    "                    \n",
    "                    T[post_state,prior_state,action] = T[post_state,prior_state,action]+prob\n",
    "                \n",
    "    \n",
    "        # Build the reward matrix\n",
    "        R = self.default_reward*np.ones((S,S,4))\n",
    "        for i, sr in enumerate(self.special_rewards):\n",
    "            post_state = self.loc_to_state(self.absorbing_locs[i],locations)\n",
    "            R[post_state,:,:]= sr\n",
    "        \n",
    "        return S, T, R, absorbing_states,locations\n",
    "    \n",
    "    def get_topology(self):\n",
    "        height = self.shape[0]\n",
    "        width = self.shape[1]\n",
    "        \n",
    "        index = 1 \n",
    "        locs = []\n",
    "        neighbour_locs = []\n",
    "        \n",
    "        for i in range(height):\n",
    "            for j in range(width):\n",
    "                # Get the locaiton of each state\n",
    "                loc = (i,j)\n",
    "                \n",
    "                #And append it to the valid state locations if it is a valid state (ie not absorbing)\n",
    "                if(self.is_location(loc)):\n",
    "                    locs.append(loc)\n",
    "                    \n",
    "                    # Get an array with the neighbours of each state, in terms of locations\n",
    "                    local_neighbours = [self.get_neighbour(loc,direction) for direction in ['nr','ea','so', 'we']]\n",
    "                    neighbour_locs.append(local_neighbours)\n",
    "                \n",
    "        # translate neighbour lists from locations to states\n",
    "        num_states = len(locs)\n",
    "        state_neighbours = np.zeros((num_states,4))\n",
    "        \n",
    "        for state in range(num_states):\n",
    "            for direction in range(4):\n",
    "                # Find neighbour location\n",
    "                nloc = neighbour_locs[state][direction]\n",
    "                \n",
    "                # Turn location into a state number\n",
    "                nstate = self.loc_to_state(nloc,locs)\n",
    "      \n",
    "                # Insert into neighbour matrix\n",
    "                state_neighbours[state,direction] = nstate;\n",
    "                \n",
    "    \n",
    "        # Translate absorbing locations into absorbing state indices\n",
    "        absorbing = np.zeros((1,num_states))\n",
    "        for a in self.absorbing_locs:\n",
    "            absorbing_state = self.loc_to_state(a,locs)\n",
    "            absorbing[0,absorbing_state] =1\n",
    "        \n",
    "        return locs, state_neighbours, absorbing \n",
    "\n",
    "    def loc_to_state(self,loc,locs):\n",
    "        #takes list of locations and gives index corresponding to input loc\n",
    "        return locs.index(tuple(loc))\n",
    "        #return locs.index(loc)\n",
    "\n",
    "\n",
    "    def is_location(self, loc):\n",
    "        # It is a valid location if it is in grid and not obstacle\n",
    "        if(loc[0]<0 or loc[1]<0 or loc[0]>self.shape[0]-1 or loc[1]>self.shape[1]-1):\n",
    "            return False\n",
    "        elif(loc in self.obstacle_locs):\n",
    "            return False\n",
    "        else:\n",
    "             return True\n",
    "            \n",
    "    def get_neighbour(self,loc,direction):\n",
    "        #Find the valid neighbours (ie that are in the grif and not obstacle)\n",
    "        i = loc[0]\n",
    "        j = loc[1]\n",
    "        \n",
    "        nr = (i-1,j)\n",
    "        ea = (i,j+1)\n",
    "        so = (i+1,j)\n",
    "        we = (i,j-1)\n",
    "        \n",
    "        # If the neighbour is a valid location, accept it, otherwise, stay put\n",
    "        if(direction == 'nr' and self.is_location(nr)):\n",
    "            return nr\n",
    "        elif(direction == 'ea' and self.is_location(ea)):\n",
    "            return ea\n",
    "        elif(direction == 'so' and self.is_location(so)):\n",
    "            return so\n",
    "        elif(direction == 'we' and self.is_location(we)):\n",
    "            return we\n",
    "        else:\n",
    "            #default is to return to the same location\n",
    "            return loc\n",
    "        \n",
    "###########################################         \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Policy is : [[0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]]\n",
      "The value of that policy is :[  1.98241877   0.           1.95774192  -1.14613637  -0.86615882\n",
      "   1.53388853  -1.6151844   -3.55004181 -28.4359267   -3.82785356\n",
      "   0.        ]\n",
      "It took 8 epochs\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The value of the optimal policy using policy iteration is [  2.616552     0.           2.59156483  -1.01794795  -0.71094248\n",
      "   2.18711656  -1.55258298  -3.08882947 -26.4748415   -3.48018993\n",
      "   0.        ]:\n",
      "The optimal policy using policy iteration is [[0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "The number of epochs for convergence are 30\n",
      "The optimal policy using value iteration is [[0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "The number of epocs for convergence is 8\n"
     ]
    }
   ],
   "source": [
    "grid = GridWorld()\n",
    "\n",
    "### Question 1 : Change the policy here:\n",
    "Policy= np.zeros((grid.state_size, grid.action_size))\n",
    "Policy = Policy + 0.25\n",
    "print(\"The Policy is : {}\".format(Policy))\n",
    "\n",
    "val, epochs = grid.policy_evaluation(Policy,0.001,0.3)\n",
    "print(\"The value of that policy is :{}\".format(val))\n",
    "print(\"It took {} epochs\".format(epochs))\n",
    "\n",
    "\n",
    "gamma_range = [0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]\n",
    "epochs_needed = []\n",
    "for gamma in gamma_range:\n",
    "    val, epochs = grid.policy_evaluation(Policy,0.001,gamma)\n",
    "    epochs_needed.append(epochs)\n",
    "    \n",
    "plt.figure()\n",
    "plt.plot(gamma_range,epochs_needed)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "V_opt, pol_opt, epochs = grid.policy_iteration()\n",
    "print(\"The value of the optimal policy using policy iteration is {}:\".format(V_opt))\n",
    "print(\"The optimal policy using policy iteration is {}\".format(pol_opt))\n",
    "print(\"The number of epochs for convergence are {}\".format(epochs))\n",
    "\n",
    "\n",
    "pol_opt2, epochs = grid.value_iteration()\n",
    "print(\"The optimal policy using value iteration is {}\".format(pol_opt2))\n",
    "print(\"The number of epocs for convergence is {}\".format(epochs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Using draw_deterministic_policy to illustrate some arbitracy policy.\n",
    "Policy2 = np.array([np.argmax(pol_opt[row,:]) for row in range(grid.state_size)])\n",
    "\n",
    "\n",
    "grid.draw_deterministic_policy(Policy2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Using draw_deterministic_policy to illustrate optimal policy.\n",
    "\n",
    "\n",
    "\n",
    "Optimal_Policy = np.array([np.argmax(pol_opt2[row,:]) for row in range(0,grid.state_size)])\n",
    "grid.draw_deterministic_policy(Optimal_Policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## back to the original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld(object):\n",
    "    def __init__(self):\n",
    "        \n",
    "        ### Attributes defining the Gridworld #######\n",
    "        # Shape of the gridworld\n",
    "        self.shape = (4,4)\n",
    "        \n",
    "        # Locations of the obstacles\n",
    "        self.obstacle_locs = [(1,2),(2,0),(3,0),(3,1),(3,3)]\n",
    "        \n",
    "        # Locations for the absorbing states\n",
    "        self.absorbing_locs = [(0,1),(3,2)]\n",
    "        \n",
    "        # Rewards for each of the absorbing states \n",
    "        self.special_rewards = [10, -100] #corresponds to each of the absorbing_locs\n",
    "        \n",
    "        # Reward for all the other states\n",
    "        self.default_reward = -1\n",
    "        \n",
    "        # Starting location\n",
    "        self.starting_loc = (0,0)\n",
    "        \n",
    "        # Action names\n",
    "        self.action_names = ['N','E','S','W']\n",
    "        \n",
    "        # Number of actions\n",
    "        self.action_size = len(self.action_names)\n",
    "        \n",
    "        \n",
    "        # Randomizing action results: [1 0 0 0] to no Noise in the action results.\n",
    "        self.action_randomizing_array = [1, 0, 0, 0]\n",
    "        \n",
    "        ############################################\n",
    "    \n",
    "    \n",
    "    \n",
    "        #### Internal State  ####\n",
    "        \n",
    "    \n",
    "        # Get attributes defining the world\n",
    "        state_size, T, R, pi, absorbing, locs = self.build_grid_world()\n",
    "        \n",
    "        # Number of valid states in the gridworld (there are 11 of them)\n",
    "        self.state_size = state_size\n",
    "        \n",
    "        # Transition operator (3D tensor)\n",
    "        self.T = T\n",
    "        \n",
    "        # Reward function (3D tensor)\n",
    "        self.R = R\n",
    "        \n",
    "        \n",
    "        # Probability function for stochasticity (2D tensor)\n",
    "        self.pi = pi\n",
    "        \n",
    "        # Absorbing states\n",
    "        self.absorbing = absorbing\n",
    "        \n",
    "        # The locations of the valid states\n",
    "        self.locs = locs\n",
    "        \n",
    "        # Number of the starting state\n",
    "        self.starting_state = self.loc_to_state(self.starting_loc, locs);\n",
    "        \n",
    "        # Locating the initial state\n",
    "        self.initial = np.zeros((1,len(locs)));\n",
    "        self.initial[0,self.starting_state] = 1\n",
    "        \n",
    "        \n",
    "        # Placing the walls on a bitmap\n",
    "        self.walls = np.zeros(self.shape);\n",
    "        for ob in self.obstacle_locs:\n",
    "            self.walls[ob]=1\n",
    "            \n",
    "        # Placing the absorbers on a grid for illustration\n",
    "        self.absorbers = np.zeros(self.shape)\n",
    "        for ab in self.absorbing_locs:\n",
    "            self.absorbers[ab] = -1\n",
    "        \n",
    "        # Placing the rewarders on a grid for illustration\n",
    "        self.rewarders = np.zeros(self.shape)\n",
    "        for i, rew in enumerate(self.absorbing_locs):\n",
    "            self.rewarders[rew] = self.special_rewards[i]\n",
    "        \n",
    "        #Illustrating the grid world\n",
    "        self.paint_maps()\n",
    "        ################################\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ####### Getters ###########\n",
    "    \n",
    "    def get_transition_matrix(self):\n",
    "        return self.T\n",
    "    \n",
    "    def get_reward_matrix(self):\n",
    "        return self.R\n",
    "    \n",
    "    def get_probability_matrix(self):\n",
    "        return self.pi\n",
    "    \n",
    "    \n",
    "    ########################\n",
    "    \n",
    "    ####### Methods #########\n",
    "    \n",
    "    \n",
    "    def value_iteration(self, discount = 0.3, threshold = 0.0001):\n",
    "        V = np.zeros(self.state_size)\n",
    "        \n",
    "        T = self.get_transition_matrix()\n",
    "        R = self.get_reward_matrix()\n",
    "        \n",
    "        ####\n",
    "        pi = self.get_probability_matrix()\n",
    "        \n",
    "        epochs = 0\n",
    "        while True:\n",
    "            epochs+=1\n",
    "            delta = 0\n",
    "\n",
    "            for state_idx in range(self.state_size):\n",
    "                if(self.absorbing[0,state_idx]):\n",
    "                    continue \n",
    "                    \n",
    "                v = V[state_idx]\n",
    "\n",
    "                Q = np.zeros(4)\n",
    "                for state_idx_prime in range(self.state_size):\n",
    "                    Q +=  pi[state_idx,:]*T[state_idx_prime,state_idx,:] * (R[state_idx_prime,state_idx, :] + discount * V[state_idx_prime])\n",
    "                    ########### added P\n",
    "                    \n",
    "                V[state_idx]= np.max(Q)\n",
    "                delta = max(delta,np.abs(v - V[state_idx]))\n",
    "                \n",
    "            if(delta<threshold):\n",
    "                optimal_policy = np.zeros((self.state_size, self.action_size))\n",
    "                for state_idx in range(self.state_size):\n",
    "                    Q = np.zeros(4)\n",
    "                    for state_idx_prime in range(self.state_size):\n",
    "                        Q += pi[state_idx,:]*T[state_idx_prime,state_idx,:] * (R[state_idx_prime,state_idx, :] + discount * V[state_idx_prime])\n",
    "                    \n",
    "                    optimal_policy[state_idx, np.argmax(Q)]=1\n",
    "\n",
    "                \n",
    "                return optimal_policy,epochs\n",
    "\n",
    "\n",
    "    \n",
    "    def policy_iteration(self, discount=0.3, threshold = 0.0001):\n",
    "        policy= np.zeros((self.state_size, self.action_size))\n",
    "        policy[:,0] = 1\n",
    "        \n",
    "        T = self.get_transition_matrix()\n",
    "        R = self.get_reward_matrix()\n",
    "        \n",
    "        ########\n",
    "        pi = self.get_probability_matrix()\n",
    "        \n",
    "        epochs =0\n",
    "        while True: \n",
    "            V, epochs_eval = self.policy_evaluation(policy, threshold, discount)\n",
    "            \n",
    "            epochs+=epochs_eval\n",
    "            #Policy iteration\n",
    "            policy_stable = True\n",
    "            \n",
    "            for state_idx in range(policy.shape[0]):\n",
    "                if(self.absorbing[0,state_idx]):\n",
    "                    continue \n",
    "                    \n",
    "                old_action = np.argmax(policy[state_idx,:])\n",
    "                \n",
    "                Q = np.zeros(4)\n",
    "                for state_idx_prime in range(policy.shape[0]):\n",
    "                    Q += pi[state_idx,:] * T[state_idx_prime,state_idx,:] * (R[state_idx_prime,state_idx, :] + discount * V[state_idx_prime])\n",
    "                   \n",
    "                ####### Added P, 2D vector consisting of pi(s,a)\n",
    "                \n",
    "                new_policy = np.zeros(4)\n",
    "                new_policy[np.argmax(Q)]=1\n",
    "                policy[state_idx] = new_policy\n",
    "                \n",
    "                if(old_action !=np.argmax(policy[state_idx])):\n",
    "                    policy_stable = False\n",
    "            \n",
    "            if(policy_stable):\n",
    "                return V, policy,epochs\n",
    "                \n",
    "                \n",
    "                \n",
    "        \n",
    "    \n",
    "    def policy_evaluation(self, policy, threshold, discount):\n",
    "        \n",
    "        # Make sure delta is bigger than the threshold to start with\n",
    "        delta= 2*threshold\n",
    "        \n",
    "        #Get the reward and transition matrices\n",
    "        R = self.get_reward_matrix()\n",
    "        T = self.get_transition_matrix()\n",
    "        \n",
    "        \n",
    "        ###\n",
    "        pi = self.get_probability_matrix()\n",
    "\n",
    "        \n",
    "        # The value is initialised at 0\n",
    "        V = np.zeros(policy.shape[0])\n",
    "        # Make a deep copy of the value array to hold the update during the evaluation\n",
    "        Vnew = np.copy(V)\n",
    "        \n",
    "        epoch = 0\n",
    "        # While the Value has not yet converged do:\n",
    "        while delta>threshold:\n",
    "            epoch += 1\n",
    "            for state_idx in range(policy.shape[0]):\n",
    "                # If it is one of the absorbing states, ignore\n",
    "                if(self.absorbing[0,state_idx]):\n",
    "                    continue   \n",
    "                \n",
    "                # Accumulator variable for the Value of a state\n",
    "                tmpV = 0\n",
    "                for action_idx in range(policy.shape[1]):\n",
    "                    # Accumulator variable for the State-Action Value\n",
    "                    tmpQ = 0\n",
    "                    for state_idx_prime in range(policy.shape[0]):\n",
    "                        tmpQ = tmpQ + pi[state_idx,action_idx]*T[state_idx_prime,state_idx,action_idx] * (R[state_idx_prime,state_idx, action_idx] + discount * V[state_idx_prime])\n",
    "                    \n",
    "                    tmpV += policy[state_idx,action_idx] * tmpQ\n",
    "                    \n",
    "                # Update the value of the state\n",
    "                Vnew[state_idx] = tmpV\n",
    "            \n",
    "            # After updating the values of all states, update the delta\n",
    "            delta =  max(abs(Vnew-V))\n",
    "            # and save the new value into the old\n",
    "            V=np.copy(Vnew)\n",
    "            \n",
    "        return V, epoch\n",
    "    \n",
    "    def draw_deterministic_policy(self, Policy):\n",
    "        # Draw a deterministic policy\n",
    "        # The policy needs to be a np array of 11 values between 0 and 3 with\n",
    "        # 0 -> N, 1->E, 2->S, 3->W\n",
    "        plt.figure()\n",
    "        plt.imshow(self.walls+ self.rewarders + self.absorbers)\n",
    "        plt.imshow(self.walls)\n",
    "        \n",
    "        #plt.hold('on')\n",
    "        for state, action in enumerate(Policy):\n",
    "            if(self.absorbing[0,state]):\n",
    "                continue\n",
    "            arrows = [r\"$\\uparrow$\",r\"$\\rightarrow$\", r\"$\\downarrow$\", r\"$\\leftarrow$\"]\n",
    "            action_arrow = arrows[action]\n",
    "            location = self.locs[state]\n",
    "            plt.text(location[1], location[0], action_arrow, ha='center', va='center', color='w', size='40')\n",
    "    \n",
    "        plt.show()\n",
    "    ##########################\n",
    "    \n",
    "    \n",
    "    ########### Internal Helper Functions #####################\n",
    "    def paint_maps(self):\n",
    "        plt.figure()\n",
    "        plt.subplot(1,3,1)\n",
    "        plt.imshow(self.walls)\n",
    "        plt.subplot(1,3,2)\n",
    "        plt.imshow(self.absorbers)\n",
    "        plt.subplot(1,3,3)\n",
    "        plt.imshow(self.rewarders) \n",
    "        plt.show()\n",
    "        \n",
    "    def build_grid_world(self):\n",
    "        # Get the locations of all the valid states, the neighbours of each state (by state number),\n",
    "        # and the absorbing states (array of 0's with ones in the absorbing states)\n",
    "        locations, neighbours, absorbing_states = self.get_topology()\n",
    "        \n",
    "        # Get the number of states\n",
    "        S = len(locations)\n",
    "        \n",
    "        # Initialise the transition matrix\n",
    "        T = np.zeros((S,S,4))\n",
    "        \n",
    "        ## Initialise the probability matrix\n",
    "        pi = np.zeros((S,4))  ### add\n",
    "        \n",
    "        ##probability matrix is based on action-state pairs\n",
    "        ##if the action and outcome is the samethen p=0.3\n",
    "        p_constant = 0.5\n",
    "        \n",
    "        for action in range(4):\n",
    "            for effect in range(4):\n",
    "                \n",
    "                # Randomize the outcome of taking an action. is it random tho......\n",
    "                \n",
    "                # outcome = \n",
    "                \n",
    "                outcome = (action+effect+1) % 4\n",
    "                if outcome == 0:\n",
    "                    outcome = 3\n",
    "                else:\n",
    "                    outcome -= 1\n",
    "    \n",
    "                # Fill the probability and transition matrix\n",
    "                prob = self.action_randomizing_array[effect] \n",
    "                \n",
    "                for prior_state in range(S):\n",
    "                    post_state = neighbours[prior_state, outcome]\n",
    "                    post_state = int(post_state)\n",
    "                    \n",
    "                    if action == effect: \n",
    "                        pi[prior_state,action] = p_constant\n",
    "                                                \n",
    "                    else:\n",
    "                        pi[prior_state,action] = (1-p_constant)/3\n",
    "                        \n",
    "                    T[post_state,prior_state,action] = T[post_state,prior_state,action]+prob\n",
    "                \n",
    "    \n",
    "        # Build the reward matrix\n",
    "        R = self.default_reward*np.ones((S,S,4))\n",
    "        for i, sr in enumerate(self.special_rewards):\n",
    "            post_state = self.loc_to_state(self.absorbing_locs[i],locations)\n",
    "            R[post_state,:,:]= sr\n",
    "        \n",
    "        return S, T, R, pi, absorbing_states,locations\n",
    "    \n",
    "    def get_topology(self):\n",
    "        height = self.shape[0]\n",
    "        width = self.shape[1]\n",
    "        \n",
    "        index = 1 \n",
    "        locs = []\n",
    "        neighbour_locs = []\n",
    "        \n",
    "        for i in range(height):\n",
    "            for j in range(width):\n",
    "                # Get the locaiton of each state\n",
    "                loc = (i,j)\n",
    "                \n",
    "                #And append it to the valid state locations if it is a valid state (ie not absorbing)\n",
    "                if(self.is_location(loc)):\n",
    "                    locs.append(loc)\n",
    "                    \n",
    "                    # Get an array with the neighbours of each state, in terms of locations\n",
    "                    local_neighbours = [self.get_neighbour(loc,direction) for direction in ['nr','ea','so', 'we']]\n",
    "                    neighbour_locs.append(local_neighbours)\n",
    "                \n",
    "        # translate neighbour lists from locations to states\n",
    "        num_states = len(locs)\n",
    "        state_neighbours = np.zeros((num_states,4))\n",
    "        \n",
    "        for state in range(num_states):\n",
    "            for direction in range(4):\n",
    "                # Find neighbour location\n",
    "                nloc = neighbour_locs[state][direction]\n",
    "                \n",
    "                # Turn location into a state number\n",
    "                nstate = self.loc_to_state(nloc,locs)\n",
    "      \n",
    "                # Insert into neighbour matrix\n",
    "                state_neighbours[state,direction] = nstate;\n",
    "                \n",
    "    \n",
    "        # Translate absorbing locations into absorbing state indices\n",
    "        absorbing = np.zeros((1,num_states))\n",
    "        for a in self.absorbing_locs:\n",
    "            absorbing_state = self.loc_to_state(a,locs)\n",
    "            absorbing[0,absorbing_state] =1\n",
    "        \n",
    "        return locs, state_neighbours, absorbing \n",
    "\n",
    "    def loc_to_state(self,loc,locs):\n",
    "        #takes list of locations and gives index corresponding to input loc\n",
    "        return locs.index(tuple(loc))\n",
    "        #return locs.index(loc)\n",
    "\n",
    "\n",
    "    def is_location(self, loc):\n",
    "        # It is a valid location if it is in grid and not obstacle\n",
    "        if(loc[0]<0 or loc[1]<0 or loc[0]>self.shape[0]-1 or loc[1]>self.shape[1]-1):\n",
    "            return False\n",
    "        elif(loc in self.obstacle_locs):\n",
    "            return False\n",
    "        else:\n",
    "             return True\n",
    "            \n",
    "    def get_neighbour(self,loc,direction):\n",
    "        #Find the valid neighbours (ie that are in the grif and not obstacle)\n",
    "        i = loc[0]\n",
    "        j = loc[1]\n",
    "        \n",
    "        nr = (i-1,j)\n",
    "        ea = (i,j+1)\n",
    "        so = (i+1,j)\n",
    "        we = (i,j-1)\n",
    "        \n",
    "        # If the neighbour is a valid location, accept it, otherwise, stay put\n",
    "        if(direction == 'nr' and self.is_location(nr)):\n",
    "            return nr\n",
    "        elif(direction == 'ea' and self.is_location(ea)):\n",
    "            return ea\n",
    "        elif(direction == 'so' and self.is_location(so)):\n",
    "            return so\n",
    "        elif(direction == 'we' and self.is_location(we)):\n",
    "            return we\n",
    "        else:\n",
    "            #default is to return to the same location\n",
    "            return loc\n",
    "        \n",
    "###########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Policy is : [[0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]]\n",
      "The value of that policy is :[ 0.21590715  0.          1.15108008 -0.21562085 -0.2577203   0.19715072\n",
      " -0.27164966 -0.3190686  -4.44793091 -0.43094189  0.        ]\n",
      "It took 4 epochs\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The value of the optimal policy using policy iteration is [ 1.66666667  0.          5.          0.25       -0.08333333  1.66666667\n",
      " -0.15416667 -0.08333333 -0.17538542 -0.174375    0.        ]:\n",
      "The optimal policy using policy iteration is [[0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "The number of epochs for convergence are 18\n",
      "The optimal policy using value iteration is [[0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "The number of epocs for convergence is 4\n"
     ]
    }
   ],
   "source": [
    "grid = GridWorld()\n",
    "\n",
    "### Question 1 : Change the policy here:\n",
    "Policy= np.zeros((grid.state_size, grid.action_size))\n",
    "Policy = Policy + 0.25\n",
    "print(\"The Policy is : {}\".format(Policy))\n",
    "\n",
    "val, epochs = grid.policy_evaluation(Policy,0.001,0.3)\n",
    "print(\"The value of that policy is :{}\".format(val))\n",
    "print(\"It took {} epochs\".format(epochs))\n",
    "\n",
    "\n",
    "gamma_range = [0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]\n",
    "epochs_needed = []\n",
    "for gamma in gamma_range:\n",
    "    val, epochs = grid.policy_evaluation(Policy,0.001,gamma)\n",
    "    epochs_needed.append(epochs)\n",
    "    \n",
    "plt.figure()\n",
    "plt.plot(gamma_range,epochs_needed)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "V_opt, pol_opt, epochs = grid.policy_iteration()\n",
    "print(\"The value of the optimal policy using policy iteration is {}:\".format(V_opt))\n",
    "print(\"The optimal policy using policy iteration is {}\".format(pol_opt))\n",
    "print(\"The number of epochs for convergence are {}\".format(epochs))\n",
    "\n",
    "\n",
    "pol_opt2, epochs = grid.value_iteration()\n",
    "print(\"The optimal policy using value iteration is {}\".format(pol_opt2))\n",
    "print(\"The number of epocs for convergence is {}\".format(epochs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# OPTIMAL POLICY VIA VALUE ITERATION\n",
    "\n",
    "Policy_VALIT = np.array([np.argmax(pol_opt2[row,:]) for row in range(grid.state_size)])\n",
    "\n",
    "\n",
    "grid.draw_deterministic_policy(Policy_VALIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# OPTIMAL POLICY VIA POLICY ITERATION\n",
    "Policy_POLIT = np.array([np.argmax(pol_opt[row,:]) for row in range(grid.state_size)])\n",
    "\n",
    "\n",
    "grid.draw_deterministic_policy(Policy_POLIT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now again with changing the pi "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld(object):\n",
    "    def __init__(self):\n",
    "        \n",
    "        ### Attributes defining the Gridworld #######\n",
    "        # Shape of the gridworld\n",
    "        self.shape = (4,4)\n",
    "        \n",
    "        # Locations of the obstacles\n",
    "        self.obstacle_locs = [(1,2),(2,0),(3,0),(3,1),(3,3)]\n",
    "        \n",
    "        # Locations for the absorbing states\n",
    "        self.absorbing_locs = [(0,1),(3,2)]\n",
    "        \n",
    "        # Rewards for each of the absorbing states \n",
    "        self.special_rewards = [10, -100] #corresponds to each of the absorbing_locs\n",
    "        \n",
    "        # Reward for all the other states\n",
    "        self.default_reward = -1\n",
    "        \n",
    "        # Starting location\n",
    "        self.starting_loc = (0,0)\n",
    "        \n",
    "        # Action names\n",
    "        self.action_names = ['N','E','S','W']\n",
    "        \n",
    "        # Number of actions\n",
    "        self.action_size = len(self.action_names)\n",
    "        \n",
    "        \n",
    "        # Randomizing action results: [1 0 0 0] to no Noise in the action results.\n",
    "        self.action_randomizing_array = [1, 0, 0, 0]\n",
    "        \n",
    "        ############################################\n",
    "    \n",
    "    \n",
    "    \n",
    "        #### Internal State  ####\n",
    "        \n",
    "    \n",
    "        # Get attributes defining the world\n",
    "        state_size, T, R, pi, absorbing, locs = self.build_grid_world()\n",
    "        \n",
    "        # Number of valid states in the gridworld (there are 11 of them)\n",
    "        self.state_size = state_size\n",
    "        \n",
    "        # Transition operator (3D tensor)\n",
    "        self.T = T\n",
    "        \n",
    "        # Reward function (3D tensor)\n",
    "        self.R = R\n",
    "        \n",
    "        \n",
    "        # Probability function for stochasticity (2D tensor)\n",
    "        self.pi = pi\n",
    "        \n",
    "        # Absorbing states\n",
    "        self.absorbing = absorbing\n",
    "        \n",
    "        # The locations of the valid states\n",
    "        self.locs = locs\n",
    "        \n",
    "        # Number of the starting state\n",
    "        self.starting_state = self.loc_to_state(self.starting_loc, locs);\n",
    "        \n",
    "        # Locating the initial state\n",
    "        self.initial = np.zeros((1,len(locs)));\n",
    "        self.initial[0,self.starting_state] = 1\n",
    "        \n",
    "        \n",
    "        # Placing the walls on a bitmap\n",
    "        self.walls = np.zeros(self.shape);\n",
    "        for ob in self.obstacle_locs:\n",
    "            self.walls[ob]=1\n",
    "            \n",
    "        # Placing the absorbers on a grid for illustration\n",
    "        self.absorbers = np.zeros(self.shape)\n",
    "        for ab in self.absorbing_locs:\n",
    "            self.absorbers[ab] = -1\n",
    "        \n",
    "        # Placing the rewarders on a grid for illustration\n",
    "        self.rewarders = np.zeros(self.shape)\n",
    "        for i, rew in enumerate(self.absorbing_locs):\n",
    "            self.rewarders[rew] = self.special_rewards[i]\n",
    "        \n",
    "        #Illustrating the grid world\n",
    "        self.paint_maps()\n",
    "        ################################\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ####### Getters ###########\n",
    "    \n",
    "    def get_transition_matrix(self):\n",
    "        return self.T\n",
    "    \n",
    "    def get_reward_matrix(self):\n",
    "        return self.R\n",
    "    \n",
    "    def get_probability_matrix(self):\n",
    "        return self.pi\n",
    "    \n",
    "    \n",
    "    ########################\n",
    "    \n",
    "    ####### Methods #########\n",
    "    \n",
    "    \n",
    "    def value_iteration(self, discount = 0.3, threshold = 0.0001):\n",
    "        V = np.zeros(self.state_size)\n",
    "        \n",
    "        T = self.get_transition_matrix()\n",
    "        R = self.get_reward_matrix()\n",
    "        \n",
    "        ####\n",
    "        pi = self.get_probability_matrix()\n",
    "        \n",
    "        epochs = 0\n",
    "        while True:\n",
    "            epochs+=1\n",
    "            delta = 0\n",
    "\n",
    "            for state_idx in range(self.state_size):\n",
    "                if(self.absorbing[0,state_idx]):\n",
    "                    continue \n",
    "                    \n",
    "                v = V[state_idx]\n",
    "\n",
    "                Q = np.zeros(4)\n",
    "                pi_valit1 = pi[state_idx,:]\n",
    "                for state_idx_prime in range(self.state_size):\n",
    "                    Q +=  pi_valit1*T[state_idx_prime,state_idx,:] * (R[state_idx_prime,state_idx, :] + discount* V[state_idx_prime])\n",
    "                    ########### added P\n",
    "                    \n",
    "                V[state_idx]= np.max(Q)\n",
    "                delta = max(delta,np.abs(v - V[state_idx]))\n",
    "                \n",
    "            if(delta<threshold):\n",
    "                optimal_policy = np.zeros((self.state_size, self.action_size))\n",
    "                for state_idx in range(self.state_size):\n",
    "                    Q = np.zeros(4)\n",
    "                    pi_valit2 = pi[state_idx,:]\n",
    "                    for state_idx_prime in range(self.state_size):\n",
    "                        Q += pi_valit2*T[state_idx_prime,state_idx,:] * (R[state_idx_prime,state_idx, :] + discount * V[state_idx_prime])\n",
    "                    \n",
    "                    optimal_policy[state_idx, np.argmax(Q)]=1\n",
    "\n",
    "                \n",
    "                return optimal_policy,epochs\n",
    "\n",
    "\n",
    "    \n",
    "    def policy_iteration(self, discount=0.3, threshold = 0.0001):\n",
    "        policy= np.zeros((self.state_size, self.action_size))\n",
    "        policy[:,0] = 1\n",
    "        \n",
    "        T = self.get_transition_matrix()\n",
    "        R = self.get_reward_matrix()\n",
    "        \n",
    "        ########\n",
    "        pi = self.get_probability_matrix()\n",
    "        \n",
    "        epochs =0\n",
    "        while True: \n",
    "            V, epochs_eval = self.policy_evaluation(policy, threshold, discount)\n",
    "            \n",
    "            epochs+=epochs_eval\n",
    "            #Policy iteration\n",
    "            policy_stable = True\n",
    "            \n",
    "            for state_idx in range(policy.shape[0]):\n",
    "                if(self.absorbing[0,state_idx]):\n",
    "                    continue \n",
    "                    \n",
    "                old_action = np.argmax(policy[state_idx,:])\n",
    "                \n",
    "                Q = np.zeros(4)\n",
    "                pi_polit = pi[state_idx,:]\n",
    "                for state_idx_prime in range(policy.shape[0]):\n",
    "                    Q +=  pi_polit* T[state_idx_prime,state_idx,:] * (R[state_idx_prime,state_idx, :] + discount * V[state_idx_prime])\n",
    "                   \n",
    "                ####### Added P, 2D vector consisting of pi(s,a)\n",
    "                \n",
    "                new_policy = np.zeros(4)\n",
    "                new_policy[np.argmax(Q)]=1\n",
    "                policy[state_idx] = new_policy\n",
    "                \n",
    "                if(old_action !=np.argmax(policy[state_idx])):\n",
    "                    policy_stable = False\n",
    "            \n",
    "            if(policy_stable):\n",
    "                return V, policy,epochs\n",
    "                \n",
    "                \n",
    "                \n",
    "        \n",
    "    \n",
    "    def policy_evaluation(self, policy, threshold, discount):\n",
    "        \n",
    "        # Make sure delta is bigger than the threshold to start with\n",
    "        delta= 2*threshold\n",
    "        \n",
    "        #Get the reward and transition matrices\n",
    "        R = self.get_reward_matrix()\n",
    "        T = self.get_transition_matrix()\n",
    "        \n",
    "        \n",
    "        ###\n",
    "        pi = self.get_probability_matrix()\n",
    "\n",
    "        \n",
    "        # The value is initialised at 0\n",
    "        V = np.zeros(policy.shape[0])\n",
    "        # Make a deep copy of the value array to hold the update during the evaluation\n",
    "        Vnew = np.copy(V)\n",
    "        \n",
    "        epoch = 0\n",
    "        # While the Value has not yet converged do:\n",
    "        while delta>threshold:\n",
    "            epoch += 1\n",
    "            for state_idx in range(policy.shape[0]):\n",
    "                # If it is one of the absorbing states, ignore\n",
    "                if(self.absorbing[0,state_idx]):\n",
    "                    continue   \n",
    "                \n",
    "                # Accumulator variable for the Value of a state\n",
    "                tmpV = 0\n",
    "                for action_idx in range(policy.shape[1]):\n",
    "                    # Accumulator variable for the State-Action Value\n",
    "                    tmpQ = 0\n",
    "                    for state_idx_prime in range(policy.shape[0]):\n",
    "                        tmpQ = tmpQ + pi[state_idx,action_idx]*T[state_idx_prime,state_idx,action_idx] * (R[state_idx_prime,state_idx, action_idx] + discount * V[state_idx_prime])\n",
    "                    \n",
    "                    tmpV += policy[state_idx,action_idx] * tmpQ\n",
    "                    \n",
    "                # Update the value of the state\n",
    "                Vnew[state_idx] = tmpV\n",
    "            \n",
    "            # After updating the values of all states, update the delta\n",
    "            delta =  max(abs(Vnew-V))\n",
    "            # and save the new value into the old\n",
    "            V=np.copy(Vnew)\n",
    "            \n",
    "        return V, epoch\n",
    "    \n",
    "    def draw_deterministic_policy(self, Policy):\n",
    "        # Draw a deterministic policy\n",
    "        # The policy needs to be a np array of 11 values between 0 and 3 with\n",
    "        # 0 -> N, 1->E, 2->S, 3->W\n",
    "        plt.figure()\n",
    "        plt.imshow(self.walls+ self.rewarders + self.absorbers)\n",
    "        plt.imshow(self.walls)\n",
    "        \n",
    "        #plt.hold('on')\n",
    "        for state, action in enumerate(Policy):\n",
    "            if(self.absorbing[0,state]):\n",
    "                continue\n",
    "            arrows = [r\"$\\uparrow$\",r\"$\\rightarrow$\", r\"$\\downarrow$\", r\"$\\leftarrow$\"]\n",
    "            action_arrow = arrows[action]\n",
    "            location = self.locs[state]\n",
    "            plt.text(location[1], location[0], action_arrow, ha='center', va='center', color='w', size='40')\n",
    "    \n",
    "        plt.show()\n",
    "    ##########################\n",
    "    \n",
    "    \n",
    "    ########### Internal Helper Functions #####################\n",
    "    def paint_maps(self):\n",
    "        plt.figure()\n",
    "        plt.subplot(1,3,1)\n",
    "        plt.imshow(self.walls)\n",
    "        plt.subplot(1,3,2)\n",
    "        plt.imshow(self.absorbers)\n",
    "        plt.subplot(1,3,3)\n",
    "        plt.imshow(self.rewarders) \n",
    "        plt.show()\n",
    "        \n",
    "    def build_grid_world(self):\n",
    "        # Get the locations of all the valid states, the neighbours of each state (by state number),\n",
    "        # and the absorbing states (array of 0's with ones in the absorbing states)\n",
    "        locations, neighbours, absorbing_states = self.get_topology()\n",
    "        \n",
    "        # Get the number of states\n",
    "        S = len(locations)\n",
    "        \n",
    "        # Initialise the transition matrix\n",
    "        T = np.zeros((S,S,4))\n",
    "        \n",
    "        ## Initialise the probability matrix\n",
    "        pi = np.zeros((S,4))  ### add\n",
    "        \n",
    "        ##probability matrix is based on action-state pairs\n",
    "        ##if the action and outcome is the samethen p=0.3\n",
    "        \n",
    "        ###Iterate over different values of p to see how it affects convergence\n",
    "        p_constant = 0.25\n",
    "        \n",
    "        for action in range(4):\n",
    "            for effect in range(4):\n",
    "                \n",
    "                # Randomize the outcome of taking an action. \n",
    "\n",
    "                outcome = (action+effect+1) % 4\n",
    "                if outcome == 0:\n",
    "                    outcome = 3\n",
    "                else:\n",
    "                    outcome -= 1\n",
    "    \n",
    "                # Fill the probability and transition matrix\n",
    "                for prior_state in range(S):\n",
    "                    post_state = neighbours[prior_state, outcome]\n",
    "                    post_state = int(post_state)\n",
    "                    \n",
    "                    if action == outcome: \n",
    "                        pi[prior_state,action] = p_constant\n",
    "                        prob = pi[prior_state,action]\n",
    "                    else:\n",
    "                        pi[prior_state,action] = (1-p_constant)/3\n",
    "                        prob = pi[prior_state,action]\n",
    "                    T[post_state,prior_state,action] = T[post_state,prior_state,action]+prob\n",
    "                \n",
    "    \n",
    "        # Build the reward matrix\n",
    "        R = self.default_reward*np.ones((S,S,4))\n",
    "        for i, sr in enumerate(self.special_rewards):\n",
    "            post_state = self.loc_to_state(self.absorbing_locs[i],locations)\n",
    "            R[post_state,:,:]= sr\n",
    "        \n",
    "        return S, T, R, pi, absorbing_states,locations\n",
    "    \n",
    "    def get_topology(self):\n",
    "        height = self.shape[0]\n",
    "        width = self.shape[1]\n",
    "        \n",
    "        index = 1 \n",
    "        locs = []\n",
    "        neighbour_locs = []\n",
    "        \n",
    "        for i in range(height):\n",
    "            for j in range(width):\n",
    "                # Get the locaiton of each state\n",
    "                loc = (i,j)\n",
    "                \n",
    "                #And append it to the valid state locations if it is a valid state (ie not absorbing)\n",
    "                if(self.is_location(loc)):\n",
    "                    locs.append(loc)\n",
    "                    \n",
    "                    # Get an array with the neighbours of each state, in terms of locations\n",
    "                    local_neighbours = [self.get_neighbour(loc,direction) for direction in ['nr','ea','so', 'we']]\n",
    "                    neighbour_locs.append(local_neighbours)\n",
    "                \n",
    "        # translate neighbour lists from locations to states\n",
    "        num_states = len(locs)\n",
    "        state_neighbours = np.zeros((num_states,4))\n",
    "        \n",
    "        for state in range(num_states):\n",
    "            for direction in range(4):\n",
    "                # Find neighbour location\n",
    "                nloc = neighbour_locs[state][direction]\n",
    "                \n",
    "                # Turn location into a state number\n",
    "                nstate = self.loc_to_state(nloc,locs)\n",
    "      \n",
    "                # Insert into neighbour matrix\n",
    "                state_neighbours[state,direction] = nstate;\n",
    "                \n",
    "    \n",
    "        # Translate absorbing locations into absorbing state indices\n",
    "        absorbing = np.zeros((1,num_states))\n",
    "        for a in self.absorbing_locs:\n",
    "            absorbing_state = self.loc_to_state(a,locs)\n",
    "            absorbing[0,absorbing_state] =1\n",
    "        \n",
    "        return locs, state_neighbours, absorbing \n",
    "\n",
    "    def loc_to_state(self,loc,locs):\n",
    "        #takes list of locations and gives index corresponding to input loc\n",
    "        return locs.index(tuple(loc))\n",
    "        #return locs.index(loc)\n",
    "\n",
    "\n",
    "    def is_location(self, loc):\n",
    "        # It is a valid location if it is in grid and not obstacle\n",
    "        if(loc[0]<0 or loc[1]<0 or loc[0]>self.shape[0]-1 or loc[1]>self.shape[1]-1):\n",
    "            return False\n",
    "        elif(loc in self.obstacle_locs):\n",
    "            return False\n",
    "        else:\n",
    "             return True\n",
    "            \n",
    "    def get_neighbour(self,loc,direction):\n",
    "        #Find the valid neighbours (ie that are in the grif and not obstacle)\n",
    "        i = loc[0]\n",
    "        j = loc[1]\n",
    "        \n",
    "        nr = (i-1,j)\n",
    "        ea = (i,j+1)\n",
    "        so = (i+1,j)\n",
    "        we = (i,j-1)\n",
    "        \n",
    "        # If the neighbour is a valid location, accept it, otherwise, stay put\n",
    "        if(direction == 'nr' and self.is_location(nr)):\n",
    "            return nr\n",
    "        elif(direction == 'ea' and self.is_location(ea)):\n",
    "            return ea\n",
    "        elif(direction == 'so' and self.is_location(so)):\n",
    "            return so\n",
    "        elif(direction == 'we' and self.is_location(we)):\n",
    "            return we\n",
    "        else:\n",
    "            #default is to return to the same location\n",
    "            return loc\n",
    "        \n",
    "###########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Policy is : [[0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]]\n",
      "The value of that policy is :[ 0.44982011  0.          0.44955411 -0.25628056 -0.2425201   0.43398886\n",
      " -0.27236876 -0.37934666 -6.57524397 -0.3931026   0.        ]\n",
      "It took 4 epochs\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAHqZJREFUeJzt3Xl8VPW9//HXh30HgbATArKDohgBte51AwVtbX/a64LVcrHW2tZ7ay2Ciktrf1dtrb0q1tal1WuvFQgKKi64Aw2IkIQt7DsJS9gDIZ/fHzP2l4ZgJsnMnFnez8djHjkz55uZz2GSdw7nfOdzzN0REZHUUi/oAkREJPoU7iIiKUjhLiKSghTuIiIpSOEuIpKCFO4iIilI4S4ikoIU7iIiKUjhLiKSghoE9cLt27f3rKysoF5eRCQpLViwoNjdM6obF1i4Z2VlkZubG9TLi4gkJTNbF8k4HZYREUlBCncRkRSkcBcRSUEKdxGRFKRwFxFJQRGFu5m1MbPXzGyZmS01szMqrTcze8LMCs1ssZkNjU25IiISiUinQv4OeMvdrzazRkCzSusvA/qEb8OBp8JfRUQkANWGu5m1As4BxgK4+2HgcKVhY4AXPXTNvrnhPf3O7r4lyvWKSAr5ZGUx89fsCLqMuMvOass5fav9HFKdRLLn3gsoAv5sZkOABcAd7r6/wpiuwIYK9zeGH/uXcDezccA4gMzMzDqULSLJrnD7Xm56fj5HjjpmQVcTX+PPPTEhwr0BMBS43d3nmdnvgF8AEyuMqeqtOebK2+4+BZgCkJ2drStzi6Qpd+eeaXk0a9SA9+48l/YtGgddUsqJ5ITqRmCju88L33+NUNhXHtO9wv1uwOa6lyciqWjqF5uYu3ond13aX8EeI9WGu7tvBTaYWb/wQxcCBZWG5QA3hGfNjABKdLxdRKqy+8BhHnpzKadmtuGa07tX/w1SK5HOlrkd+Gt4psxq4CYzGw/g7k8DM4GRQCFwALgpBrWKSAp45K3l7D54hJeuPIl69dLsYHscRRTu7r4IyK708NMV1jtwWxTrEpEUtGDdLl6Zv55bvtGTgV1aBV1OStMnVEUkLsqOljNh6hI6t27CTy7qG3Q5KU/hLiJx8fxna1m2dS/3XjGQFo0Du5RE2lC4i0jMbd59kMdmr+CC/h24ZFCnoMtJCwp3EYm5yTMKKHfn/tGDsHT7xFJAFO4iElPvL9vGW/lbuf2CPnRvW7ktlcSKwl1EYubg4aNMmp5P7w4t+MHZvYIuJ63orIaIxMzv31/Jxl0HeXXcCBo10L5kPOlfW0RiYuW2vUz5aDVXn9aN4b3aBV1O2lG4i0jUuTsTpuXRvHED7r6sf9DlpCWFu4hE3d8XbmL+mp3cfVl/2qkxWCAU7iISVbv2H+bhmUs5rccJfDdbjcGConAXkaj6zdvLKDl4hAevHKzGYAFSuItI1CxYt5NX5m/g5m/0ZEBnNQYLksJdRKLiyNFyJkzNo0vrJtxxYZ+gy0l7mucuIlHx/KehxmDPXH8azdUYLHARvQNmthbYCxwFytw9u9L684DpwJrwQ6+7++TolSkiiWzT7oM8/u4KvjmgAxcP7Bh0OULN9tzPd/fir1n/sbtfXteCRCT53J+Tjzvcp8ZgCUPH3EWkTt4t2MY7Bdv48YV96HaCGoMlikjD3YF3zGyBmY07zpgzzOxLM5tlZoOiVJ+IJLADh8u4Nyefvh1bcMvZPYMuRyqI9LDMWe6+2cw6ALPNbJm7f1Rh/UKgh7vvM7ORwDTgmNPl4T8M4wAyMzPrWLqIBO2J9wrZtPsgf/v3M2hYXwcCEklE74a7bw5/3Q5MBYZVWr/H3feFl2cCDc2sfRXPM8Xds909OyMjo87Fi0hwVmzbyx8/Xs13TuvGsJ5tgy5HKqk23M2suZm1/GoZuBjIqzSmk4XPopjZsPDz7oh+uSKSCMrLnQlTl9CiSQPuHjkg6HKkCpEclukITA1ndwPgZXd/y8zGA7j708DVwK1mVgYcBK5xd49RzSISsNcWbuQfa3fxm2+fTNvmjYIuR6pQbbi7+2pgSBWPP11h+UngyeiWJiKJaNf+w/xq5lKye5zA1ad1C7ocOQ6dARGRGvn1rGXsPVTGg1epMVgiU7iLSMRy1+7k1dwN3Hx2T/p3UmOwRKZwF5GIfNUYrGubpmoMlgTU3UdEIvKnT9awfNtenr0hm2aNFB2JTnvuIlKtjbsO8Nt3V3LRwI5cpMZgSUHhLiLVun9GARBqDCbJQeEuIl/rnfytzC7Yxk++2YeubZoGXY5ESOEuIsd14HAZ988ooF/Hlnz/G2oMlkx0VkREjut3761k0+6DvDZejcGSjd4tEanSsq17eO7jNfyf7O5kZ6kxWLJRuIvIMcrLnXum5tGySQN+cVn/oMuRWlC4i8gxXluwkdx1u7h75ABOUGOwpKRwF5F/sXP/YR6etZRhWW25eqgagyUrhbuI/ItfzVzKPjUGS3oKdxH5p/lrdvK/CzZyy9m96NuxZdDlSB0o3EUEgMNl5dwzbQld2zTlxxf2DrocqaOIwt3M1prZEjNbZGa5Vaw3M3vCzArNbLGZDY1+qSISS899soYV2/YxecwgNQZLATV5B8939+LjrLsM6BO+DQeeCn8VkSSwYecBfvfeCi4Z1JELB6gxWCqI1p/nMcCL4eumzjWzNmbW2d23ROn5JQ0V7ytFV+KNj/ty8qlnxr1XqDFYqog03B14x8wceMbdp1Ra3xXYUOH+xvBjCneplUnT83jx83VBl5FWJowcQBc1BksZkYb7We6+2cw6ALPNbJm7f1RhfVXzpY7Z5zKzccA4gMzMzBoXK+lh3uodvPj5OkYP6cLpPfWx93ho26wRlw7uFHQZEkURhbu7bw5/3W5mU4FhQMVw3wh0r3C/G7C5iueZAkwByM7O1n+45RihGRt5dDuhKY98+2SaNqofdEkiSana2TJm1tzMWn61DFwM5FUalgPcEJ41MwIo0fF2qY0/frKaldtDMzYU7CK1F8mee0dgqpl9Nf5ld3/LzMYDuPvTwExgJFAIHABuik25kso27DzAE++t5JJBHbmgv2ZsiNRFteHu7quBIVU8/nSFZQdui25pkk7cnXs1Y0MkavQJVUkIb+dv4/1l2/nZRX01Y0MkChTuErj9pWXcPyOf/p1aMvbMrKDLEUkJ+oyxBO63765gS8khnvzeUBroUm4iUaHfJAlUweY9/OnTtVw7LJPTepwQdDkiKUPhLoEpL3fumbaENk0bctel/YIuRySlKNwlMK/mbmDh+t38cuQA2jTTpdxEoknhLoEo3lfKr2ctY3jPtnxraNegyxFJOQp3CcTDM5dy4HAZD101mPAH5EQkihTuEnefr9rB6ws3Me6cXvTuoEu5icSCwl3i6qtLuXVv25Qfnd8n6HJEUpbmuUtcPfvxalYV7efPY09XYzCRGNKeu8TN+h2hxmCXDe7E+f07BF2OSEpTuEtcuDuTcvJoUM+YdMXAoMsRSXkKd4mLt/K2Mmd5ET+9qC+dW6sxmEisKdwl5vaVlnH/jAIGdG6lxmAicaJwl5h7fPYKtu09xMNXDVZjMJE4ifg3zczqm9kXZvZGFevGmlmRmS0K326JbpmSrPI3l/DnT9dw7bBMTs1UYzCReKnJVMg7gKVAq+Osf9Xdf1T3kiRVlJc7E6bmcUKzRtx1Sf+gyxFJKxHtuZtZN2AU8MfYliOp5JV/rGfRht3cc/kAWjdrGHQ5Imkl0sMyvwV+DpR/zZhvm9liM3vNzLpXNcDMxplZrpnlFhUV1bRWSSJFe0t5ZNYyzujVjitPUWMwkXirNtzN7HJgu7sv+JphM4Asdz8ZeBd4oapB7j7F3bPdPTsjI6NWBUty+NXMpRw8cpQHrlRjMJEgRLLnfhYw2szWAv8DXGBmf6k4wN13uHtp+O6zwGlRrVKSymerinn9i02MP/dEendoEXQ5Immp2nB397vdvZu7ZwHXAO+7+3UVx5hZ5wp3RxM68SppqLTsKPdMyyOzbTNuO7930OWIpK1aNw4zs8lArrvnAD82s9FAGbATGBud8iTZPPvRalYX7ef5m06nSUM1BhMJSo3C3d3nAHPCy5MqPH43cHc0C5Pks27Hfn7/fiGjTurMef3UGEwkSPq4oESFuzNpej4N6hkTL1djMJGgKdwlKmblbeXDFUXceXE/OrVuEnQ5ImlP4S51tvfQEe6fkc/Azq244YweQZcjIuhKTBIFj81ewfa9pTx93WlqDCaSIPSbKHWSt6mEFz5by78NV2MwkUSicJdaO1ruTJi6hLbNG/GfagwmklAU7lJrL89fz5cbS7hn1EBaN1VjMJFEonCXWinaW8pv3lrGmSe2Y8wpXYIuR0QqUbhLrTz0ZgGlR8rVGEwkQSncpcY+LSxm2qLNjD+3FydmqDGYSCJSuEuNlJYdZeK0PHq0a8YP1RhMJGFpnrvUyDMfrmZ18X5e+P4wNQYTSWDac5eIrS3ez5MfFDLq5M6c21cXWxFJZAp3iYi7M3F6Ho3q12OSGoOJJDyFu0TkzSVb+HhlMXde3JeOrdQYTCTRRRzuZlbfzL4wszeqWNfYzF41s0Izm2dmWdEsUoK199ARJs8oYHDXVlw/Qo3BRJJBTfbc7+D4l8+7Gdjl7r2Bx4FH6lqYJI5H31lB0b5SHrryJDUGE0kSEf2mmlk3YBTwx+MMGQO8EF5+DbjQ9MmWlLBkYwkvfr6W64b3YEj3NkGXIyIRinQq5G+BnwMtj7O+K7ABwN3LzKwEaAcU17lCYdnWPfzhg1WUHS2P+2vnbS6hbfPG/Mcl/eL+2iJSe9WGu5ldDmx39wVmdt7xhlXxmFfxXOOAcQCZmZk1KDN9HS4r5/aXv2BrySE6t4n/icyWjRvywJh+agwmkmQi2XM/CxhtZiOBJkArM/uLu19XYcxGoDuw0cwaAK2BnZWfyN2nAFMAsrOzjwl/OdZzn6xh5fZ9PHdjNhcO6Bh0OSKSJKo95u7ud7t7N3fPAq4B3q8U7AA5wI3h5avDYxTedbRh5wF+994KLhnUUcEuIjVS6/YDZjYZyHX3HOA54CUzKyS0x35NlOpLW+7OfTn51DPj3isGBV2OiCSZGoW7u88B5oSXJ1V4/BDwnWgWlu7eKdjGe8u2M2HkALq0aRp0OSKSZDRpOQHtLy3jvpx8+ndqydizsoIuR0SSkLpCJqDfvruCLSWHePJ7Q2moDw2JSC0oORLM0i17+NOna7l2WHdO63FC0OWISJJSuCeQ8nJnwtQltG7akLsu7R90OSKSxBTuCeTV3A0sXL+bCSMH0KZZo6DLEZEkpnBPEMX7Svn1rGUM79mWbw3tGnQ5IpLkFO4J4lczl3HgcBkPXTUY9VwTkbpSuCeAz1ft4O8LNzLunF707nC83mwiIpFTuAfscFk5E6fn0e2Epvzo/D5BlyMiKULz3AP27MerKdy+jz+PPZ2mjeoHXY6IpAjtuQdo/Y4DPPHeSi4b3Inz+3cIuhwRSSEK94C4O/fm5NGgnjHpioFBlyMiKUbhHpC387fywfIifnpRXzq3VmMwEYkuhXsA9pWWcV9OAQM6t2LsmVlBlyMiKUjhHoDHZ69g295DPHTVYBqoMZiIxICSJc7yN5fw/GdruXZYJkMz1RhMRGKj2nA3syZmNt/MvjSzfDO7v4oxY82syMwWhW+3xKbc5BZqDJZHm6YNuesSNQYTkdiJZJ57KXCBu+8zs4bAJ2Y2y93nVhr3qrv/KPolpo5X/rGeRRt289h3h9C6WcOgyxGRFFZtuIcvdL0vfLdh+KaLX9dQ8b5SHpm1jBG92nLVqWoMJiKxFdExdzOrb2aLgO3AbHefV8Wwb5vZYjN7zcy6R7XKFPDwm0s5eOQoD155khqDiUjMRRTu7n7U3U8BugHDzGxwpSEzgCx3Pxl4F3ihqucxs3FmlmtmuUVFRXWpO6l8tqqY17/YxPhzT6R3hxZBlyMiaaBGs2XcfTcwB7i00uM73L00fPdZ4LTjfP8Ud8929+yMjIxalJt8SsuOcs+0PDLbNuO283sHXY6IpIlIZstkmFmb8HJT4JvAskpjOle4OxpYGs0ik9mzH61mddF+Jo8ZRJOGagwmIvERyWyZzsALZlaf0B+Dv7n7G2Y2Gch19xzgx2Y2GigDdgJjY1VwMlm/4wC/f7+QUSd15rx+agwmIvETyWyZxcCpVTw+qcLy3cDd0S0tubk7E6eHGoNNvFyNwUQkvvQJ1RiZlbeVD1cUcefF/ejUuknQ5YhImlG4x8DeQ0e4f0Y+g7q04oYzegRdjoikIV2JKQYen72S7XtLeeb6bDUGE5FAKHmiLG9TCc9/toZ/G57JKd3bBF2OiKQphXsUHS13JkzLo23zRvynGoOJSIAU7lH0yvz1fLlhN/eMGkjrpmoMJiLBUbhHSdHeUh55axlnntiOMad0CbocEUlzCvcoeejNAkqPlPPAlYPVGExEAqdwj4LPCouZtmgz48/txYkZagwmIsFTuNfRV43BerRrxg/VGExEEoTmudfRMx+uZnXxfl74/jA1BhORhKE99zpYW7yfJz8oZNTJnTm3b3q0MBaR5KBwr6WvGoM1ql+PSWoMJiIJRuFeS28u2cLHK4v5j4v70rGVGoOJSGJRuNfC3kNHmDyjgMFdW3H9GVlBlyMicgydUK2FR99ZQdG+Up69IZv69TSnXUQSTySX2WtiZvPN7Eszyzez+6sY09jMXjWzQjObZ2ZZsSg2ESzZWMKLn6/l+hE9GKLGYCKSoCI5LFMKXODuQ4BTgEvNbESlMTcDu9y9N/A48Eh0y0wMocZgS2jbvDF3Xtwv6HJERI6r2nD3kH3huw3DN680bAzwQnj5NeBCS8HP4L88bx2LN5Yw8fIBagwmIgktohOqZlbfzBYB24HZ7j6v0pCuwAYAdy8DSoB2VTzPODPLNbPcoqKiulUeZ9v3HuI3by/nG73bM3qIGoOJSGKLKNzd/ai7nwJ0A4aZ2eBKQ6raS6+8d4+7T3H3bHfPzshIrg/9PPjGUkqPlDN5zCA1BhORhFejqZDuvhuYA1xaadVGoDuAmTUAWgM7o1BfQvhkZTE5X27m1vNOpJcag4lIEohktkyGmbUJLzcFvgksqzQsB7gxvHw18L67H7PnnowOHTnKxOl5ZLVrxq3nnRh0OSIiEYlknntn4AUzq0/oj8Hf3P0NM5sM5Lp7DvAc8JKZFRLaY78mZhXH2dMfrmJN8X5eulmNwUQkeVQb7u6+GDi1iscnVVg+BHwnuqUFb03xfv57ziquGNKFs/sk1zkCEUlvaj9wHO7OpOl5NK5fj4mjBgRdjohIjSjcj2PG4nBjsEv60UGNwUQkySjcq7Dn0BEeeKOAk7q25roRPYIuR0SkxtQ4rAqPvr2cHftK+dONp6sxmIgkJe25V7J4425enLuO60f04KRurYMuR0SkVhTuFRwtdyZMzaN9i8bceYkag4lI8lK4V/CXuetYsqmESZcPpFUTNQYTkeSlcA/bvucQ//X2cs7u057LT+4cdDkiInWicA974M2llB4tZ/KYwWoMJiJJT+EOfLSiiBlfbua283rTs33zoMsREamztA/3Q0eOMml6Hj3bN2f8eb2CLkdEJCrSfp77U3NWsXbHAf5y83AaN1BjMBFJDWm95766aB9PzVnFmFO68I0+7YMuR0QkatI23N2didPzaNywHhPUGExEUkzahnvOl5v5tHAHP7+kHx1aqjGYiKSWtAz3koNHeOCNpQzp1prvDVdjMBFJPZFcZq+7mX1gZkvNLN/M7qhizHlmVmJmi8K3SVU9V6L4r7eXs3N/KQ9ddZIag4lISopktkwZcKe7LzSzlsACM5vt7gWVxn3s7pdHv8To+nLDbv4ybx03npHF4K5qDCYiqanaPXd33+LuC8PLe4GlQNdYFxYLZUfL+eXUJWS0aMydF/cNuhwRkZip0TF3M8sidD3VeVWsPsPMvjSzWWY26DjfP87Mcs0st6ioqMbF1tVLc9eRv3kPk64YSEs1BhORFBZxuJtZC+DvwE/cfU+l1QuBHu4+BPg9MK2q53D3Ke6e7e7ZGRnxveD0tj2HePSdFZzTN4NRJ6kxmIiktojC3cwaEgr2v7r765XXu/sed98XXp4JNDSzhPpU0OQ3Cjh8tJzJowepMZiIpLxIZssY8Byw1N0fO86YTuFxmNmw8PPuiGahdfHhiiLeXLyFH53fmyw1BhORNBDJbJmzgOuBJWa2KPzYL4FMAHd/GrgauNXMyoCDwDXu7jGot8a+agzWq31z/v1cNQYTkfRQbbi7+yfA1x7HcPcngSejVVQ0/fcHhazbcYCXb1FjMBFJHyn9CdVVRft46sNVXHlKF87snVCnAEREYiplw93dmTgtjyYN6zNh1MCgyxERiauUDffpizbz2aod/PzS/mS0bBx0OSIicZWS4V5y4AgPvlnAkO5t+N6wzKDLERGJu5QM9//7zjJ27j/MQ1cOVmMwEUlLKRfuX6zfxV/nrefGM9UYTETSV0qFe9nRciZMzaNDy8b87CI1BhOR9JVS4f7i5+so2LKHe68YpMZgIpLWUibct5Yc4tF3lnNu3wwuG9wp6HJERAKVMuE++Y18ysqdyWPUGExEJCXC/YPl25m5ZCu3X9CbHu3UGExEJOnD/Z+NwTKa84Nz1BhMRAQi6wqZ0J58v5ANOw/y8g/UGExE5CtJvedeuH0fz3y0im+d2pUzT1RjMBGRryRtuLs790xbQtOG9fnlqAFBlyMiklAiuRJTdzP7wMyWmlm+md1RxRgzsyfMrNDMFpvZ0NiU+/9N/WITc1fv5K7L+tO+hRqDiYhUFMkx9zLgTndfaGYtgQVmNtvdCyqMuQzoE74NB54Kf42JkgNHeOjNpZya2YZrT1djMBGRyqrdc3f3Le6+MLy8F1gKdK00bAzwoofMBdqYWeeoVxv2yNvL2HXgMA9eOZh6agwmInKMGh1zN7Ms4FRgXqVVXYENFe5v5Ng/AFGxcP0uXp63npvO6smgLmoMJiJSlYjD3cxaAH8HfuLueyqvruJbjrlAtpmNM7NcM8stKiqqWaVh9c04u097fqrGYCIixxVRuJtZQ0LB/ld3f72KIRuB7hXudwM2Vx7k7lPcPdvdszMyMmpTL0O6t+Glm4fTonHST9EXEYmZSGbLGPAcsNTdHzvOsBzghvCsmRFAibtviWKdIiJSA5Hs/p4FXA8sMbNF4cd+CWQCuPvTwExgJFAIHABuin6pIiISqWrD3d0/oepj6hXHOHBbtIoSEZG6SdpPqIqIyPEp3EVEUpDCXUQkBSncRURSkMJdRCQFWWiiSwAvbFYErKvlt7cHiqNYTjLQNqcHbXN6qMs293D3aj8FGli414WZ5bp7dtB1xJO2OT1om9NDPLZZh2VERFKQwl1EJAUla7hPCbqAAGib04O2OT3EfJuT8pi7iIh8vWTdcxcRka+R0OFuZpea2fLwhbd/UcX6xmb2anj9vPCVopJaBNv8MzMrCF+I/D0z6xFEndFU3TZXGHe1mbmZJf3Miki22cy+G36v883s5XjXGG0R/GxnmtkHZvZF+Od7ZBB1RouZ/cnMtptZ3nHWm5k9Ef73WGxmQ6NagLsn5A2oD6wCegGNgC+BgZXG/BB4Orx8DfBq0HXHYZvPB5qFl29Nh20Oj2sJfATMBbKDrjsO73Mf4AvghPD9DkHXHYdtngLcGl4eCKwNuu46bvM5wFAg7zjrRwKzCHXdHQHMi+brJ/Ke+zCg0N1Xu/th4H8IXYi7ojHAC+Hl14ALwxcXSVbVbrO7f+DuB8J35xK66lUyi+R9BngA+A1wKJ7FxUgk2/wD4A/uvgvA3bfHucZoi2SbHWgVXm5NFVdzSybu/hGw82uGjAFe9JC5QBsz6xyt10/kcI/kotv/HOPuZUAJ0C4u1cVGTS80fjOhv/zJrNptNrNTge7u/kY8C4uhSN7nvkBfM/vUzOaa2aVxqy42Itnm+4DrzGwjoQsA3R6f0gJT09/3GknkC5FGctHtiC7MnUQi3h4zuw7IBs6NaUWx97XbbGb1gMeBsfEqKA4ieZ8bEDo0cx6h/519bGaD3X13jGuLlUi2+VrgeXd/1MzOAF4Kb3N57MsLREzzK5H33CO56PY/x5hZA0L/lfu6/wYluoguNG5m3wQmAKPdvTROtcVKddvcEhgMzDGztYSOTeYk+UnVSH+2p7v7EXdfAywnFPbJKpJtvhn4G4C7fw40IdSDJVVF9PteW4kc7v8A+phZTzNrROiEaU6lMTnAjeHlq4H3PXymIklVu83hQxTPEAr2ZD8OC9Vss7uXuHt7d89y9yxC5xlGu3tuMOVGRSQ/29MInTzHzNoTOkyzOq5VRlck27weuBDAzAYQCveiuFYZXznADeFZMyOAEnffErVnD/qMcjVnm0cCKwidZZ8QfmwyoV9uCL35/0vowtzzgV5B1xyHbX4X2AYsCt9ygq451ttcaewckny2TITvswGPAQXAEuCaoGuOwzYPBD4lNJNmEXBx0DXXcXtfAbYARwjtpd8MjAfGV3iP/xD+91gS7Z9rfUJVRCQFJfJhGRERqSWFu4hIClK4i4ikIIW7iEgKUriLiKQghbuISApSuIuIpCCFu4hICvp/vn71J5pgKukAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The value of the optimal policy using policy iteration is [ 0.449821    0.          0.44955302 -0.2562883  -0.24252309  0.43397729\n",
      " -0.27238976 -0.37937403 -6.57526925 -0.39313409  0.        ]:\n",
      "The optimal policy using policy iteration is [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "The number of epochs for convergence are 5\n",
      "The optimal policy using value iteration is [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "The number of epocs for convergence is 5\n"
     ]
    }
   ],
   "source": [
    "grid = GridWorld()\n",
    "\n",
    "### Question 1 : Change the policy here:\n",
    "Policy= np.zeros((grid.state_size, grid.action_size))\n",
    "Policy = Policy + 0.25\n",
    "print(\"The Policy is : {}\".format(Policy))\n",
    "\n",
    "val, epochs = grid.policy_evaluation(Policy,0.001,0.3)\n",
    "print(\"The value of that policy is :{}\".format(val))\n",
    "print(\"It took {} epochs\".format(epochs))\n",
    "\n",
    "\n",
    "gamma_range = [0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]\n",
    "epochs_needed = []\n",
    "for gamma in gamma_range:\n",
    "    val, epochs = grid.policy_evaluation(Policy,0.001,gamma)\n",
    "    epochs_needed.append(epochs)\n",
    "    \n",
    "plt.figure()\n",
    "plt.plot(gamma_range,epochs_needed)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "V_opt, pol_opt, epochs = grid.policy_iteration()\n",
    "print(\"The value of the optimal policy using policy iteration is {}:\".format(V_opt))\n",
    "print(\"The optimal policy using policy iteration is {}\".format(pol_opt))\n",
    "print(\"The number of epochs for convergence are {}\".format(epochs))\n",
    "\n",
    "\n",
    "pol_opt2, epochs = grid.value_iteration()\n",
    "print(\"The optimal policy using value iteration is {}\".format(pol_opt2))\n",
    "print(\"The number of epocs for convergence is {}\".format(epochs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# OPTIMAL POLICY VIA POLICY ITERATION\n",
    "Policy_POLIT = np.array([np.argmax(pol_opt[row,:]) for row in range(grid.state_size)])\n",
    "\n",
    "\n",
    "grid.draw_deterministic_policy(Policy_POLIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
