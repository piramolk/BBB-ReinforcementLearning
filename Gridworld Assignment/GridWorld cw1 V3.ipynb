{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld(object):\n",
    "    def __init__(self):\n",
    "        \n",
    "        ### Attributes defining the Gridworld #######\n",
    "        # Shape of the gridworld\n",
    "        self.shape = (4,4)\n",
    "        \n",
    "        # Locations of the obstacles\n",
    "        self.obstacle_locs = [(1,2),(2,0),(3,0),(3,1),(3,3)]\n",
    "        \n",
    "        # Locations for the absorbing states\n",
    "        self.absorbing_locs = [(0,1),(3,2)]\n",
    "        \n",
    "        # Rewards for each of the absorbing states \n",
    "        self.special_rewards = [10, -100] #corresponds to each of the absorbing_locs\n",
    "        \n",
    "        # Reward for all the other states\n",
    "        self.default_reward = -1\n",
    "        \n",
    "        # Starting location\n",
    "        self.starting_loc = (0,0)\n",
    "        \n",
    "        # Action names\n",
    "        self.action_names = ['N','E','S','W']\n",
    "        \n",
    "        # Number of actions\n",
    "        self.action_size = len(self.action_names)\n",
    "        \n",
    "        \n",
    "        # Randomizing action results: [1 0 0 0] to no Noise in the action results.\n",
    "        self.action_randomizing_array = [1, 0, 0, 0]\n",
    "        \n",
    "        ############################################\n",
    "    \n",
    "        #### Internal State  ####    \n",
    "    \n",
    "        # Get attributes defining the world\n",
    "        state_size, T, R, pi, absorbing, locs = self.build_grid_world()\n",
    "        \n",
    "        # Number of valid states in the gridworld (there are 11 of them)\n",
    "        self.state_size = state_size\n",
    "        \n",
    "        # Transition operator (3D tensor)\n",
    "        self.T = T\n",
    "        \n",
    "        # Reward function (3D tensor)\n",
    "        self.R = R\n",
    "        \n",
    "        # Probability function for stochasticity (2D tensor)\n",
    "        self.pi = pi\n",
    "        \n",
    "        # Absorbing states\n",
    "        self.absorbing = absorbing\n",
    "        \n",
    "        # The locations of the valid states\n",
    "        self.locs = locs\n",
    "        \n",
    "        # Number of the starting state\n",
    "        self.starting_state = self.loc_to_state(self.starting_loc, locs);\n",
    "        \n",
    "        # Locating the initial state\n",
    "        self.initial = np.zeros((1,len(locs)));\n",
    "        self.initial[0,self.starting_state] = 1\n",
    "        \n",
    "        \n",
    "        # Placing the walls on a bitmap\n",
    "        self.walls = np.zeros(self.shape);\n",
    "        for ob in self.obstacle_locs:\n",
    "            self.walls[ob]=1\n",
    "            \n",
    "        # Placing the absorbers on a grid for illustration\n",
    "        self.absorbers = np.zeros(self.shape)\n",
    "        for ab in self.absorbing_locs:\n",
    "            self.absorbers[ab] = -1\n",
    "        \n",
    "        # Placing the rewarders on a grid for illustration\n",
    "        self.rewarders = np.zeros(self.shape)\n",
    "        for i, rew in enumerate(self.absorbing_locs):\n",
    "            self.rewarders[rew] = self.special_rewards[i]\n",
    "        \n",
    "        #Illustrating the grid world\n",
    "        self.paint_maps()\n",
    "        ################################\n",
    "    \n",
    "    \n",
    "\n",
    "    ####### Getters ###########\n",
    "    \n",
    "    def get_transition_matrix(self):\n",
    "        return self.T\n",
    "    \n",
    "    def get_reward_matrix(self):\n",
    "        return self.R\n",
    "    \n",
    "    def get_probability_matrix(self):\n",
    "        return self.pi\n",
    "    \n",
    "    \n",
    "    ########################\n",
    "    \n",
    "    ####### Methods #########\n",
    "    \n",
    "    \n",
    "    def value_iteration(self, discount = 0.3, threshold = 0.0001):\n",
    "        V = np.zeros(self.state_size)\n",
    "        \n",
    "        T = self.get_transition_matrix()\n",
    "        R = self.get_reward_matrix()\n",
    "        pi = self.get_probability_matrix()\n",
    "        \n",
    "        epochs = 0\n",
    "        while True:\n",
    "            epochs+=1\n",
    "            delta = 0\n",
    "\n",
    "            for state_idx in range(self.state_size):\n",
    "                if(self.absorbing[0,state_idx]):\n",
    "                    continue \n",
    "                    \n",
    "                v = V[state_idx]\n",
    "\n",
    "                Q = np.zeros(4)\n",
    "                \n",
    "                # store the value of the probability for that state\n",
    "                pi_valit1 = pi[state_idx,:]\n",
    "                for state_idx_prime in range(self.state_size):\n",
    "                    Q +=  pi_valit1*T[state_idx_prime,state_idx,:] * (R[state_idx_prime,state_idx, :] + discount* V[state_idx_prime])\n",
    "                    ########### added P\n",
    "                    \n",
    "                V[state_idx]= np.max(Q)\n",
    "                delta = max(delta,np.abs(v - V[state_idx]))\n",
    "                \n",
    "            if(delta<threshold):\n",
    "                optimal_policy = np.zeros((self.state_size, self.action_size))\n",
    "                for state_idx in range(self.state_size):\n",
    "                    Q = np.zeros(4)\n",
    "                    pi_valit2 = pi[state_idx,:]\n",
    "                    for state_idx_prime in range(self.state_size):\n",
    "                        Q += pi_valit2*T[state_idx_prime,state_idx,:] * (R[state_idx_prime,state_idx, :] + discount * V[state_idx_prime])\n",
    "                    \n",
    "                    optimal_policy[state_idx, np.argmax(Q)]=1\n",
    "\n",
    "                \n",
    "                return optimal_policy,epochs\n",
    "\n",
    "\n",
    "    \n",
    "    def policy_iteration(self, discount=0.3, threshold = 0.0001):\n",
    "        policy= np.zeros((self.state_size, self.action_size))\n",
    "        policy[:,0] = 1\n",
    "        \n",
    "        T = self.get_transition_matrix()\n",
    "        R = self.get_reward_matrix()\n",
    "        \n",
    "        ########\n",
    "        pi = self.get_probability_matrix()\n",
    "        \n",
    "        epochs =0\n",
    "        while True: \n",
    "            V, epochs_eval = self.policy_evaluation(policy, threshold, discount)\n",
    "            \n",
    "            epochs+=epochs_eval\n",
    "            \n",
    "            #Policy iteration\n",
    "            policy_stable = True\n",
    "            \n",
    "            for state_idx in range(policy.shape[0]):\n",
    "                if(self.absorbing[0,state_idx]):\n",
    "                    continue \n",
    "                    \n",
    "                old_action = np.argmax(policy[state_idx,:])\n",
    "                \n",
    "                Q = np.zeros(4)\n",
    "                \n",
    "                # store the value of the probability for that state\n",
    "                pi_polit = pi[state_idx,:]\n",
    "                \n",
    "                for state_idx_prime in range(policy.shape[0]):\n",
    "                    Q +=  pi_polit* T[state_idx_prime,state_idx,:] * (R[state_idx_prime,state_idx, :] + discount * V[state_idx_prime])\n",
    "                \n",
    "                new_policy = np.zeros(4)\n",
    "                new_policy[np.argmax(Q)]=1\n",
    "                policy[state_idx] = new_policy\n",
    "                \n",
    "                if(old_action !=np.argmax(policy[state_idx])):\n",
    "                    policy_stable = False\n",
    "            \n",
    "            if(policy_stable):\n",
    "                return V, policy,epochs\n",
    "                \n",
    "    \n",
    "    def policy_evaluation(self, policy, threshold, discount):\n",
    "        \n",
    "        # Make sure delta is bigger than the threshold to start with\n",
    "        delta= 2*threshold\n",
    "        \n",
    "        #Get the reward and transition matrices\n",
    "        R = self.get_reward_matrix()\n",
    "        T = self.get_transition_matrix()\n",
    "        \n",
    "        \n",
    "        ###Get probability matrix pi\n",
    "        pi = self.get_probability_matrix()\n",
    "\n",
    "        \n",
    "        # The value is initialised at 0\n",
    "        V = np.zeros(policy.shape[0])\n",
    "        # Make a deep copy of the value array to hold the update during the evaluation\n",
    "        Vnew = np.copy(V)\n",
    "        \n",
    "        epoch = 0\n",
    "        # While the Value has not yet converged do:\n",
    "        while delta>threshold:\n",
    "            epoch += 1\n",
    "            for state_idx in range(policy.shape[0]):\n",
    "                # If it is one of the absorbing states, ignore\n",
    "                if(self.absorbing[0,state_idx]):\n",
    "                    continue   \n",
    "                \n",
    "                # Accumulator variable for the Value of a state\n",
    "                tmpV = 0\n",
    "                for action_idx in range(policy.shape[1]):\n",
    "                    # Accumulator variable for the State-Action Value\n",
    "                    tmpQ = 0\n",
    "                    for state_idx_prime in range(policy.shape[0]):\n",
    "                        tmpQ = tmpQ + pi[state_idx,action_idx]*T[state_idx_prime,state_idx,action_idx] * (R[state_idx_prime,state_idx, action_idx] + discount * V[state_idx_prime])\n",
    "                    \n",
    "                    tmpV += policy[state_idx,action_idx] * tmpQ\n",
    "                    \n",
    "                # Update the value of the state\n",
    "                Vnew[state_idx] = tmpV\n",
    "            \n",
    "            # After updating the values of all states, update the delta\n",
    "            delta =  max(abs(Vnew-V))\n",
    "            # and save the new value into the old\n",
    "            V=np.copy(Vnew)\n",
    "            \n",
    "        return V, epoch\n",
    "    \n",
    "    def draw_stochastic_policy(self, Policy):\n",
    "        # Draw a stochastic policy\n",
    "        # The policy needs to be a np array of 11 values between 0 and 3 with\n",
    "        # 0 -> N, 1->E, 2->S, 3->W\n",
    "        plt.figure()\n",
    "        plt.imshow(self.walls+ self.rewarders + self.absorbers)\n",
    "        plt.imshow(self.walls)\n",
    "        \n",
    "        \n",
    "        for state, action in enumerate(Policy):\n",
    "            if(self.absorbing[0,state]):\n",
    "                continue\n",
    "            arrows = [r\"$\\uparrow$\",r\"$\\rightarrow$\", r\"$\\downarrow$\", r\"$\\leftarrow$\"]\n",
    "            action_arrow = arrows[action]\n",
    "            location = self.locs[state]\n",
    "            plt.text(location[1], location[0], action_arrow, ha='center', va='center', color='w', size='40')\n",
    "    \n",
    "        plt.show()\n",
    "    ##########################\n",
    "    \n",
    "    \n",
    "    ########### Internal Helper Functions #####################\n",
    "    def paint_maps(self):\n",
    "        plt.figure()\n",
    "        plt.subplot(1,3,1)\n",
    "        plt.imshow(self.walls)\n",
    "        plt.subplot(1,3,2)\n",
    "        plt.imshow(self.absorbers)\n",
    "        plt.subplot(1,3,3)\n",
    "        plt.imshow(self.rewarders) \n",
    "        plt.show()\n",
    "        \n",
    "    def build_grid_world(self):\n",
    "        # Get the locations of all the valid states, the neighbours of each state (by state number),\n",
    "        # and the absorbing states (array of 0's with ones in the absorbing states)\n",
    "        locations, neighbours, absorbing_states = self.get_topology()\n",
    "        \n",
    "        # Get the number of states\n",
    "        S = len(locations)\n",
    "        \n",
    "        # Initialise the transition matrix\n",
    "        T = np.zeros((S,S,4))\n",
    "        \n",
    "        ## Initialise the probability matrix\n",
    "        pi = np.zeros((S,4))  ### add\n",
    "        \n",
    "        ##probability matrix is based on action-state pairs\n",
    "        ##if the action and outcome is the samethen p=0.3\n",
    "        \n",
    "        ### You can change this value to see the effect of changing p\n",
    "        ###### Probability p refers to the probability that the selected action results in the corresponding outcome\n",
    "        p_constant = 0.3\n",
    "        \n",
    "        for action in range(4):\n",
    "            for effect in range(4):\n",
    "                \n",
    "                # Randomize the outcome of taking an action. \n",
    "\n",
    "                outcome = (action+effect+1) % 4\n",
    "                if outcome == 0:\n",
    "                    outcome = 3\n",
    "                else:\n",
    "                    outcome -= 1\n",
    "    \n",
    "                # Fill the probability and transition matrix\n",
    "                for prior_state in range(S):\n",
    "                    post_state = neighbours[prior_state, outcome]\n",
    "                    post_state = int(post_state)\n",
    "                    \n",
    "                    if action == outcome: \n",
    "                        pi[prior_state,action] = p_constant\n",
    "                        prob = pi[prior_state,action]\n",
    "                    else:\n",
    "                        pi[prior_state,action] = (1-p_constant)/3\n",
    "                        prob = pi[prior_state,action]\n",
    "                        \n",
    "                    T[post_state,prior_state,action] = T[post_state,prior_state,action]+prob\n",
    "                \n",
    "    \n",
    "        # Build the reward matrix\n",
    "        R = self.default_reward*np.ones((S,S,4))\n",
    "        for i, sr in enumerate(self.special_rewards):\n",
    "            post_state = self.loc_to_state(self.absorbing_locs[i],locations)\n",
    "            R[post_state,:,:]= sr\n",
    "        \n",
    "        return S, T, R, pi, absorbing_states,locations\n",
    "    \n",
    "    def get_topology(self):\n",
    "        height = self.shape[0]\n",
    "        width = self.shape[1]\n",
    "        \n",
    "        index = 1 \n",
    "        locs = []\n",
    "        neighbour_locs = []\n",
    "        \n",
    "        for i in range(height):\n",
    "            for j in range(width):\n",
    "                # Get the locaiton of each state\n",
    "                loc = (i,j)\n",
    "                \n",
    "                #And append it to the valid state locations if it is a valid state (ie not absorbing)\n",
    "                if(self.is_location(loc)):\n",
    "                    locs.append(loc)\n",
    "                    \n",
    "                    # Get an array with the neighbours of each state, in terms of locations\n",
    "                    local_neighbours = [self.get_neighbour(loc,direction) for direction in ['nr','ea','so', 'we']]\n",
    "                    neighbour_locs.append(local_neighbours)\n",
    "                \n",
    "        # translate neighbour lists from locations to states\n",
    "        num_states = len(locs)\n",
    "        state_neighbours = np.zeros((num_states,4))\n",
    "        \n",
    "        for state in range(num_states):\n",
    "            for direction in range(4):\n",
    "                # Find neighbour location\n",
    "                nloc = neighbour_locs[state][direction]\n",
    "                \n",
    "                # Turn location into a state number\n",
    "                nstate = self.loc_to_state(nloc,locs)\n",
    "      \n",
    "                # Insert into neighbour matrix\n",
    "                state_neighbours[state,direction] = nstate;\n",
    "                \n",
    "    \n",
    "        # Translate absorbing locations into absorbing state indices\n",
    "        absorbing = np.zeros((1,num_states))\n",
    "        for a in self.absorbing_locs:\n",
    "            absorbing_state = self.loc_to_state(a,locs)\n",
    "            absorbing[0,absorbing_state] =1\n",
    "        \n",
    "        return locs, state_neighbours, absorbing \n",
    "\n",
    "    def loc_to_state(self,loc,locs):\n",
    "        #takes list of locations and gives index corresponding to input loc\n",
    "        return locs.index(tuple(loc))\n",
    "        #return locs.index(loc)\n",
    "\n",
    "\n",
    "    def is_location(self, loc):\n",
    "        # It is a valid location if it is in grid and not obstacle\n",
    "        if(loc[0]<0 or loc[1]<0 or loc[0]>self.shape[0]-1 or loc[1]>self.shape[1]-1):\n",
    "            return False\n",
    "        elif(loc in self.obstacle_locs):\n",
    "            return False\n",
    "        else:\n",
    "             return True\n",
    "            \n",
    "    def get_neighbour(self,loc,direction):\n",
    "        #Find the valid neighbours (ie that are in the grif and not obstacle)\n",
    "        i = loc[0]\n",
    "        j = loc[1]\n",
    "        \n",
    "        nr = (i-1,j)\n",
    "        ea = (i,j+1)\n",
    "        so = (i+1,j)\n",
    "        we = (i,j-1)\n",
    "        \n",
    "        # If the neighbour is a valid location, accept it, otherwise, stay put\n",
    "        if(direction == 'nr' and self.is_location(nr)):\n",
    "            return nr\n",
    "        elif(direction == 'ea' and self.is_location(ea)):\n",
    "            return ea\n",
    "        elif(direction == 'so' and self.is_location(so)):\n",
    "            return so\n",
    "        elif(direction == 'we' and self.is_location(we)):\n",
    "            return we\n",
    "        else:\n",
    "            #default is to return to the same location\n",
    "            return loc\n",
    "        \n",
    "###########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Policy is : [[0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]]\n",
      "The value of that policy is :[ 0.43932108  0.          0.43748854 -0.25031864 -0.21465092  0.39019043\n",
      " -0.30110003 -0.56456702 -6.35943795 -0.60014173  0.        ]\n",
      "It took 6 epochs\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The value of the optimal policy using policy iteration is [ 0.57858039  0.          0.57669573 -0.23436917 -0.19485797  0.5291782\n",
      " -0.29654476 -0.50627575 -5.93441616 -0.55692755  0.        ]:\n",
      "The optimal policy using policy iteration is [[0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "The number of epochs for convergence are 21\n",
      "The optimal policy using value iteration is [[0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "The number of epocs for convergence is 6\n"
     ]
    }
   ],
   "source": [
    "grid = GridWorld()\n",
    "\n",
    "Policy= np.zeros((grid.state_size, grid.action_size))\n",
    "Policy = Policy + 0.25\n",
    "print(\"The Policy is : {}\".format(Policy))\n",
    "\n",
    "val, epochs = grid.policy_evaluation(Policy,0.001,0.3)\n",
    "print(\"The value of that policy is :{}\".format(val))\n",
    "print(\"It took {} epochs\".format(epochs))\n",
    "\n",
    "\n",
    "gamma_range = [0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]\n",
    "epochs_needed = []\n",
    "for gamma in gamma_range:\n",
    "    val, epochs = grid.policy_evaluation(Policy,0.001,gamma)\n",
    "    epochs_needed.append(epochs)\n",
    "    \n",
    "plt.figure()\n",
    "plt.plot(gamma_range,epochs_needed)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "V_opt, pol_opt, epochs = grid.policy_iteration()\n",
    "print(\"The value of the optimal policy using policy iteration is {}:\".format(V_opt))\n",
    "print(\"The optimal policy using policy iteration is {}\".format(pol_opt))\n",
    "print(\"The number of epochs for convergence are {}\".format(epochs))\n",
    "\n",
    "\n",
    "pol_opt2, epochs = grid.value_iteration()\n",
    "print(\"The optimal policy using value iteration is {}\".format(pol_opt2))\n",
    "print(\"The number of epocs for convergence is {}\".format(epochs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# OPTIMAL POLICY VIA POLICY ITERATION\n",
    "Policy_POLIT = np.array([np.argmax(pol_opt[row,:]) for row in range(grid.state_size)])\n",
    "\n",
    "\n",
    "grid.draw_stochastic_policy(Policy_POLIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
