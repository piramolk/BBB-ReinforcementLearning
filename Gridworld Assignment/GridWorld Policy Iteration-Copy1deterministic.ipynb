{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld(object):\n",
    "    def __init__(self):\n",
    "        \n",
    "        ### Attributes defining the Gridworld #######\n",
    "        # Shape of the gridworld\n",
    "        self.shape = (4,4)\n",
    "        \n",
    "        # Locations of the obstacles\n",
    "        self.obstacle_locs = [(1,2),(2,0),(3,0),(3,1),(3,3)]\n",
    "        \n",
    "        # Locations for the absorbing states\n",
    "        self.absorbing_locs = [(0,1),(3,2)]\n",
    "        \n",
    "        # Rewards for each of the absorbing states \n",
    "        self.special_rewards = [10, -100] #corresponds to each of the absorbing_locs\n",
    "        \n",
    "        # Reward for all the other states\n",
    "        self.default_reward = -1\n",
    "        \n",
    "        # Starting location\n",
    "        self.starting_loc = (0,0)\n",
    "        \n",
    "        # Action names\n",
    "        self.action_names = ['N','E','S','W']\n",
    "        \n",
    "        # Number of actions\n",
    "        self.action_size = len(self.action_names)\n",
    "        \n",
    "        \n",
    "        # Randomizing action results: [1 0 0 0] to no Noise in the action results.\n",
    "        self.action_randomizing_array = [1, 0, 0 , 0]\n",
    "        \n",
    "        ############################################\n",
    "    \n",
    "    \n",
    "    \n",
    "        #### Internal State  ####\n",
    "        \n",
    "    \n",
    "        # Get attributes defining the world\n",
    "        state_size, T, R, absorbing, locs = self.build_grid_world()\n",
    "        \n",
    "        # Number of valid states in the gridworld (there are 11 of them)\n",
    "        self.state_size = state_size\n",
    "        \n",
    "        # Transition operator (3D tensor)\n",
    "        self.T = T\n",
    "        \n",
    "        # Reward function (3D tensor)\n",
    "        self.R = R\n",
    "        \n",
    "        \n",
    "        # Probability function for stochasticity (2D tensor)\n",
    "        #self.pi = pi\n",
    "        \n",
    "        # Absorbing states\n",
    "        self.absorbing = absorbing\n",
    "        \n",
    "        # The locations of the valid states\n",
    "        self.locs = locs\n",
    "        \n",
    "        # Number of the starting state\n",
    "        self.starting_state = self.loc_to_state(self.starting_loc, locs);\n",
    "        \n",
    "        # Locating the initial state\n",
    "        self.initial = np.zeros((1,len(locs)));\n",
    "        self.initial[0,self.starting_state] = 1\n",
    "        \n",
    "        \n",
    "        # Placing the walls on a bitmap\n",
    "        self.walls = np.zeros(self.shape);\n",
    "        for ob in self.obstacle_locs:\n",
    "            self.walls[ob]=1\n",
    "            \n",
    "        # Placing the absorbers on a grid for illustration\n",
    "        self.absorbers = np.zeros(self.shape)\n",
    "        for ab in self.absorbing_locs:\n",
    "            self.absorbers[ab] = -1\n",
    "        \n",
    "        # Placing the rewarders on a grid for illustration\n",
    "        self.rewarders = np.zeros(self.shape)\n",
    "        for i, rew in enumerate(self.absorbing_locs):\n",
    "            self.rewarders[rew] = self.special_rewards[i]\n",
    "        \n",
    "        #Illustrating the grid world\n",
    "        self.paint_maps()\n",
    "        ################################\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ####### Getters ###########\n",
    "    \n",
    "    def get_transition_matrix(self):\n",
    "        return self.T\n",
    "    \n",
    "    def get_reward_matrix(self):\n",
    "        return self.R\n",
    "    \n",
    "  #  def get_probability_matrix(self):\n",
    "       # return self.pi\n",
    "    \n",
    "    \n",
    "    ########################\n",
    "    \n",
    "    ####### Methods #########\n",
    "    \n",
    "    \n",
    "    def value_iteration(self, discount = 0.3, threshold = 0.0001):\n",
    "        V = np.zeros(self.state_size)\n",
    "        \n",
    "        T = self.get_transition_matrix()\n",
    "        R = self.get_reward_matrix()\n",
    "        \n",
    "        ####\n",
    "        #pi = self.get_probability_matrix()\n",
    "        \n",
    "        epochs = 0\n",
    "        while True:\n",
    "            epochs+=1\n",
    "            delta = 0\n",
    "\n",
    "            for state_idx in range(self.state_size):\n",
    "                if(self.absorbing[0,state_idx]):\n",
    "                    continue \n",
    "                    \n",
    "                v = V[state_idx]\n",
    "\n",
    "                Q = np.zeros(4)\n",
    "                for state_idx_prime in range(self.state_size):\n",
    "                    Q +=  T[state_idx_prime,state_idx,:] * (R[state_idx_prime,state_idx, :] + discount * V[state_idx_prime])\n",
    "                    ########### added P\n",
    "                    \n",
    "                V[state_idx]= np.max(Q)\n",
    "                delta = max(delta,np.abs(v - V[state_idx]))\n",
    "                \n",
    "            if(delta<threshold):\n",
    "                optimal_policy = np.zeros((self.state_size, self.action_size))\n",
    "                for state_idx in range(self.state_size):\n",
    "                    Q = np.zeros(4)\n",
    "                    for state_idx_prime in range(self.state_size):\n",
    "                        Q += T[state_idx_prime,state_idx,:] * (R[state_idx_prime,state_idx, :] + discount * V[state_idx_prime])\n",
    "                    \n",
    "                    optimal_policy[state_idx, np.argmax(Q)]=1\n",
    "\n",
    "                \n",
    "                return optimal_policy,epochs\n",
    "\n",
    "\n",
    "    \n",
    "    def policy_iteration(self, discount=0.3, threshold = 0.0001):\n",
    "        policy= np.zeros((self.state_size, self.action_size))\n",
    "        policy[:,0] = 1\n",
    "        \n",
    "        T = self.get_transition_matrix()\n",
    "        R = self.get_reward_matrix()\n",
    "        \n",
    "        ########\n",
    "       # pi = self.get_probability_matrix()\n",
    "        \n",
    "        epochs =0\n",
    "        while True: \n",
    "            V, epochs_eval = self.policy_evaluation(policy, threshold, discount)\n",
    "            \n",
    "            epochs+=epochs_eval\n",
    "            #Policy iteration\n",
    "            policy_stable = True\n",
    "            \n",
    "            for state_idx in range(policy.shape[0]):\n",
    "                if(self.absorbing[0,state_idx]):\n",
    "                    continue \n",
    "                    \n",
    "                old_action = np.argmax(policy[state_idx,:])\n",
    "                \n",
    "                Q = np.zeros(4)\n",
    "                for state_idx_prime in range(policy.shape[0]):\n",
    "                    Q += T[state_idx_prime,state_idx,:] * (R[state_idx_prime,state_idx, :] + discount * V[state_idx_prime])\n",
    "                   \n",
    "                ####### Added P, 2D vector consisting of pi(s,a)\n",
    "                \n",
    "                new_policy = np.zeros(4)\n",
    "                new_policy[np.argmax(Q)]=1\n",
    "                policy[state_idx] = new_policy\n",
    "                \n",
    "                if(old_action !=np.argmax(policy[state_idx])):\n",
    "                    policy_stable = False\n",
    "            \n",
    "            if(policy_stable):\n",
    "                return V, policy,epochs\n",
    "                \n",
    "                \n",
    "                \n",
    "        \n",
    "    \n",
    "    def policy_evaluation(self, policy, threshold, discount):\n",
    "        \n",
    "        # Make sure delta is bigger than the threshold to start with\n",
    "        delta= 2*threshold\n",
    "        \n",
    "        #Get the reward and transition matrices\n",
    "        R = self.get_reward_matrix()\n",
    "        T = self.get_transition_matrix()\n",
    "        \n",
    "        \n",
    "        ###\n",
    "       # pi = self.get_probability_matrix()\n",
    "\n",
    "        \n",
    "        # The value is initialised at 0\n",
    "        V = np.zeros(policy.shape[0])\n",
    "        # Make a deep copy of the value array to hold the update during the evaluation\n",
    "        Vnew = np.copy(V)\n",
    "        \n",
    "        epoch = 0\n",
    "        # While the Value has not yet converged do:\n",
    "        while delta>threshold:\n",
    "            epoch += 1\n",
    "            for state_idx in range(policy.shape[0]):\n",
    "                # If it is one of the absorbing states, ignore\n",
    "                if(self.absorbing[0,state_idx]):\n",
    "                    continue   \n",
    "                \n",
    "                # Accumulator variable for the Value of a state\n",
    "                tmpV = 0\n",
    "                for action_idx in range(policy.shape[1]):\n",
    "                    # Accumulator variable for the State-Action Value\n",
    "                    tmpQ = 0\n",
    "                    for state_idx_prime in range(policy.shape[0]):\n",
    "                        tmpQ = tmpQ +T[state_idx_prime,state_idx,action_idx] * (R[state_idx_prime,state_idx, action_idx] + discount * V[state_idx_prime])\n",
    "                    \n",
    "                    tmpV += policy[state_idx,action_idx] * tmpQ\n",
    "                    \n",
    "                # Update the value of the state\n",
    "                Vnew[state_idx] = tmpV\n",
    "            \n",
    "            # After updating the values of all states, update the delta\n",
    "            delta =  max(abs(Vnew-V))\n",
    "            # and save the new value into the old\n",
    "            V=np.copy(Vnew)\n",
    "            \n",
    "        return V, epoch\n",
    "    \n",
    "    def draw_deterministic_policy(self, Policy):\n",
    "        # Draw a deterministic policy\n",
    "        # The policy needs to be a np array of 11 values between 0 and 3 with\n",
    "        # 0 -> N, 1->E, 2->S, 3->W\n",
    "        plt.figure()\n",
    "        plt.imshow(self.walls+ self.rewarders + self.absorbers)\n",
    "        plt.imshow(self.walls)\n",
    "        \n",
    "        #plt.hold('on')\n",
    "        for state, action in enumerate(Policy):\n",
    "            if(self.absorbing[0,state]):\n",
    "                continue\n",
    "            arrows = [r\"$\\uparrow$\",r\"$\\rightarrow$\", r\"$\\downarrow$\", r\"$\\leftarrow$\"]\n",
    "            action_arrow = arrows[action]\n",
    "            location = self.locs[state]\n",
    "            plt.text(location[1], location[0], action_arrow, ha='center', va='center',color='w')\n",
    "    \n",
    "        plt.show()\n",
    "    ##########################\n",
    "    \n",
    "    \n",
    "    ########### Internal Helper Functions #####################\n",
    "    def paint_maps(self):\n",
    "        plt.figure()\n",
    "        plt.subplot(1,3,1)\n",
    "        plt.imshow(self.walls)\n",
    "        plt.subplot(1,3,2)\n",
    "        plt.imshow(self.absorbers)\n",
    "        plt.subplot(1,3,3)\n",
    "        plt.imshow(self.rewarders) \n",
    "        plt.show()\n",
    "        \n",
    "    def build_grid_world(self):\n",
    "        # Get the locations of all the valid states, the neighbours of each state (by state number),\n",
    "        # and the absorbing states (array of 0's with ones in the absorbing states)\n",
    "        locations, neighbours, absorbing_states = self.get_topology()\n",
    "        \n",
    "        # Get the number of states\n",
    "        S = len(locations)\n",
    "        \n",
    "        # Initialise the transition matrix\n",
    "        T = np.zeros((S,S,4))\n",
    "        \n",
    "        ## Initialise the probability matrix\n",
    "        #pi = np.zeros((S,4))  ### add\n",
    "        \n",
    "        ##probability matrix is based on action-state pairs\n",
    "        ##if the action and outcome is the samethen p=0.3\n",
    "        #p_constant = 0.3\n",
    "        \n",
    "        for action in range(4):\n",
    "            for effect in range(4):\n",
    "                \n",
    "                # Randomize the outcome of taking an action. is it random tho......\n",
    "                \n",
    "                outcome = (action+effect+1) % 4\n",
    "                if outcome == 0:\n",
    "                    outcome = 3\n",
    "                else:\n",
    "                    outcome -= 1\n",
    "    \n",
    "                # Fill the transition matrix\n",
    "                prob = self.action_randomizing_array[effect]\n",
    "                for prior_state in range(S):\n",
    "                    post_state = neighbours[prior_state, outcome]\n",
    "                    post_state = int(post_state)\n",
    "                    T[post_state,prior_state,action] = T[post_state,prior_state,action]+prob\n",
    "                \n",
    "                   \n",
    "        # Build the reward matrix\n",
    "        R = self.default_reward*np.ones((S,S,4))\n",
    "        for i, sr in enumerate(self.special_rewards):\n",
    "            post_state = self.loc_to_state(self.absorbing_locs[i],locations)\n",
    "            R[post_state,:,:]= sr\n",
    "        \n",
    "        return S, T, R, absorbing_states,locations\n",
    "    \n",
    "    def get_topology(self):\n",
    "        height = self.shape[0]\n",
    "        width = self.shape[1]\n",
    "        \n",
    "        index = 1 \n",
    "        locs = []\n",
    "        neighbour_locs = []\n",
    "        \n",
    "        for i in range(height):\n",
    "            for j in range(width):\n",
    "                # Get the locaiton of each state\n",
    "                loc = (i,j)\n",
    "                \n",
    "                #And append it to the valid state locations if it is a valid state (ie not absorbing)\n",
    "                if(self.is_location(loc)):\n",
    "                    locs.append(loc)\n",
    "                    \n",
    "                    # Get an array with the neighbours of each state, in terms of locations\n",
    "                    local_neighbours = [self.get_neighbour(loc,direction) for direction in ['nr','ea','so', 'we']]\n",
    "                    neighbour_locs.append(local_neighbours)\n",
    "                \n",
    "        # translate neighbour lists from locations to states\n",
    "        num_states = len(locs)\n",
    "        state_neighbours = np.zeros((num_states,4))\n",
    "        \n",
    "        for state in range(num_states):\n",
    "            for direction in range(4):\n",
    "                # Find neighbour location\n",
    "                nloc = neighbour_locs[state][direction]\n",
    "                \n",
    "                # Turn location into a state number\n",
    "                nstate = self.loc_to_state(nloc,locs)\n",
    "      \n",
    "                # Insert into neighbour matrix\n",
    "                state_neighbours[state,direction] = nstate;\n",
    "                \n",
    "    \n",
    "        # Translate absorbing locations into absorbing state indices\n",
    "        absorbing = np.zeros((1,num_states))\n",
    "        for a in self.absorbing_locs:\n",
    "            absorbing_state = self.loc_to_state(a,locs)\n",
    "            absorbing[0,absorbing_state] =1\n",
    "        \n",
    "        return locs, state_neighbours, absorbing \n",
    "\n",
    "    def loc_to_state(self,loc,locs):\n",
    "        #takes list of locations and gives index corresponding to input loc\n",
    "        return locs.index(tuple(loc))\n",
    "        #return locs.index(loc)\n",
    "\n",
    "\n",
    "    def is_location(self, loc):\n",
    "        # It is a valid location if it is in grid and not obstacle\n",
    "        if(loc[0]<0 or loc[1]<0 or loc[0]>self.shape[0]-1 or loc[1]>self.shape[1]-1):\n",
    "            return False\n",
    "        elif(loc in self.obstacle_locs):\n",
    "            return False\n",
    "        else:\n",
    "             return True\n",
    "            \n",
    "    def get_neighbour(self,loc,direction):\n",
    "        #Find the valid neighbours (ie that are in the grif and not obstacle)\n",
    "        i = loc[0]\n",
    "        j = loc[1]\n",
    "        \n",
    "        nr = (i-1,j)\n",
    "        ea = (i,j+1)\n",
    "        so = (i+1,j)\n",
    "        we = (i,j-1)\n",
    "        \n",
    "        # If the neighbour is a valid location, accept it, otherwise, stay put\n",
    "        if(direction == 'nr' and self.is_location(nr)):\n",
    "            return nr\n",
    "        elif(direction == 'ea' and self.is_location(ea)):\n",
    "            return ea\n",
    "        elif(direction == 'so' and self.is_location(so)):\n",
    "            return so\n",
    "        elif(direction == 'we' and self.is_location(we)):\n",
    "            return we\n",
    "        else:\n",
    "            #default is to return to the same location\n",
    "            return loc\n",
    "        \n",
    "###########################################         \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Policy is : [[0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]]\n",
      "The value of that policy is :[  1.98241877   0.           1.95774192  -1.14613637  -0.86615882\n",
      "   1.53388853  -1.6151844   -3.55004181 -28.4359267   -3.82785356\n",
      "   0.        ]\n",
      "It took 8 epochs\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The value of the optimal policy using policy iteration is [10.    0.   10.    2.    2.   10.   -0.4   2.   -0.4  -1.12  0.  ]:\n",
      "The optimal policy using policy iteration is [[0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "The number of epochs for convergence are 28\n",
      "The optimal policy using value iteration is [[0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "The number of epocs for convergence is 3\n"
     ]
    }
   ],
   "source": [
    "grid = GridWorld()\n",
    "\n",
    "### Question 1 : Change the policy here:\n",
    "Policy= np.zeros((grid.state_size, grid.action_size))\n",
    "Policy = Policy + 0.25\n",
    "print(\"The Policy is : {}\".format(Policy))\n",
    "\n",
    "val, epochs = grid.policy_evaluation(Policy,0.001,0.3)\n",
    "print(\"The value of that policy is :{}\".format(val))\n",
    "print(\"It took {} epochs\".format(epochs))\n",
    "\n",
    "\n",
    "gamma_range = [0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]\n",
    "epochs_needed = []\n",
    "for gamma in gamma_range:\n",
    "    val, epochs = grid.policy_evaluation(Policy,0.001,gamma)\n",
    "    epochs_needed.append(epochs)\n",
    "    \n",
    "plt.figure()\n",
    "plt.plot(gamma_range,epochs_needed)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "V_opt, pol_opt, epochs = grid.policy_iteration()\n",
    "print(\"The value of the optimal policy using policy iteration is {}:\".format(V_opt))\n",
    "print(\"The optimal policy using policy iteration is {}\".format(pol_opt))\n",
    "print(\"The number of epochs for convergence are {}\".format(epochs))\n",
    "\n",
    "\n",
    "pol_opt2, epochs = grid.value_iteration()\n",
    "print(\"The optimal policy using value iteration is {}\".format(pol_opt2))\n",
    "print(\"The number of epocs for convergence is {}\".format(epochs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Using draw_deterministic_policy to illustrate some arbitracy policy.\n",
    "Policy2 = np.array([np.argmax(pol_opt[row,:]) for row in range(grid.state_size)])\n",
    "\n",
    "\n",
    "grid.draw_deterministic_policy(Policy2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Using draw_deterministic_policy to illustrate optimal policy.\n",
    "\n",
    "\n",
    "\n",
    "Optimal_Policy = np.array([np.argmax(pol_opt2[row,:]) for row in range(0,grid.state_size)])\n",
    "grid.draw_deterministic_policy(Optimal_Policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
