{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld(object):\n",
    "    def __init__(self):\n",
    "        \n",
    "        ### Attributes defining the Gridworld #######\n",
    "        # Shape of the gridworld\n",
    "        self.shape = (4,4)\n",
    "        \n",
    "        # Locations of the obstacles\n",
    "        self.obstacle_locs = [(1,2),(2,0),(3,0),(3,1),(3,3)]\n",
    "        \n",
    "        # Locations for the absorbing states\n",
    "        self.absorbing_locs = [(0,1),(3,2)]\n",
    "        \n",
    "        # Rewards for each of the absorbing states \n",
    "        self.special_rewards = [10, -100] #corresponds to each of the absorbing_locs\n",
    "        \n",
    "        # Reward for all the other states\n",
    "        self.default_reward = -1\n",
    "        \n",
    "        # Starting location\n",
    "        self.starting_loc = (0,0)\n",
    "        \n",
    "        # Action names\n",
    "        self.action_names = ['N','E','S','W']\n",
    "        \n",
    "        # Number of actions\n",
    "        self.action_size = len(self.action_names)\n",
    "        \n",
    "        \n",
    "        # Randomizing action results: [1 0 0 0] to no Noise in the action results.\n",
    "        self.action_randomizing_array = [0.7, 0.1, 0.1 , 0.1]\n",
    "        \n",
    "        ############################################\n",
    "    \n",
    "    \n",
    "    \n",
    "        #### Internal State  ####\n",
    "        \n",
    "    \n",
    "        # Get attributes defining the world\n",
    "        state_size, T, R, pi, absorbing, locs = self.build_grid_world()\n",
    "        \n",
    "        # Number of valid states in the gridworld (there are 11 of them)\n",
    "        self.state_size = state_size\n",
    "        \n",
    "        # Transition operator (3D tensor)\n",
    "        self.T = T\n",
    "        \n",
    "        # Reward function (3D tensor)\n",
    "        self.R = R\n",
    "        \n",
    "        \n",
    "        # Probability function for stochasticity (2D tensor)\n",
    "        self.pi = pi\n",
    "        \n",
    "        # Absorbing states\n",
    "        self.absorbing = absorbing\n",
    "        \n",
    "        # The locations of the valid states\n",
    "        self.locs = locs\n",
    "        \n",
    "        # Number of the starting state\n",
    "        self.starting_state = self.loc_to_state(self.starting_loc, locs);\n",
    "        \n",
    "        # Locating the initial state\n",
    "        self.initial = np.zeros((1,len(locs)));\n",
    "        self.initial[0,self.starting_state] = 1\n",
    "        \n",
    "        \n",
    "        # Placing the walls on a bitmap\n",
    "        self.walls = np.zeros(self.shape);\n",
    "        for ob in self.obstacle_locs:\n",
    "            self.walls[ob]=1\n",
    "            \n",
    "        # Placing the absorbers on a grid for illustration\n",
    "        self.absorbers = np.zeros(self.shape)\n",
    "        for ab in self.absorbing_locs:\n",
    "            self.absorbers[ab] = -1\n",
    "        \n",
    "        # Placing the rewarders on a grid for illustration\n",
    "        self.rewarders = np.zeros(self.shape)\n",
    "        for i, rew in enumerate(self.absorbing_locs):\n",
    "            self.rewarders[rew] = self.special_rewards[i]\n",
    "        \n",
    "        #Illustrating the grid world\n",
    "        self.paint_maps()\n",
    "        ################################\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ####### Getters ###########\n",
    "    \n",
    "    def get_transition_matrix(self):\n",
    "        return self.T\n",
    "    \n",
    "    def get_reward_matrix(self):\n",
    "        return self.R\n",
    "    \n",
    "    def get_probability_matrix(self):\n",
    "        return self.pi\n",
    "    \n",
    "    \n",
    "    ########################\n",
    "    \n",
    "    ####### Methods #########\n",
    "    \n",
    "    \n",
    "    def value_iteration(self, discount = 0.3, threshold = 0.0001):\n",
    "        V = np.zeros(self.state_size)\n",
    "        \n",
    "        T = self.get_transition_matrix()\n",
    "        R = self.get_reward_matrix()\n",
    "        \n",
    "        ####\n",
    "        pi = self.get_probability_matrix()\n",
    "        \n",
    "        epochs = 0\n",
    "        while True:\n",
    "            epochs+=1\n",
    "            delta = 0\n",
    "\n",
    "            for state_idx in range(self.state_size):\n",
    "                if(self.absorbing[0,state_idx]):\n",
    "                    continue \n",
    "                    \n",
    "                v = V[state_idx]\n",
    "\n",
    "                Q = np.zeros(4)\n",
    "                for state_idx_prime in range(self.state_size):\n",
    "                    Q +=  pi[state_idx,:]*T[state_idx_prime,state_idx,:] * (R[state_idx_prime,state_idx, :] + discount * V[state_idx_prime])\n",
    "                    ########### added P\n",
    "                    \n",
    "                V[state_idx]= np.max(Q)\n",
    "                delta = max(delta,np.abs(v - V[state_idx]))\n",
    "                \n",
    "            if(delta<threshold):\n",
    "                optimal_policy = np.zeros((self.state_size, self.action_size))\n",
    "                for state_idx in range(self.state_size):\n",
    "                    Q = np.zeros(4)\n",
    "                    for state_idx_prime in range(self.state_size):\n",
    "                        Q += pi[state_idx,:]*T[state_idx_prime,state_idx,:] * (R[state_idx_prime,state_idx, :] + discount * V[state_idx_prime])\n",
    "                    \n",
    "                    optimal_policy[state_idx, np.argmax(Q)]=1\n",
    "\n",
    "                \n",
    "                return optimal_policy,epochs\n",
    "\n",
    "\n",
    "    \n",
    "    def policy_iteration(self, discount=0.3, threshold = 0.0001):\n",
    "        policy= np.zeros((self.state_size, self.action_size))\n",
    "        policy[:,0] = 1\n",
    "        \n",
    "        T = self.get_transition_matrix()\n",
    "        R = self.get_reward_matrix()\n",
    "        \n",
    "        ########\n",
    "        pi = self.get_probability_matrix()\n",
    "        \n",
    "        epochs =0\n",
    "        while True: \n",
    "            V, epochs_eval = self.policy_evaluation(policy, threshold, discount)\n",
    "            \n",
    "            epochs+=epochs_eval\n",
    "            #Policy iteration\n",
    "            policy_stable = True\n",
    "            \n",
    "            for state_idx in range(policy.shape[0]):\n",
    "                if(self.absorbing[0,state_idx]):\n",
    "                    continue \n",
    "                    \n",
    "                old_action = np.argmax(policy[state_idx,:])\n",
    "                \n",
    "                Q = np.zeros(4)\n",
    "                for state_idx_prime in range(policy.shape[0]):\n",
    "                    Q += pi[state_idx,:] * T[state_idx_prime,state_idx,:] * (R[state_idx_prime,state_idx, :] + discount * V[state_idx_prime])\n",
    "                   \n",
    "                ####### Added P, 2D vector consisting of pi(s,a)\n",
    "                \n",
    "                new_policy = np.zeros(4)\n",
    "                new_policy[np.argmax(Q)]=1\n",
    "                policy[state_idx] = new_policy\n",
    "                \n",
    "                if(old_action !=np.argmax(policy[state_idx])):\n",
    "                    policy_stable = False\n",
    "            \n",
    "            if(policy_stable):\n",
    "                return V, policy,epochs\n",
    "                \n",
    "                \n",
    "                \n",
    "        \n",
    "    \n",
    "    def policy_evaluation(self, policy, threshold, discount):\n",
    "        \n",
    "        # Make sure delta is bigger than the threshold to start with\n",
    "        delta= 2*threshold\n",
    "        \n",
    "        #Get the reward and transition matrices\n",
    "        R = self.get_reward_matrix()\n",
    "        T = self.get_transition_matrix()\n",
    "        \n",
    "        \n",
    "        ###\n",
    "        pi = self.get_probability_matrix()\n",
    "\n",
    "        \n",
    "        # The value is initialised at 0\n",
    "        V = np.zeros(policy.shape[0])\n",
    "        # Make a deep copy of the value array to hold the update during the evaluation\n",
    "        Vnew = np.copy(V)\n",
    "        \n",
    "        epoch = 0\n",
    "        # While the Value has not yet converged do:\n",
    "        while delta>threshold:\n",
    "            epoch += 1\n",
    "            for state_idx in range(policy.shape[0]):\n",
    "                # If it is one of the absorbing states, ignore\n",
    "                if(self.absorbing[0,state_idx]):\n",
    "                    continue   \n",
    "                \n",
    "                # Accumulator variable for the Value of a state\n",
    "                tmpV = 0\n",
    "                for action_idx in range(policy.shape[1]):\n",
    "                    # Accumulator variable for the State-Action Value\n",
    "                    tmpQ = 0\n",
    "                    for state_idx_prime in range(policy.shape[0]):\n",
    "                        tmpQ = tmpQ + pi[state_idx,action_idx]*T[state_idx_prime,state_idx,action_idx] * (R[state_idx_prime,state_idx, action_idx] + discount * V[state_idx_prime])\n",
    "                    \n",
    "                    tmpV += policy[state_idx,action_idx] * tmpQ\n",
    "                    \n",
    "                # Update the value of the state\n",
    "                Vnew[state_idx] = tmpV\n",
    "            \n",
    "            # After updating the values of all states, update the delta\n",
    "            delta =  max(abs(Vnew-V))\n",
    "            # and save the new value into the old\n",
    "            V=np.copy(Vnew)\n",
    "            \n",
    "        return V, epoch\n",
    "    \n",
    "    def draw_deterministic_policy(self, Policy):\n",
    "        # Draw a deterministic policy\n",
    "        # The policy needs to be a np array of 11 values between 0 and 3 with\n",
    "        # 0 -> N, 1->E, 2->S, 3->W\n",
    "        plt.figure()\n",
    "        plt.imshow(self.walls+ self.rewarders + self.absorbers)\n",
    "        plt.imshow(self.walls)\n",
    "        \n",
    "        #plt.hold('on')\n",
    "        for state, action in enumerate(Policy):\n",
    "            if(self.absorbing[0,state]):\n",
    "                continue\n",
    "            arrows = [r\"$\\uparrow$\",r\"$\\rightarrow$\", r\"$\\downarrow$\", r\"$\\leftarrow$\"]\n",
    "            action_arrow = arrows[action]\n",
    "            location = self.locs[state]\n",
    "            plt.text(location[1], location[0], action_arrow, ha='center', va='center', color='b', size='50')\n",
    "    \n",
    "        plt.show()\n",
    "    ##########################\n",
    "    \n",
    "    \n",
    "    ########### Internal Helper Functions #####################\n",
    "    def paint_maps(self):\n",
    "        plt.figure()\n",
    "        plt.subplot(1,3,1)\n",
    "        plt.imshow(self.walls)\n",
    "        plt.subplot(1,3,2)\n",
    "        plt.imshow(self.absorbers)\n",
    "        plt.subplot(1,3,3)\n",
    "        plt.imshow(self.rewarders) \n",
    "        plt.show()\n",
    "        \n",
    "    def build_grid_world(self):\n",
    "        # Get the locations of all the valid states, the neighbours of each state (by state number),\n",
    "        # and the absorbing states (array of 0's with ones in the absorbing states)\n",
    "        locations, neighbours, absorbing_states = self.get_topology()\n",
    "        \n",
    "        # Get the number of states\n",
    "        S = len(locations)\n",
    "        \n",
    "        # Initialise the transition matrix\n",
    "        T = np.zeros((S,S,4))\n",
    "        \n",
    "        ## Initialise the probability matrix\n",
    "        pi = np.zeros((S,4))  ### add\n",
    "        \n",
    "        ##probability matrix is based on action-state pairs\n",
    "        ##if the action and outcome is the samethen p=0.3\n",
    "        p_constant = 0.3\n",
    "        \n",
    "        for action in range(4):\n",
    "            for effect in range(4):\n",
    "                \n",
    "                # Randomize the outcome of taking an action. is it random tho......\n",
    "                \n",
    "                # outcome = \n",
    "                \n",
    "                outcome = (action+effect+1) % 4\n",
    "                if outcome == 0:\n",
    "                    outcome = 3\n",
    "                else:\n",
    "                    outcome -= 1\n",
    "    \n",
    "                # Fill the transition matrix\n",
    "                prob = self.action_randomizing_array[effect]\n",
    "                for prior_state in range(S):\n",
    "                    post_state = neighbours[prior_state, outcome]\n",
    "                    post_state = int(post_state)\n",
    "                    T[post_state,prior_state,action] = T[post_state,prior_state,action]+prob\n",
    "                \n",
    "                    ##### Fill the probability matrix\n",
    "                    if action == effect: \n",
    "                        pi[prior_state,action] = p_constant\n",
    "                    \n",
    "                    else:\n",
    "                        pi[prior_state,action] = (1-p_constant)/3\n",
    "    \n",
    "        # Build the reward matrix\n",
    "        R = self.default_reward*np.ones((S,S,4))\n",
    "        for i, sr in enumerate(self.special_rewards):\n",
    "            post_state = self.loc_to_state(self.absorbing_locs[i],locations)\n",
    "            R[post_state,:,:]= sr\n",
    "        \n",
    "        return S, T, R, pi, absorbing_states,locations\n",
    "    \n",
    "    def get_topology(self):\n",
    "        height = self.shape[0]\n",
    "        width = self.shape[1]\n",
    "        \n",
    "        index = 1 \n",
    "        locs = []\n",
    "        neighbour_locs = []\n",
    "        \n",
    "        for i in range(height):\n",
    "            for j in range(width):\n",
    "                # Get the locaiton of each state\n",
    "                loc = (i,j)\n",
    "                \n",
    "                #And append it to the valid state locations if it is a valid state (ie not absorbing)\n",
    "                if(self.is_location(loc)):\n",
    "                    locs.append(loc)\n",
    "                    \n",
    "                    # Get an array with the neighbours of each state, in terms of locations\n",
    "                    local_neighbours = [self.get_neighbour(loc,direction) for direction in ['nr','ea','so', 'we']]\n",
    "                    neighbour_locs.append(local_neighbours)\n",
    "                \n",
    "        # translate neighbour lists from locations to states\n",
    "        num_states = len(locs)\n",
    "        state_neighbours = np.zeros((num_states,4))\n",
    "        \n",
    "        for state in range(num_states):\n",
    "            for direction in range(4):\n",
    "                # Find neighbour location\n",
    "                nloc = neighbour_locs[state][direction]\n",
    "                \n",
    "                # Turn location into a state number\n",
    "                nstate = self.loc_to_state(nloc,locs)\n",
    "      \n",
    "                # Insert into neighbour matrix\n",
    "                state_neighbours[state,direction] = nstate;\n",
    "                \n",
    "    \n",
    "        # Translate absorbing locations into absorbing state indices\n",
    "        absorbing = np.zeros((1,num_states))\n",
    "        for a in self.absorbing_locs:\n",
    "            absorbing_state = self.loc_to_state(a,locs)\n",
    "            absorbing[0,absorbing_state] =1\n",
    "        \n",
    "        return locs, state_neighbours, absorbing \n",
    "\n",
    "    def loc_to_state(self,loc,locs):\n",
    "        #takes list of locations and gives index corresponding to input loc\n",
    "        return locs.index(tuple(loc))\n",
    "        #return locs.index(loc)\n",
    "\n",
    "\n",
    "    def is_location(self, loc):\n",
    "        # It is a valid location if it is in grid and not obstacle\n",
    "        if(loc[0]<0 or loc[1]<0 or loc[0]>self.shape[0]-1 or loc[1]>self.shape[1]-1):\n",
    "            return False\n",
    "        elif(loc in self.obstacle_locs):\n",
    "            return False\n",
    "        else:\n",
    "             return True\n",
    "            \n",
    "    def get_neighbour(self,loc,direction):\n",
    "        #Find the valid neighbours (ie that are in the grif and not obstacle)\n",
    "        i = loc[0]\n",
    "        j = loc[1]\n",
    "        \n",
    "        nr = (i-1,j)\n",
    "        ea = (i,j+1)\n",
    "        so = (i+1,j)\n",
    "        we = (i,j-1)\n",
    "        \n",
    "        # If the neighbour is a valid location, accept it, otherwise, stay put\n",
    "        if(direction == 'nr' and self.is_location(nr)):\n",
    "            return nr\n",
    "        elif(direction == 'ea' and self.is_location(ea)):\n",
    "            return ea\n",
    "        elif(direction == 'so' and self.is_location(so)):\n",
    "            return so\n",
    "        elif(direction == 'we' and self.is_location(we)):\n",
    "            return we\n",
    "        else:\n",
    "            #default is to return to the same location\n",
    "            return loc\n",
    "        \n",
    "###########################################         \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Policy is : [[0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]]\n",
      "The value of that policy is :[ 0.42205576  0.          0.53469987 -0.25276722 -0.24464248  0.40549623\n",
      " -0.2723888  -0.37087583 -6.31873698 -0.40203814  0.        ]\n",
      "It took 4 epochs\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The value of the optimal policy using policy iteration is [ 1.58448894  0.          2.0452249  -0.17655524 -0.14674336  1.57207295\n",
      " -0.24731843 -0.17680708 -2.57569568 -0.26722404  0.        ]:\n",
      "The optimal policy using policy iteration is [[0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "The number of epochs for convergence are 14\n",
      "The optimal policy using value iteration is [[0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "The number of epocs for convergence is 4\n"
     ]
    }
   ],
   "source": [
    "grid = GridWorld()\n",
    "\n",
    "### Question 1 : Change the policy here:\n",
    "Policy= np.zeros((grid.state_size, grid.action_size))\n",
    "Policy = Policy + 0.25\n",
    "print(\"The Policy is : {}\".format(Policy))\n",
    "\n",
    "val, epochs = grid.policy_evaluation(Policy,0.001,0.3)\n",
    "print(\"The value of that policy is :{}\".format(val))\n",
    "print(\"It took {} epochs\".format(epochs))\n",
    "\n",
    "\n",
    "gamma_range = [0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]\n",
    "epochs_needed = []\n",
    "for gamma in gamma_range:\n",
    "    val, epochs = grid.policy_evaluation(Policy,0.001,gamma)\n",
    "    epochs_needed.append(epochs)\n",
    "    \n",
    "plt.figure()\n",
    "plt.plot(gamma_range,epochs_needed)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "V_opt, pol_opt, epochs = grid.policy_iteration()\n",
    "print(\"The value of the optimal policy using policy iteration is {}:\".format(V_opt))\n",
    "print(\"The optimal policy using policy iteration is {}\".format(pol_opt))\n",
    "print(\"The number of epochs for convergence are {}\".format(epochs))\n",
    "\n",
    "\n",
    "pol_opt2, epochs = grid.value_iteration()\n",
    "print(\"The optimal policy using value iteration is {}\".format(pol_opt2))\n",
    "print(\"The number of epocs for convergence is {}\".format(epochs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Using draw_deterministic_policy to illustrate some arbitracy policy.\n",
    "Policy2 = np.array([np.argmax(pol_opt[row,:]) for row in range(grid.state_size)])\n",
    "\n",
    "\n",
    "grid.draw_deterministic_policy(Policy2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Using draw_deterministic_policy to illustrate optimal policy.\n",
    "\n",
    "\n",
    "\n",
    "Optimal_Policy = np.array([np.argmax(pol_opt2[row,:]) for row in range(0,grid.state_size)])\n",
    "grid.draw_deterministic_policy(Optimal_Policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## changing to no noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld(object):\n",
    "    def __init__(self):\n",
    "        \n",
    "        ### Attributes defining the Gridworld #######\n",
    "        # Shape of the gridworld\n",
    "        self.shape = (4,4)\n",
    "        \n",
    "        # Locations of the obstacles\n",
    "        self.obstacle_locs = [(1,2),(2,0),(3,0),(3,1),(3,3)]\n",
    "        \n",
    "        # Locations for the absorbing states\n",
    "        self.absorbing_locs = [(0,1),(3,2)]\n",
    "        \n",
    "        # Rewards for each of the absorbing states \n",
    "        self.special_rewards = [10, -100] #corresponds to each of the absorbing_locs\n",
    "        \n",
    "        # Reward for all the other states\n",
    "        self.default_reward = -1\n",
    "        \n",
    "        # Starting location\n",
    "        self.starting_loc = (0,0)\n",
    "        \n",
    "        # Action names\n",
    "        self.action_names = ['N','E','S','W']\n",
    "        \n",
    "        # Number of actions\n",
    "        self.action_size = len(self.action_names)\n",
    "        \n",
    "        \n",
    "        # Randomizing action results: [1 0 0 0] to no Noise in the action results.\n",
    "        self.action_randomizing_array = [1, 0, 0 , 0]\n",
    "        \n",
    "        ############################################\n",
    "    \n",
    "    \n",
    "    \n",
    "        #### Internal State  ####\n",
    "        \n",
    "    \n",
    "        # Get attributes defining the world\n",
    "        state_size, T, R, pi, absorbing, locs = self.build_grid_world()\n",
    "        \n",
    "        # Number of valid states in the gridworld (there are 11 of them)\n",
    "        self.state_size = state_size\n",
    "        \n",
    "        # Transition operator (3D tensor)\n",
    "        self.T = T\n",
    "        \n",
    "        # Reward function (3D tensor)\n",
    "        self.R = R\n",
    "        \n",
    "        \n",
    "        # Probability function for stochasticity (2D tensor)\n",
    "        self.pi = pi\n",
    "        \n",
    "        # Absorbing states\n",
    "        self.absorbing = absorbing\n",
    "        \n",
    "        # The locations of the valid states\n",
    "        self.locs = locs\n",
    "        \n",
    "        # Number of the starting state\n",
    "        self.starting_state = self.loc_to_state(self.starting_loc, locs);\n",
    "        \n",
    "        # Locating the initial state\n",
    "        self.initial = np.zeros((1,len(locs)));\n",
    "        self.initial[0,self.starting_state] = 1\n",
    "        \n",
    "        \n",
    "        # Placing the walls on a bitmap\n",
    "        self.walls = np.zeros(self.shape);\n",
    "        for ob in self.obstacle_locs:\n",
    "            self.walls[ob]=1\n",
    "            \n",
    "        # Placing the absorbers on a grid for illustration\n",
    "        self.absorbers = np.zeros(self.shape)\n",
    "        for ab in self.absorbing_locs:\n",
    "            self.absorbers[ab] = -1\n",
    "        \n",
    "        # Placing the rewarders on a grid for illustration\n",
    "        self.rewarders = np.zeros(self.shape)\n",
    "        for i, rew in enumerate(self.absorbing_locs):\n",
    "            self.rewarders[rew] = self.special_rewards[i]\n",
    "        \n",
    "        #Illustrating the grid world\n",
    "        self.paint_maps()\n",
    "        ################################\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ####### Getters ###########\n",
    "    \n",
    "    def get_transition_matrix(self):\n",
    "        return self.T\n",
    "    \n",
    "    def get_reward_matrix(self):\n",
    "        return self.R\n",
    "    \n",
    "    def get_probability_matrix(self):\n",
    "        return self.pi\n",
    "    \n",
    "    \n",
    "    ########################\n",
    "    \n",
    "    ####### Methods #########\n",
    "    \n",
    "    \n",
    "    def value_iteration(self, discount = 0.3, threshold = 0.0001):\n",
    "        V = np.zeros(self.state_size)\n",
    "        \n",
    "        T = self.get_transition_matrix()\n",
    "        R = self.get_reward_matrix()\n",
    "        \n",
    "        ####\n",
    "        pi = self.get_probability_matrix()\n",
    "        \n",
    "        epochs = 0\n",
    "        while True:\n",
    "            epochs+=1\n",
    "            delta = 0\n",
    "\n",
    "            for state_idx in range(self.state_size):\n",
    "                if(self.absorbing[0,state_idx]):\n",
    "                    continue \n",
    "                    \n",
    "                v = V[state_idx]\n",
    "\n",
    "                Q = np.zeros(4)\n",
    "                for state_idx_prime in range(self.state_size):\n",
    "                    Q +=  pi[state_idx,:]*T[state_idx_prime,state_idx,:] * (R[state_idx_prime,state_idx, :] + discount * V[state_idx_prime])\n",
    "                    ########### added P\n",
    "                    \n",
    "                V[state_idx]= np.max(Q)\n",
    "                delta = max(delta,np.abs(v - V[state_idx]))\n",
    "                \n",
    "            if(delta<threshold):\n",
    "                optimal_policy = np.zeros((self.state_size, self.action_size))\n",
    "                for state_idx in range(self.state_size):\n",
    "                    Q = np.zeros(4)\n",
    "                    for state_idx_prime in range(self.state_size):\n",
    "                        Q += pi[state_idx,:]*T[state_idx_prime,state_idx,:] * (R[state_idx_prime,state_idx, :] + discount * V[state_idx_prime])\n",
    "                    \n",
    "                    optimal_policy[state_idx, np.argmax(Q)]=1\n",
    "\n",
    "                \n",
    "                return optimal_policy,epochs\n",
    "\n",
    "\n",
    "    \n",
    "    def policy_iteration(self, discount=0.3, threshold = 0.0001):\n",
    "        policy= np.zeros((self.state_size, self.action_size))\n",
    "        policy[:,0] = 1\n",
    "        \n",
    "        T = self.get_transition_matrix()\n",
    "        R = self.get_reward_matrix()\n",
    "        \n",
    "        ########\n",
    "        pi = self.get_probability_matrix()\n",
    "        \n",
    "        epochs =0\n",
    "        while True: \n",
    "            V, epochs_eval = self.policy_evaluation(policy, threshold, discount)\n",
    "            \n",
    "            epochs+=epochs_eval\n",
    "            #Policy iteration\n",
    "            policy_stable = True\n",
    "            \n",
    "            for state_idx in range(policy.shape[0]):\n",
    "                if(self.absorbing[0,state_idx]):\n",
    "                    continue \n",
    "                    \n",
    "                old_action = np.argmax(policy[state_idx,:])\n",
    "                \n",
    "                Q = np.zeros(4)\n",
    "                for state_idx_prime in range(policy.shape[0]):\n",
    "                    Q += pi[state_idx,:] * T[state_idx_prime,state_idx,:] * (R[state_idx_prime,state_idx, :] + discount * V[state_idx_prime])\n",
    "                   \n",
    "                ####### Added P, 2D vector consisting of pi(s,a)\n",
    "                \n",
    "                new_policy = np.zeros(4)\n",
    "                new_policy[np.argmax(Q)]=1\n",
    "                policy[state_idx] = new_policy\n",
    "                \n",
    "                if(old_action !=np.argmax(policy[state_idx])):\n",
    "                    policy_stable = False\n",
    "            \n",
    "            if(policy_stable):\n",
    "                return V, policy,epochs\n",
    "                \n",
    "                \n",
    "                \n",
    "        \n",
    "    \n",
    "    def policy_evaluation(self, policy, threshold, discount):\n",
    "        \n",
    "        # Make sure delta is bigger than the threshold to start with\n",
    "        delta= 2*threshold\n",
    "        \n",
    "        #Get the reward and transition matrices\n",
    "        R = self.get_reward_matrix()\n",
    "        T = self.get_transition_matrix()\n",
    "        \n",
    "        \n",
    "        ###\n",
    "        pi = self.get_probability_matrix()\n",
    "\n",
    "        \n",
    "        # The value is initialised at 0\n",
    "        V = np.zeros(policy.shape[0])\n",
    "        # Make a deep copy of the value array to hold the update during the evaluation\n",
    "        Vnew = np.copy(V)\n",
    "        \n",
    "        epoch = 0\n",
    "        # While the Value has not yet converged do:\n",
    "        while delta>threshold:\n",
    "            epoch += 1\n",
    "            for state_idx in range(policy.shape[0]):\n",
    "                # If it is one of the absorbing states, ignore\n",
    "                if(self.absorbing[0,state_idx]):\n",
    "                    continue   \n",
    "                \n",
    "                # Accumulator variable for the Value of a state\n",
    "                tmpV = 0\n",
    "                for action_idx in range(policy.shape[1]):\n",
    "                    # Accumulator variable for the State-Action Value\n",
    "                    tmpQ = 0\n",
    "                    for state_idx_prime in range(policy.shape[0]):\n",
    "                        tmpQ = tmpQ + pi[state_idx,action_idx]*T[state_idx_prime,state_idx,action_idx] * (R[state_idx_prime,state_idx, action_idx] + discount * V[state_idx_prime])\n",
    "                    \n",
    "                    tmpV += policy[state_idx,action_idx] * tmpQ\n",
    "                    \n",
    "                # Update the value of the state\n",
    "                Vnew[state_idx] = tmpV\n",
    "            \n",
    "            # After updating the values of all states, update the delta\n",
    "            delta =  max(abs(Vnew-V))\n",
    "            # and save the new value into the old\n",
    "            V=np.copy(Vnew)\n",
    "            \n",
    "        return V, epoch\n",
    "    \n",
    "    def draw_deterministic_policy(self, Policy):\n",
    "        # Draw a deterministic policy\n",
    "        # The policy needs to be a np array of 11 values between 0 and 3 with\n",
    "        # 0 -> N, 1->E, 2->S, 3->W\n",
    "        plt.figure()\n",
    "        plt.imshow(self.walls+ self.rewarders + self.absorbers)\n",
    "        plt.imshow(self.walls)\n",
    "        \n",
    "        #plt.hold('on')\n",
    "        for state, action in enumerate(Policy):\n",
    "            if(self.absorbing[0,state]):\n",
    "                continue\n",
    "            arrows = [r\"$\\uparrow$\",r\"$\\rightarrow$\", r\"$\\downarrow$\", r\"$\\leftarrow$\"]\n",
    "            action_arrow = arrows[action]\n",
    "            location = self.locs[state]\n",
    "            plt.text(location[1], location[0], action_arrow, ha='center', va='center', color='b', size='50')\n",
    "    \n",
    "        plt.show()\n",
    "    ##########################\n",
    "    \n",
    "    \n",
    "    ########### Internal Helper Functions #####################\n",
    "    def paint_maps(self):\n",
    "        plt.figure()\n",
    "        plt.subplot(1,3,1)\n",
    "        plt.imshow(self.walls)\n",
    "        plt.subplot(1,3,2)\n",
    "        plt.imshow(self.absorbers)\n",
    "        plt.subplot(1,3,3)\n",
    "        plt.imshow(self.rewarders) \n",
    "        plt.show()\n",
    "        \n",
    "    def build_grid_world(self):\n",
    "        # Get the locations of all the valid states, the neighbours of each state (by state number),\n",
    "        # and the absorbing states (array of 0's with ones in the absorbing states)\n",
    "        locations, neighbours, absorbing_states = self.get_topology()\n",
    "        \n",
    "        # Get the number of states\n",
    "        S = len(locations)\n",
    "        \n",
    "        # Initialise the transition matrix\n",
    "        T = np.zeros((S,S,4))\n",
    "        \n",
    "        ## Initialise the probability matrix\n",
    "        pi = np.zeros((S,4))  ### add\n",
    "        \n",
    "        ##probability matrix is based on action-state pairs\n",
    "        ##if the action and outcome is the samethen p=0.3\n",
    "        p_constant = 0.3\n",
    "        \n",
    "        for action in range(4):\n",
    "            for effect in range(4):\n",
    "                \n",
    "                # Randomize the outcome of taking an action. is it random tho......\n",
    "                \n",
    "                # outcome = \n",
    "                \n",
    "                outcome = (action+effect+1) % 4\n",
    "                if outcome == 0:\n",
    "                    outcome = 3\n",
    "                else:\n",
    "                    outcome -= 1\n",
    "    \n",
    "                # Fill the transition matrix\n",
    "                prob = self.action_randomizing_array[effect]\n",
    "                for prior_state in range(S):\n",
    "                    post_state = neighbours[prior_state, outcome]\n",
    "                    post_state = int(post_state)\n",
    "                    T[post_state,prior_state,action] = T[post_state,prior_state,action]+prob\n",
    "                \n",
    "                    ##### Fill the probability matrix\n",
    "                    if action == effect: \n",
    "                        pi[prior_state,action] = p_constant\n",
    "                    \n",
    "                    else:\n",
    "                        pi[prior_state,action] = (1-p_constant)/3\n",
    "    \n",
    "        # Build the reward matrix\n",
    "        R = self.default_reward*np.ones((S,S,4))\n",
    "        for i, sr in enumerate(self.special_rewards):\n",
    "            post_state = self.loc_to_state(self.absorbing_locs[i],locations)\n",
    "            R[post_state,:,:]= sr\n",
    "        \n",
    "        return S, T, R, pi, absorbing_states,locations\n",
    "    \n",
    "    def get_topology(self):\n",
    "        height = self.shape[0]\n",
    "        width = self.shape[1]\n",
    "        \n",
    "        index = 1 \n",
    "        locs = []\n",
    "        neighbour_locs = []\n",
    "        \n",
    "        for i in range(height):\n",
    "            for j in range(width):\n",
    "                # Get the locaiton of each state\n",
    "                loc = (i,j)\n",
    "                \n",
    "                #And append it to the valid state locations if it is a valid state (ie not absorbing)\n",
    "                if(self.is_location(loc)):\n",
    "                    locs.append(loc)\n",
    "                    \n",
    "                    # Get an array with the neighbours of each state, in terms of locations\n",
    "                    local_neighbours = [self.get_neighbour(loc,direction) for direction in ['nr','ea','so', 'we']]\n",
    "                    neighbour_locs.append(local_neighbours)\n",
    "                \n",
    "        # translate neighbour lists from locations to states\n",
    "        num_states = len(locs)\n",
    "        state_neighbours = np.zeros((num_states,4))\n",
    "        \n",
    "        for state in range(num_states):\n",
    "            for direction in range(4):\n",
    "                # Find neighbour location\n",
    "                nloc = neighbour_locs[state][direction]\n",
    "                \n",
    "                # Turn location into a state number\n",
    "                nstate = self.loc_to_state(nloc,locs)\n",
    "      \n",
    "                # Insert into neighbour matrix\n",
    "                state_neighbours[state,direction] = nstate;\n",
    "                \n",
    "    \n",
    "        # Translate absorbing locations into absorbing state indices\n",
    "        absorbing = np.zeros((1,num_states))\n",
    "        for a in self.absorbing_locs:\n",
    "            absorbing_state = self.loc_to_state(a,locs)\n",
    "            absorbing[0,absorbing_state] =1\n",
    "        \n",
    "        return locs, state_neighbours, absorbing \n",
    "\n",
    "    def loc_to_state(self,loc,locs):\n",
    "        #takes list of locations and gives index corresponding to input loc\n",
    "        return locs.index(tuple(loc))\n",
    "        #return locs.index(loc)\n",
    "\n",
    "\n",
    "    def is_location(self, loc):\n",
    "        # It is a valid location if it is in grid and not obstacle\n",
    "        if(loc[0]<0 or loc[1]<0 or loc[0]>self.shape[0]-1 or loc[1]>self.shape[1]-1):\n",
    "            return False\n",
    "        elif(loc in self.obstacle_locs):\n",
    "            return False\n",
    "        else:\n",
    "             return True\n",
    "            \n",
    "    def get_neighbour(self,loc,direction):\n",
    "        #Find the valid neighbours (ie that are in the grif and not obstacle)\n",
    "        i = loc[0]\n",
    "        j = loc[1]\n",
    "        \n",
    "        nr = (i-1,j)\n",
    "        ea = (i,j+1)\n",
    "        so = (i+1,j)\n",
    "        we = (i,j-1)\n",
    "        \n",
    "        # If the neighbour is a valid location, accept it, otherwise, stay put\n",
    "        if(direction == 'nr' and self.is_location(nr)):\n",
    "            return nr\n",
    "        elif(direction == 'ea' and self.is_location(ea)):\n",
    "            return ea\n",
    "        elif(direction == 'so' and self.is_location(so)):\n",
    "            return so\n",
    "        elif(direction == 'we' and self.is_location(we)):\n",
    "            return we\n",
    "        else:\n",
    "            #default is to return to the same location\n",
    "            return loc\n",
    "        \n",
    "########################################### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Policy is : [[0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]]\n",
      "The value of that policy is :[ 0.40350065  0.          0.59131749 -0.25021351 -0.24601286  0.38651047\n",
      " -0.27238145 -0.36541502 -6.14791314 -0.40731925  0.        ]\n",
      "It took 4 epochs\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAHqZJREFUeJzt3Xl8VPW9//HXh30HgbATArKDohgBte51AwVtbX/a64LVcrHW2tZ7ay2Ciktrf1dtrb0q1tal1WuvFQgKKi64Aw2IkIQt7DsJS9gDIZ/fHzP2l4ZgJsnMnFnez8djHjkz55uZz2GSdw7nfOdzzN0REZHUUi/oAkREJPoU7iIiKUjhLiKSghTuIiIpSOEuIpKCFO4iIilI4S4ikoIU7iIiKUjhLiKSghoE9cLt27f3rKysoF5eRCQpLViwoNjdM6obF1i4Z2VlkZubG9TLi4gkJTNbF8k4HZYREUlBCncRkRSkcBcRSUEKdxGRFKRwFxFJQRGFu5m1MbPXzGyZmS01szMqrTcze8LMCs1ssZkNjU25IiISiUinQv4OeMvdrzazRkCzSusvA/qEb8OBp8JfRUQkANWGu5m1As4BxgK4+2HgcKVhY4AXPXTNvrnhPf3O7r4lyvWKSAr5ZGUx89fsCLqMuMvOass5fav9HFKdRLLn3gsoAv5sZkOABcAd7r6/wpiuwIYK9zeGH/uXcDezccA4gMzMzDqULSLJrnD7Xm56fj5HjjpmQVcTX+PPPTEhwr0BMBS43d3nmdnvgF8AEyuMqeqtOebK2+4+BZgCkJ2drStzi6Qpd+eeaXk0a9SA9+48l/YtGgddUsqJ5ITqRmCju88L33+NUNhXHtO9wv1uwOa6lyciqWjqF5uYu3ond13aX8EeI9WGu7tvBTaYWb/wQxcCBZWG5QA3hGfNjABKdLxdRKqy+8BhHnpzKadmtuGa07tX/w1SK5HOlrkd+Gt4psxq4CYzGw/g7k8DM4GRQCFwALgpBrWKSAp45K3l7D54hJeuPIl69dLsYHscRRTu7r4IyK708NMV1jtwWxTrEpEUtGDdLl6Zv55bvtGTgV1aBV1OStMnVEUkLsqOljNh6hI6t27CTy7qG3Q5KU/hLiJx8fxna1m2dS/3XjGQFo0Du5RE2lC4i0jMbd59kMdmr+CC/h24ZFCnoMtJCwp3EYm5yTMKKHfn/tGDsHT7xFJAFO4iElPvL9vGW/lbuf2CPnRvW7ktlcSKwl1EYubg4aNMmp5P7w4t+MHZvYIuJ63orIaIxMzv31/Jxl0HeXXcCBo10L5kPOlfW0RiYuW2vUz5aDVXn9aN4b3aBV1O2lG4i0jUuTsTpuXRvHED7r6sf9DlpCWFu4hE3d8XbmL+mp3cfVl/2qkxWCAU7iISVbv2H+bhmUs5rccJfDdbjcGConAXkaj6zdvLKDl4hAevHKzGYAFSuItI1CxYt5NX5m/g5m/0ZEBnNQYLksJdRKLiyNFyJkzNo0vrJtxxYZ+gy0l7mucuIlHx/KehxmDPXH8azdUYLHARvQNmthbYCxwFytw9u9L684DpwJrwQ6+7++TolSkiiWzT7oM8/u4KvjmgAxcP7Bh0OULN9tzPd/fir1n/sbtfXteCRCT53J+Tjzvcp8ZgCUPH3EWkTt4t2MY7Bdv48YV96HaCGoMlikjD3YF3zGyBmY07zpgzzOxLM5tlZoOiVJ+IJLADh8u4Nyefvh1bcMvZPYMuRyqI9LDMWe6+2cw6ALPNbJm7f1Rh/UKgh7vvM7ORwDTgmNPl4T8M4wAyMzPrWLqIBO2J9wrZtPsgf/v3M2hYXwcCEklE74a7bw5/3Q5MBYZVWr/H3feFl2cCDc2sfRXPM8Xds909OyMjo87Fi0hwVmzbyx8/Xs13TuvGsJ5tgy5HKqk23M2suZm1/GoZuBjIqzSmk4XPopjZsPDz7oh+uSKSCMrLnQlTl9CiSQPuHjkg6HKkCpEclukITA1ndwPgZXd/y8zGA7j708DVwK1mVgYcBK5xd49RzSISsNcWbuQfa3fxm2+fTNvmjYIuR6pQbbi7+2pgSBWPP11h+UngyeiWJiKJaNf+w/xq5lKye5zA1ad1C7ocOQ6dARGRGvn1rGXsPVTGg1epMVgiU7iLSMRy1+7k1dwN3Hx2T/p3UmOwRKZwF5GIfNUYrGubpmoMlgTU3UdEIvKnT9awfNtenr0hm2aNFB2JTnvuIlKtjbsO8Nt3V3LRwI5cpMZgSUHhLiLVun9GARBqDCbJQeEuIl/rnfytzC7Yxk++2YeubZoGXY5ESOEuIsd14HAZ988ooF/Hlnz/G2oMlkx0VkREjut3761k0+6DvDZejcGSjd4tEanSsq17eO7jNfyf7O5kZ6kxWLJRuIvIMcrLnXum5tGySQN+cVn/oMuRWlC4i8gxXluwkdx1u7h75ABOUGOwpKRwF5F/sXP/YR6etZRhWW25eqgagyUrhbuI/ItfzVzKPjUGS3oKdxH5p/lrdvK/CzZyy9m96NuxZdDlSB0o3EUEgMNl5dwzbQld2zTlxxf2DrocqaOIwt3M1prZEjNbZGa5Vaw3M3vCzArNbLGZDY1+qSISS899soYV2/YxecwgNQZLATV5B8939+LjrLsM6BO+DQeeCn8VkSSwYecBfvfeCi4Z1JELB6gxWCqI1p/nMcCL4eumzjWzNmbW2d23ROn5JQ0V7ytFV+KNj/ty8qlnxr1XqDFYqog03B14x8wceMbdp1Ra3xXYUOH+xvBjCneplUnT83jx83VBl5FWJowcQBc1BksZkYb7We6+2cw6ALPNbJm7f1RhfVXzpY7Z5zKzccA4gMzMzBoXK+lh3uodvPj5OkYP6cLpPfWx93ho26wRlw7uFHQZEkURhbu7bw5/3W5mU4FhQMVw3wh0r3C/G7C5iueZAkwByM7O1n+45RihGRt5dDuhKY98+2SaNqofdEkiSana2TJm1tzMWn61DFwM5FUalgPcEJ41MwIo0fF2qY0/frKaldtDMzYU7CK1F8mee0dgqpl9Nf5ld3/LzMYDuPvTwExgJFAIHABuik25kso27DzAE++t5JJBHbmgv2ZsiNRFteHu7quBIVU8/nSFZQdui25pkk7cnXs1Y0MkavQJVUkIb+dv4/1l2/nZRX01Y0MkChTuErj9pWXcPyOf/p1aMvbMrKDLEUkJ+oyxBO63765gS8khnvzeUBroUm4iUaHfJAlUweY9/OnTtVw7LJPTepwQdDkiKUPhLoEpL3fumbaENk0bctel/YIuRySlKNwlMK/mbmDh+t38cuQA2jTTpdxEoknhLoEo3lfKr2ctY3jPtnxraNegyxFJOQp3CcTDM5dy4HAZD101mPAH5EQkihTuEnefr9rB6ws3Me6cXvTuoEu5icSCwl3i6qtLuXVv25Qfnd8n6HJEUpbmuUtcPfvxalYV7efPY09XYzCRGNKeu8TN+h2hxmCXDe7E+f07BF2OSEpTuEtcuDuTcvJoUM+YdMXAoMsRSXkKd4mLt/K2Mmd5ET+9qC+dW6sxmEisKdwl5vaVlnH/jAIGdG6lxmAicaJwl5h7fPYKtu09xMNXDVZjMJE4ifg3zczqm9kXZvZGFevGmlmRmS0K326JbpmSrPI3l/DnT9dw7bBMTs1UYzCReKnJVMg7gKVAq+Osf9Xdf1T3kiRVlJc7E6bmcUKzRtx1Sf+gyxFJKxHtuZtZN2AU8MfYliOp5JV/rGfRht3cc/kAWjdrGHQ5Imkl0sMyvwV+DpR/zZhvm9liM3vNzLpXNcDMxplZrpnlFhUV1bRWSSJFe0t5ZNYyzujVjitPUWMwkXirNtzN7HJgu7sv+JphM4Asdz8ZeBd4oapB7j7F3bPdPTsjI6NWBUty+NXMpRw8cpQHrlRjMJEgRLLnfhYw2szWAv8DXGBmf6k4wN13uHtp+O6zwGlRrVKSymerinn9i02MP/dEendoEXQ5Immp2nB397vdvZu7ZwHXAO+7+3UVx5hZ5wp3RxM68SppqLTsKPdMyyOzbTNuO7930OWIpK1aNw4zs8lArrvnAD82s9FAGbATGBud8iTZPPvRalYX7ef5m06nSUM1BhMJSo3C3d3nAHPCy5MqPH43cHc0C5Pks27Hfn7/fiGjTurMef3UGEwkSPq4oESFuzNpej4N6hkTL1djMJGgKdwlKmblbeXDFUXceXE/OrVuEnQ5ImlP4S51tvfQEe6fkc/Azq244YweQZcjIuhKTBIFj81ewfa9pTx93WlqDCaSIPSbKHWSt6mEFz5by78NV2MwkUSicJdaO1ruTJi6hLbNG/GfagwmklAU7lJrL89fz5cbS7hn1EBaN1VjMJFEonCXWinaW8pv3lrGmSe2Y8wpXYIuR0QqUbhLrTz0ZgGlR8rVGEwkQSncpcY+LSxm2qLNjD+3FydmqDGYSCJSuEuNlJYdZeK0PHq0a8YP1RhMJGFpnrvUyDMfrmZ18X5e+P4wNQYTSWDac5eIrS3ez5MfFDLq5M6c21cXWxFJZAp3iYi7M3F6Ho3q12OSGoOJJDyFu0TkzSVb+HhlMXde3JeOrdQYTCTRRRzuZlbfzL4wszeqWNfYzF41s0Izm2dmWdEsUoK199ARJs8oYHDXVlw/Qo3BRJJBTfbc7+D4l8+7Gdjl7r2Bx4FH6lqYJI5H31lB0b5SHrryJDUGE0kSEf2mmlk3YBTwx+MMGQO8EF5+DbjQ9MmWlLBkYwkvfr6W64b3YEj3NkGXIyIRinQq5G+BnwMtj7O+K7ABwN3LzKwEaAcU17lCYdnWPfzhg1WUHS2P+2vnbS6hbfPG/Mcl/eL+2iJSe9WGu5ldDmx39wVmdt7xhlXxmFfxXOOAcQCZmZk1KDN9HS4r5/aXv2BrySE6t4n/icyWjRvywJh+agwmkmQi2XM/CxhtZiOBJkArM/uLu19XYcxGoDuw0cwaAK2BnZWfyN2nAFMAsrOzjwl/OdZzn6xh5fZ9PHdjNhcO6Bh0OSKSJKo95u7ud7t7N3fPAq4B3q8U7AA5wI3h5avDYxTedbRh5wF+994KLhnUUcEuIjVS6/YDZjYZyHX3HOA54CUzKyS0x35NlOpLW+7OfTn51DPj3isGBV2OiCSZGoW7u88B5oSXJ1V4/BDwnWgWlu7eKdjGe8u2M2HkALq0aRp0OSKSZDRpOQHtLy3jvpx8+ndqydizsoIuR0SSkLpCJqDfvruCLSWHePJ7Q2moDw2JSC0oORLM0i17+NOna7l2WHdO63FC0OWISJJSuCeQ8nJnwtQltG7akLsu7R90OSKSxBTuCeTV3A0sXL+bCSMH0KZZo6DLEZEkpnBPEMX7Svn1rGUM79mWbw3tGnQ5IpLkFO4J4lczl3HgcBkPXTUY9VwTkbpSuCeAz1ft4O8LNzLunF707nC83mwiIpFTuAfscFk5E6fn0e2Epvzo/D5BlyMiKULz3AP27MerKdy+jz+PPZ2mjeoHXY6IpAjtuQdo/Y4DPPHeSi4b3Inz+3cIuhwRSSEK94C4O/fm5NGgnjHpioFBlyMiKUbhHpC387fywfIifnpRXzq3VmMwEYkuhXsA9pWWcV9OAQM6t2LsmVlBlyMiKUjhHoDHZ69g295DPHTVYBqoMZiIxICSJc7yN5fw/GdruXZYJkMz1RhMRGKj2nA3syZmNt/MvjSzfDO7v4oxY82syMwWhW+3xKbc5BZqDJZHm6YNuesSNQYTkdiJZJ57KXCBu+8zs4bAJ2Y2y93nVhr3qrv/KPolpo5X/rGeRRt289h3h9C6WcOgyxGRFFZtuIcvdL0vfLdh+KaLX9dQ8b5SHpm1jBG92nLVqWoMJiKxFdExdzOrb2aLgO3AbHefV8Wwb5vZYjN7zcy6R7XKFPDwm0s5eOQoD155khqDiUjMRRTu7n7U3U8BugHDzGxwpSEzgCx3Pxl4F3ihqucxs3FmlmtmuUVFRXWpO6l8tqqY17/YxPhzT6R3hxZBlyMiaaBGs2XcfTcwB7i00uM73L00fPdZ4LTjfP8Ud8929+yMjIxalJt8SsuOcs+0PDLbNuO283sHXY6IpIlIZstkmFmb8HJT4JvAskpjOle4OxpYGs0ik9mzH61mddF+Jo8ZRJOGagwmIvERyWyZzsALZlaf0B+Dv7n7G2Y2Gch19xzgx2Y2GigDdgJjY1VwMlm/4wC/f7+QUSd15rx+agwmIvETyWyZxcCpVTw+qcLy3cDd0S0tubk7E6eHGoNNvFyNwUQkvvQJ1RiZlbeVD1cUcefF/ejUuknQ5YhImlG4x8DeQ0e4f0Y+g7q04oYzegRdjoikIV2JKQYen72S7XtLeeb6bDUGE5FAKHmiLG9TCc9/toZ/G57JKd3bBF2OiKQphXsUHS13JkzLo23zRvynGoOJSIAU7lH0yvz1fLlhN/eMGkjrpmoMJiLBUbhHSdHeUh55axlnntiOMad0CbocEUlzCvcoeejNAkqPlPPAlYPVGExEAqdwj4LPCouZtmgz48/txYkZagwmIsFTuNfRV43BerRrxg/VGExEEoTmudfRMx+uZnXxfl74/jA1BhORhKE99zpYW7yfJz8oZNTJnTm3b3q0MBaR5KBwr6WvGoM1ql+PSWoMJiIJRuFeS28u2cLHK4v5j4v70rGVGoOJSGJRuNfC3kNHmDyjgMFdW3H9GVlBlyMicgydUK2FR99ZQdG+Up69IZv69TSnXUQSTySX2WtiZvPN7Eszyzez+6sY09jMXjWzQjObZ2ZZsSg2ESzZWMKLn6/l+hE9GKLGYCKSoCI5LFMKXODuQ4BTgEvNbESlMTcDu9y9N/A48Eh0y0wMocZgS2jbvDF3Xtwv6HJERI6r2nD3kH3huw3DN680bAzwQnj5NeBCS8HP4L88bx2LN5Yw8fIBagwmIgktohOqZlbfzBYB24HZ7j6v0pCuwAYAdy8DSoB2VTzPODPLNbPcoqKiulUeZ9v3HuI3by/nG73bM3qIGoOJSGKLKNzd/ai7nwJ0A4aZ2eBKQ6raS6+8d4+7T3H3bHfPzshIrg/9PPjGUkqPlDN5zCA1BhORhFejqZDuvhuYA1xaadVGoDuAmTUAWgM7o1BfQvhkZTE5X27m1vNOpJcag4lIEohktkyGmbUJLzcFvgksqzQsB7gxvHw18L67H7PnnowOHTnKxOl5ZLVrxq3nnRh0OSIiEYlknntn4AUzq0/oj8Hf3P0NM5sM5Lp7DvAc8JKZFRLaY78mZhXH2dMfrmJN8X5eulmNwUQkeVQb7u6+GDi1iscnVVg+BHwnuqUFb03xfv57ziquGNKFs/sk1zkCEUlvaj9wHO7OpOl5NK5fj4mjBgRdjohIjSjcj2PG4nBjsEv60UGNwUQkySjcq7Dn0BEeeKOAk7q25roRPYIuR0SkxtQ4rAqPvr2cHftK+dONp6sxmIgkJe25V7J4425enLuO60f04KRurYMuR0SkVhTuFRwtdyZMzaN9i8bceYkag4lI8lK4V/CXuetYsqmESZcPpFUTNQYTkeSlcA/bvucQ//X2cs7u057LT+4cdDkiInWicA974M2llB4tZ/KYwWoMJiJJT+EOfLSiiBlfbua283rTs33zoMsREamztA/3Q0eOMml6Hj3bN2f8eb2CLkdEJCrSfp77U3NWsXbHAf5y83AaN1BjMBFJDWm95766aB9PzVnFmFO68I0+7YMuR0QkatI23N2didPzaNywHhPUGExEUkzahnvOl5v5tHAHP7+kHx1aqjGYiKSWtAz3koNHeOCNpQzp1prvDVdjMBFJPZFcZq+7mX1gZkvNLN/M7qhizHlmVmJmi8K3SVU9V6L4r7eXs3N/KQ9ddZIag4lISopktkwZcKe7LzSzlsACM5vt7gWVxn3s7pdHv8To+nLDbv4ybx03npHF4K5qDCYiqanaPXd33+LuC8PLe4GlQNdYFxYLZUfL+eXUJWS0aMydF/cNuhwRkZip0TF3M8sidD3VeVWsPsPMvjSzWWY26DjfP87Mcs0st6ioqMbF1tVLc9eRv3kPk64YSEs1BhORFBZxuJtZC+DvwE/cfU+l1QuBHu4+BPg9MK2q53D3Ke6e7e7ZGRnxveD0tj2HePSdFZzTN4NRJ6kxmIiktojC3cwaEgr2v7r765XXu/sed98XXp4JNDSzhPpU0OQ3Cjh8tJzJowepMZiIpLxIZssY8Byw1N0fO86YTuFxmNmw8PPuiGahdfHhiiLeXLyFH53fmyw1BhORNBDJbJmzgOuBJWa2KPzYL4FMAHd/GrgauNXMyoCDwDXu7jGot8a+agzWq31z/v1cNQYTkfRQbbi7+yfA1x7HcPcngSejVVQ0/fcHhazbcYCXb1FjMBFJHyn9CdVVRft46sNVXHlKF87snVCnAEREYiplw93dmTgtjyYN6zNh1MCgyxERiauUDffpizbz2aod/PzS/mS0bBx0OSIicZWS4V5y4AgPvlnAkO5t+N6wzKDLERGJu5QM9//7zjJ27j/MQ1cOVmMwEUlLKRfuX6zfxV/nrefGM9UYTETSV0qFe9nRciZMzaNDy8b87CI1BhOR9JVS4f7i5+so2LKHe68YpMZgIpLWUibct5Yc4tF3lnNu3wwuG9wp6HJERAKVMuE++Y18ysqdyWPUGExEJCXC/YPl25m5ZCu3X9CbHu3UGExEJOnD/Z+NwTKa84Nz1BhMRAQi6wqZ0J58v5ANOw/y8g/UGExE5CtJvedeuH0fz3y0im+d2pUzT1RjMBGRryRtuLs790xbQtOG9fnlqAFBlyMiklAiuRJTdzP7wMyWmlm+md1RxRgzsyfMrNDMFpvZ0NiU+/9N/WITc1fv5K7L+tO+hRqDiYhUFMkx9zLgTndfaGYtgQVmNtvdCyqMuQzoE74NB54Kf42JkgNHeOjNpZya2YZrT1djMBGRyqrdc3f3Le6+MLy8F1gKdK00bAzwoofMBdqYWeeoVxv2yNvL2HXgMA9eOZh6agwmInKMGh1zN7Ms4FRgXqVVXYENFe5v5Ng/AFGxcP0uXp63npvO6smgLmoMJiJSlYjD3cxaAH8HfuLueyqvruJbjrlAtpmNM7NcM8stKiqqWaVh9c04u097fqrGYCIixxVRuJtZQ0LB/ld3f72KIRuB7hXudwM2Vx7k7lPcPdvdszMyMmpTL0O6t+Glm4fTonHST9EXEYmZSGbLGPAcsNTdHzvOsBzghvCsmRFAibtviWKdIiJSA5Hs/p4FXA8sMbNF4cd+CWQCuPvTwExgJFAIHABuin6pIiISqWrD3d0/oepj6hXHOHBbtIoSEZG6SdpPqIqIyPEp3EVEUpDCXUQkBSncRURSkMJdRCQFWWiiSwAvbFYErKvlt7cHiqNYTjLQNqcHbXN6qMs293D3aj8FGli414WZ5bp7dtB1xJO2OT1om9NDPLZZh2VERFKQwl1EJAUla7hPCbqAAGib04O2OT3EfJuT8pi7iIh8vWTdcxcRka+R0OFuZpea2fLwhbd/UcX6xmb2anj9vPCVopJaBNv8MzMrCF+I/D0z6xFEndFU3TZXGHe1mbmZJf3Miki22cy+G36v883s5XjXGG0R/GxnmtkHZvZF+Od7ZBB1RouZ/cnMtptZ3nHWm5k9Ef73WGxmQ6NagLsn5A2oD6wCegGNgC+BgZXG/BB4Orx8DfBq0HXHYZvPB5qFl29Nh20Oj2sJfATMBbKDrjsO73Mf4AvghPD9DkHXHYdtngLcGl4eCKwNuu46bvM5wFAg7zjrRwKzCHXdHQHMi+brJ/Ke+zCg0N1Xu/th4H8IXYi7ojHAC+Hl14ALwxcXSVbVbrO7f+DuB8J35xK66lUyi+R9BngA+A1wKJ7FxUgk2/wD4A/uvgvA3bfHucZoi2SbHWgVXm5NFVdzSybu/hGw82uGjAFe9JC5QBsz6xyt10/kcI/kotv/HOPuZUAJ0C4u1cVGTS80fjOhv/zJrNptNrNTge7u/kY8C4uhSN7nvkBfM/vUzOaa2aVxqy42Itnm+4DrzGwjoQsA3R6f0gJT09/3GknkC5FGctHtiC7MnUQi3h4zuw7IBs6NaUWx97XbbGb1gMeBsfEqKA4ieZ8bEDo0cx6h/519bGaD3X13jGuLlUi2+VrgeXd/1MzOAF4Kb3N57MsLREzzK5H33CO56PY/x5hZA0L/lfu6/wYluoguNG5m3wQmAKPdvTROtcVKddvcEhgMzDGztYSOTeYk+UnVSH+2p7v7EXdfAywnFPbJKpJtvhn4G4C7fw40IdSDJVVF9PteW4kc7v8A+phZTzNrROiEaU6lMTnAjeHlq4H3PXymIklVu83hQxTPEAr2ZD8OC9Vss7uXuHt7d89y9yxC5xlGu3tuMOVGRSQ/29MInTzHzNoTOkyzOq5VRlck27weuBDAzAYQCveiuFYZXznADeFZMyOAEnffErVnD/qMcjVnm0cCKwidZZ8QfmwyoV9uCL35/0vowtzzgV5B1xyHbX4X2AYsCt9ygq451ttcaewckny2TITvswGPAQXAEuCaoGuOwzYPBD4lNJNmEXBx0DXXcXtfAbYARwjtpd8MjAfGV3iP/xD+91gS7Z9rfUJVRCQFJfJhGRERqSWFu4hIClK4i4ikIIW7iEgKUriLiKQghbuISApSuIuIpCCFu4hICvp/vn71J5pgKukAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The value of the optimal policy using policy iteration is [ 2.33333333  0.          3.         -0.03       -0.07        2.33333333\n",
      " -0.23543333 -0.07       -0.25082029 -0.24981367  0.        ]:\n",
      "The optimal policy using policy iteration is [[0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "The number of epochs for convergence are 18\n",
      "The optimal policy using value iteration is [[0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "The number of epocs for convergence is 4\n"
     ]
    }
   ],
   "source": [
    "grid = GridWorld()\n",
    "\n",
    "### Question 1 : Change the policy here:\n",
    "Policy= np.zeros((grid.state_size, grid.action_size))\n",
    "Policy = Policy + 0.25\n",
    "print(\"The Policy is : {}\".format(Policy))\n",
    "\n",
    "val, epochs = grid.policy_evaluation(Policy,0.001,0.3)\n",
    "print(\"The value of that policy is :{}\".format(val))\n",
    "print(\"It took {} epochs\".format(epochs))\n",
    "\n",
    "\n",
    "gamma_range = [0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]\n",
    "epochs_needed = []\n",
    "for gamma in gamma_range:\n",
    "    val, epochs = grid.policy_evaluation(Policy,0.001,gamma)\n",
    "    epochs_needed.append(epochs)\n",
    "    \n",
    "plt.figure()\n",
    "plt.plot(gamma_range,epochs_needed)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "V_opt, pol_opt, epochs = grid.policy_iteration()\n",
    "print(\"The value of the optimal policy using policy iteration is {}:\".format(V_opt))\n",
    "print(\"The optimal policy using policy iteration is {}\".format(pol_opt))\n",
    "print(\"The number of epochs for convergence are {}\".format(epochs))\n",
    "\n",
    "\n",
    "pol_opt2, epochs = grid.value_iteration()\n",
    "print(\"The optimal policy using value iteration is {}\".format(pol_opt2))\n",
    "print(\"The number of epocs for convergence is {}\".format(epochs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Using draw_deterministic_policy to illustrate some arbitracy policy.\n",
    "Policy2 = np.array([np.argmax(pol_opt[row,:]) for row in range(grid.state_size)])\n",
    "\n",
    "\n",
    "grid.draw_deterministic_policy(Policy2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MORE NOISE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld(object):\n",
    "    def __init__(self):\n",
    "        \n",
    "        ### Attributes defining the Gridworld #######\n",
    "        # Shape of the gridworld\n",
    "        self.shape = (4,4)\n",
    "        \n",
    "        # Locations of the obstacles\n",
    "        self.obstacle_locs = [(1,2),(2,0),(3,0),(3,1),(3,3)]\n",
    "        \n",
    "        # Locations for the absorbing states\n",
    "        self.absorbing_locs = [(0,1),(3,2)]\n",
    "        \n",
    "        # Rewards for each of the absorbing states \n",
    "        self.special_rewards = [10, -100] #corresponds to each of the absorbing_locs\n",
    "        \n",
    "        # Reward for all the other states\n",
    "        self.default_reward = -1\n",
    "        \n",
    "        # Starting location\n",
    "        self.starting_loc = (0,0)\n",
    "        \n",
    "        # Action names\n",
    "        self.action_names = ['N','E','S','W']\n",
    "        \n",
    "        # Number of actions\n",
    "        self.action_size = len(self.action_names)\n",
    "        \n",
    "        \n",
    "        # Randomizing action results: [1 0 0 0] to no Noise in the action results.\n",
    "        self.action_randomizing_array = [1, 0, 0, 0]\n",
    "        \n",
    "        ############################################\n",
    "    \n",
    "    \n",
    "    \n",
    "        #### Internal State  ####\n",
    "        \n",
    "    \n",
    "        # Get attributes defining the world\n",
    "        state_size, T, R, pi, absorbing, locs = self.build_grid_world()\n",
    "        \n",
    "        # Number of valid states in the gridworld (there are 11 of them)\n",
    "        self.state_size = state_size\n",
    "        \n",
    "        # Transition operator (3D tensor)\n",
    "        self.T = T\n",
    "        \n",
    "        # Reward function (3D tensor)\n",
    "        self.R = R\n",
    "        \n",
    "        \n",
    "        # Probability function for stochasticity (2D tensor)\n",
    "        self.pi = pi\n",
    "        \n",
    "        # Absorbing states\n",
    "        self.absorbing = absorbing\n",
    "        \n",
    "        # The locations of the valid states\n",
    "        self.locs = locs\n",
    "        \n",
    "        # Number of the starting state\n",
    "        self.starting_state = self.loc_to_state(self.starting_loc, locs);\n",
    "        \n",
    "        # Locating the initial state\n",
    "        self.initial = np.zeros((1,len(locs)));\n",
    "        self.initial[0,self.starting_state] = 1\n",
    "        \n",
    "        \n",
    "        # Placing the walls on a bitmap\n",
    "        self.walls = np.zeros(self.shape);\n",
    "        for ob in self.obstacle_locs:\n",
    "            self.walls[ob]=1\n",
    "            \n",
    "        # Placing the absorbers on a grid for illustration\n",
    "        self.absorbers = np.zeros(self.shape)\n",
    "        for ab in self.absorbing_locs:\n",
    "            self.absorbers[ab] = -1\n",
    "        \n",
    "        # Placing the rewarders on a grid for illustration\n",
    "        self.rewarders = np.zeros(self.shape)\n",
    "        for i, rew in enumerate(self.absorbing_locs):\n",
    "            self.rewarders[rew] = self.special_rewards[i]\n",
    "        \n",
    "        #Illustrating the grid world\n",
    "        self.paint_maps()\n",
    "        ################################\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ####### Getters ###########\n",
    "    \n",
    "    def get_transition_matrix(self):\n",
    "        return self.T\n",
    "    \n",
    "    def get_reward_matrix(self):\n",
    "        return self.R\n",
    "    \n",
    "    def get_probability_matrix(self):\n",
    "        return self.pi\n",
    "    \n",
    "    \n",
    "    ########################\n",
    "    \n",
    "    ####### Methods #########\n",
    "    \n",
    "    \n",
    "    def value_iteration(self, discount = 0.3, threshold = 0.0001):\n",
    "        V = np.zeros(self.state_size)\n",
    "        \n",
    "        T = self.get_transition_matrix()\n",
    "        R = self.get_reward_matrix()\n",
    "        \n",
    "        ####\n",
    "        pi = self.get_probability_matrix()\n",
    "        \n",
    "        epochs = 0\n",
    "        while True:\n",
    "            epochs+=1\n",
    "            delta = 0\n",
    "\n",
    "            for state_idx in range(self.state_size):\n",
    "                if(self.absorbing[0,state_idx]):\n",
    "                    continue \n",
    "                    \n",
    "                v = V[state_idx]\n",
    "\n",
    "                Q = np.zeros(4)\n",
    "                for state_idx_prime in range(self.state_size):\n",
    "                    Q +=  pi[state_idx,:]*T[state_idx_prime,state_idx,:] * (R[state_idx_prime,state_idx, :] + discount * V[state_idx_prime])\n",
    "                    ########### added P\n",
    "                    \n",
    "                V[state_idx]= np.max(Q)\n",
    "                delta = max(delta,np.abs(v - V[state_idx]))\n",
    "                \n",
    "            if(delta<threshold):\n",
    "                optimal_policy = np.zeros((self.state_size, self.action_size))\n",
    "                for state_idx in range(self.state_size):\n",
    "                    Q = np.zeros(4)\n",
    "                    for state_idx_prime in range(self.state_size):\n",
    "                        Q += pi[state_idx,:]*T[state_idx_prime,state_idx,:] * (R[state_idx_prime,state_idx, :] + discount * V[state_idx_prime])\n",
    "                    \n",
    "                    optimal_policy[state_idx, np.argmax(Q)]=1\n",
    "\n",
    "                \n",
    "                return optimal_policy,epochs\n",
    "\n",
    "\n",
    "    \n",
    "    def policy_iteration(self, discount=0.3, threshold = 0.0001):\n",
    "        policy= np.zeros((self.state_size, self.action_size))\n",
    "        policy[:,0] = 1\n",
    "        \n",
    "        T = self.get_transition_matrix()\n",
    "        R = self.get_reward_matrix()\n",
    "        \n",
    "        ########\n",
    "        pi = self.get_probability_matrix()\n",
    "        \n",
    "        epochs =0\n",
    "        while True: \n",
    "            V, epochs_eval = self.policy_evaluation(policy, threshold, discount)\n",
    "            \n",
    "            epochs+=epochs_eval\n",
    "            #Policy iteration\n",
    "            policy_stable = True\n",
    "            \n",
    "            for state_idx in range(policy.shape[0]):\n",
    "                if(self.absorbing[0,state_idx]):\n",
    "                    continue \n",
    "                    \n",
    "                old_action = np.argmax(policy[state_idx,:])\n",
    "                \n",
    "                Q = np.zeros(4)\n",
    "                for state_idx_prime in range(policy.shape[0]):\n",
    "                    Q += pi[state_idx,:] * T[state_idx_prime,state_idx,:] * (R[state_idx_prime,state_idx, :] + discount * V[state_idx_prime])\n",
    "                   \n",
    "                ####### Added P, 2D vector consisting of pi(s,a)\n",
    "                \n",
    "                new_policy = np.zeros(4)\n",
    "                new_policy[np.argmax(Q)]=1\n",
    "                policy[state_idx] = new_policy\n",
    "                \n",
    "                if(old_action !=np.argmax(policy[state_idx])):\n",
    "                    policy_stable = False\n",
    "            \n",
    "            if(policy_stable):\n",
    "                return V, policy,epochs\n",
    "                \n",
    "                \n",
    "                \n",
    "        \n",
    "    \n",
    "    def policy_evaluation(self, policy, threshold, discount):\n",
    "        \n",
    "        # Make sure delta is bigger than the threshold to start with\n",
    "        delta= 2*threshold\n",
    "        \n",
    "        #Get the reward and transition matrices\n",
    "        R = self.get_reward_matrix()\n",
    "        T = self.get_transition_matrix()\n",
    "        \n",
    "        \n",
    "        ###\n",
    "        pi = self.get_probability_matrix()\n",
    "\n",
    "        \n",
    "        # The value is initialised at 0\n",
    "        V = np.zeros(policy.shape[0])\n",
    "        # Make a deep copy of the value array to hold the update during the evaluation\n",
    "        Vnew = np.copy(V)\n",
    "        \n",
    "        epoch = 0\n",
    "        # While the Value has not yet converged do:\n",
    "        while delta>threshold:\n",
    "            epoch += 1\n",
    "            for state_idx in range(policy.shape[0]):\n",
    "                # If it is one of the absorbing states, ignore\n",
    "                if(self.absorbing[0,state_idx]):\n",
    "                    continue   \n",
    "                \n",
    "                # Accumulator variable for the Value of a state\n",
    "                tmpV = 0\n",
    "                for action_idx in range(policy.shape[1]):\n",
    "                    # Accumulator variable for the State-Action Value\n",
    "                    tmpQ = 0\n",
    "                    for state_idx_prime in range(policy.shape[0]):\n",
    "                        tmpQ = tmpQ + pi[state_idx,action_idx]*T[state_idx_prime,state_idx,action_idx] * (R[state_idx_prime,state_idx, action_idx] + discount * V[state_idx_prime])\n",
    "                    \n",
    "                    tmpV += policy[state_idx,action_idx] * tmpQ\n",
    "                    \n",
    "                # Update the value of the state\n",
    "                Vnew[state_idx] = tmpV\n",
    "            \n",
    "            # After updating the values of all states, update the delta\n",
    "            delta =  max(abs(Vnew-V))\n",
    "            # and save the new value into the old\n",
    "            V=np.copy(Vnew)\n",
    "            \n",
    "        return V, epoch\n",
    "    \n",
    "    def draw_deterministic_policy(self, Policy):\n",
    "        # Draw a deterministic policy\n",
    "        # The policy needs to be a np array of 11 values between 0 and 3 with\n",
    "        # 0 -> N, 1->E, 2->S, 3->W\n",
    "        plt.figure()\n",
    "        plt.imshow(self.walls+ self.rewarders + self.absorbers)\n",
    "        plt.imshow(self.walls)\n",
    "        \n",
    "        #plt.hold('on')\n",
    "        for state, action in enumerate(Policy):\n",
    "            if(self.absorbing[0,state]):\n",
    "                continue\n",
    "            arrows = [r\"$\\uparrow$\",r\"$\\rightarrow$\", r\"$\\downarrow$\", r\"$\\leftarrow$\"]\n",
    "            action_arrow = arrows[action]\n",
    "            location = self.locs[state]\n",
    "            plt.text(location[1], location[0], action_arrow, ha='center', va='center', color='w', size='40')\n",
    "    \n",
    "        plt.show()\n",
    "    ##########################\n",
    "    \n",
    "    \n",
    "    ########### Internal Helper Functions #####################\n",
    "    def paint_maps(self):\n",
    "        plt.figure()\n",
    "        plt.subplot(1,3,1)\n",
    "        plt.imshow(self.walls)\n",
    "        plt.subplot(1,3,2)\n",
    "        plt.imshow(self.absorbers)\n",
    "        plt.subplot(1,3,3)\n",
    "        plt.imshow(self.rewarders) \n",
    "        plt.show()\n",
    "        \n",
    "    def build_grid_world(self):\n",
    "        # Get the locations of all the valid states, the neighbours of each state (by state number),\n",
    "        # and the absorbing states (array of 0's with ones in the absorbing states)\n",
    "        locations, neighbours, absorbing_states = self.get_topology()\n",
    "        \n",
    "        # Get the number of states\n",
    "        S = len(locations)\n",
    "        \n",
    "        # Initialise the transition matrix\n",
    "        T = np.zeros((S,S,4))\n",
    "        \n",
    "        ## Initialise the probability matrix\n",
    "        pi = np.zeros((S,4))  ### add\n",
    "        \n",
    "        ##probability matrix is based on action-state pairs\n",
    "        ##if the action and outcome is the samethen p=0.3\n",
    "        p_constant = 0.3\n",
    "        \n",
    "        for action in range(4):\n",
    "            for effect in range(4):\n",
    "                \n",
    "                # Randomize the outcome of taking an action. is it random tho......\n",
    "                \n",
    "                # outcome = \n",
    "                \n",
    "                outcome = (action+effect+1) % 4\n",
    "                if outcome == 0:\n",
    "                    outcome = 3\n",
    "                else:\n",
    "                    outcome -= 1\n",
    "    \n",
    "                # Fill the probability and transition matrix\n",
    "                #prob = self.action_randomizing_array[effect] \n",
    "                \n",
    "                for prior_state in range(S):\n",
    "                    post_state = neighbours[prior_state, outcome]\n",
    "                    post_state = int(post_state)\n",
    "                    \n",
    "                    if action == outcome: \n",
    "                        pi[prior_state,action] = p_constant\n",
    "                        prob = p_constant\n",
    "                        \n",
    "                    else:\n",
    "                        pi[prior_state,action] = (1-p_constant)/3\n",
    "                        prob = (1-p_constant)/3\n",
    "                        \n",
    "                    T[post_state,prior_state,action] = T[post_state,prior_state,action]+prob\n",
    "                \n",
    "    \n",
    "        # Build the reward matrix\n",
    "        R = self.default_reward*np.ones((S,S,4))\n",
    "        for i, sr in enumerate(self.special_rewards):\n",
    "            post_state = self.loc_to_state(self.absorbing_locs[i],locations)\n",
    "            R[post_state,:,:]= sr\n",
    "        \n",
    "        return S, T, R, pi, absorbing_states,locations\n",
    "    \n",
    "    def get_topology(self):\n",
    "        height = self.shape[0]\n",
    "        width = self.shape[1]\n",
    "        \n",
    "        index = 1 \n",
    "        locs = []\n",
    "        neighbour_locs = []\n",
    "        \n",
    "        for i in range(height):\n",
    "            for j in range(width):\n",
    "                # Get the locaiton of each state\n",
    "                loc = (i,j)\n",
    "                \n",
    "                #And append it to the valid state locations if it is a valid state (ie not absorbing)\n",
    "                if(self.is_location(loc)):\n",
    "                    locs.append(loc)\n",
    "                    \n",
    "                    # Get an array with the neighbours of each state, in terms of locations\n",
    "                    local_neighbours = [self.get_neighbour(loc,direction) for direction in ['nr','ea','so', 'we']]\n",
    "                    neighbour_locs.append(local_neighbours)\n",
    "                \n",
    "        # translate neighbour lists from locations to states\n",
    "        num_states = len(locs)\n",
    "        state_neighbours = np.zeros((num_states,4))\n",
    "        \n",
    "        for state in range(num_states):\n",
    "            for direction in range(4):\n",
    "                # Find neighbour location\n",
    "                nloc = neighbour_locs[state][direction]\n",
    "                \n",
    "                # Turn location into a state number\n",
    "                nstate = self.loc_to_state(nloc,locs)\n",
    "      \n",
    "                # Insert into neighbour matrix\n",
    "                state_neighbours[state,direction] = nstate;\n",
    "                \n",
    "    \n",
    "        # Translate absorbing locations into absorbing state indices\n",
    "        absorbing = np.zeros((1,num_states))\n",
    "        for a in self.absorbing_locs:\n",
    "            absorbing_state = self.loc_to_state(a,locs)\n",
    "            absorbing[0,absorbing_state] =1\n",
    "        \n",
    "        return locs, state_neighbours, absorbing \n",
    "\n",
    "    def loc_to_state(self,loc,locs):\n",
    "        #takes list of locations and gives index corresponding to input loc\n",
    "        return locs.index(tuple(loc))\n",
    "        #return locs.index(loc)\n",
    "\n",
    "\n",
    "    def is_location(self, loc):\n",
    "        # It is a valid location if it is in grid and not obstacle\n",
    "        if(loc[0]<0 or loc[1]<0 or loc[0]>self.shape[0]-1 or loc[1]>self.shape[1]-1):\n",
    "            return False\n",
    "        elif(loc in self.obstacle_locs):\n",
    "            return False\n",
    "        else:\n",
    "             return True\n",
    "            \n",
    "    def get_neighbour(self,loc,direction):\n",
    "        #Find the valid neighbours (ie that are in the grif and not obstacle)\n",
    "        i = loc[0]\n",
    "        j = loc[1]\n",
    "        \n",
    "        nr = (i-1,j)\n",
    "        ea = (i,j+1)\n",
    "        so = (i+1,j)\n",
    "        we = (i,j-1)\n",
    "        \n",
    "        # If the neighbour is a valid location, accept it, otherwise, stay put\n",
    "        if(direction == 'nr' and self.is_location(nr)):\n",
    "            return nr\n",
    "        elif(direction == 'ea' and self.is_location(ea)):\n",
    "            return ea\n",
    "        elif(direction == 'so' and self.is_location(so)):\n",
    "            return so\n",
    "        elif(direction == 'we' and self.is_location(we)):\n",
    "            return we\n",
    "        else:\n",
    "            #default is to return to the same location\n",
    "            return loc\n",
    "        \n",
    "###########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Policy is : [[0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]]\n",
      "The value of that policy is :[ 0.41902897  0.          0.41881401 -0.23877617 -0.22684287  0.40541948\n",
      " -0.25259373 -0.34555087 -6.12785596 -0.35748073  0.        ]\n",
      "It took 4 epochs\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The value of the optimal policy using policy iteration is [ 0.55107186  0.          0.55084705 -0.23351059 -0.22019077  0.53650379\n",
      " -0.25208023 -0.326305   -5.72937394 -0.34342427  0.        ]:\n",
      "The optimal policy using policy iteration is [[0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "The number of epochs for convergence are 15\n",
      "The optimal policy using value iteration is [[0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "The number of epocs for convergence is 5\n"
     ]
    }
   ],
   "source": [
    "grid = GridWorld()\n",
    "\n",
    "### Question 1 : Change the policy here:\n",
    "Policy= np.zeros((grid.state_size, grid.action_size))\n",
    "Policy = Policy + 0.25\n",
    "print(\"The Policy is : {}\".format(Policy))\n",
    "\n",
    "val, epochs = grid.policy_evaluation(Policy,0.001,0.3)\n",
    "print(\"The value of that policy is :{}\".format(val))\n",
    "print(\"It took {} epochs\".format(epochs))\n",
    "\n",
    "\n",
    "gamma_range = [0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]\n",
    "epochs_needed = []\n",
    "for gamma in gamma_range:\n",
    "    val, epochs = grid.policy_evaluation(Policy,0.001,gamma)\n",
    "    epochs_needed.append(epochs)\n",
    "    \n",
    "plt.figure()\n",
    "plt.plot(gamma_range,epochs_needed)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "V_opt, pol_opt, epochs = grid.policy_iteration()\n",
    "print(\"The value of the optimal policy using policy iteration is {}:\".format(V_opt))\n",
    "print(\"The optimal policy using policy iteration is {}\".format(pol_opt))\n",
    "print(\"The number of epochs for convergence are {}\".format(epochs))\n",
    "\n",
    "\n",
    "pol_opt2, epochs = grid.value_iteration()\n",
    "print(\"The optimal policy using value iteration is {}\".format(pol_opt2))\n",
    "print(\"The number of epocs for convergence is {}\".format(epochs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# OPTIMAL POLICY VIA POLICY ITERATION\n",
    "Policy_POLIT = np.array([np.argmax(pol_opt[row,:]) for row in range(grid.state_size)])\n",
    "\n",
    "\n",
    "grid.draw_deterministic_policy(Policy_POLIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQUAAAD7CAYAAABjeYFMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAFg5JREFUeJzt3Xl0lPW9x/F3iIksoiQuoK1H78H4I7ZKBSwqWD22YJUqW6+2WsUVqueKgD21elVsq1cRlBbrUrG0dWtVlArKHnY0KNi69NLvUdpLr9eltpIASSBAcv+YyZhfDFmemXmeWT6vczwnz5Ln+f7ym/n4zMzDfAsaGxsREWnSJeoCRCSzKBRExKNQEBGPQkFEPAoFEfEoFETEc0CQX3LOdQOeBI4AdgDjzOyTFvvMBw4F9gB1ZnZukrWKSAiCXilcC7xtZmcAjwO3trLPccBQMztLgSCSPYKGwlBgcfznRcA3mm90zvUGegELnHPrnHPfCl6iiISp3ZcPzrmrgMktVn8MVMd/3gEc0mJ7MXAf8HOgFFjvnHvNzP6xv/Ps2rWrcd++ho7WnZTi4mLq6+tDOVeYcnVckLtjC3NcdXV1HHbYoQXt7dduKJjZr4BfNV/nnHsB6Blf7AlUtfi1j4BHzGwv8A/n3B8BB+w3FN75szH41HBeZWyoXBTaucKUq+OC3B1bmOPaULmIww47tN39gr58WA+cF//5XGBti+3fAJ4FcM4dBHwZ2BzwXCISokCfPgAPA791zq0D6oGLAZxz9wJzzWyRc+4c51wl0ADcYmb/TEnFIpJWgULBzGqBf29l/Q+b/TwpibpEJCK6eUlEPAoFEfEoFETEo1AQEY9CQUQ8CgUR8SgURMSjUBARj0JBRDwKBRHxKBRExKNQEBGPQqENBQXtfh+FpEku/+0zfWwKhf0oLi7mxXm/YfKkCVGXknd69z6c119bwvBhZ0ZdSsplw9gUCq0oLi5m7rOzGTFiGDOmT1UwhKh378OpWDaXk7/yZV54fk5GP3k6K1vGplBoxUknlXP22UMTywqGcPTpcwQrlj9PeXkZAN26dWPC+Msirio1smlsCoVWbNz4JqNGX0FdXV1inYIhvfr0OYKKZXPp1++4xLrFi1fw3UuujbCq1Mi2sSkU9mN5xRoFQ0hae9IsWlTB6LFXZv03OGfj2BQKbVAwpF9rT5qFC5cz5ttXZeyTpqOydWxB28Z1AR4C+gO7gavN7L1m268BJgB7gTvN7KUU1Nqm44/vS8+ePTq0b/fu3Rg48KQO7butqoqf3jmT/7rrlsS6GdOnAjDzZ7/sfKGScOSRvalYNhfn+ibWbd78LndPe4ATT+zn7duZOWuL2RZ27qxJ+jjt6ejYUjUuSN3YChobGzv9S865McAFZna5c+5U4GYzGxnf1gdYBgwCugLrgEFmtrutY27c9GZjMt9/X7F8LmedeXrg3w9i3OXX8+RTz4d6zrZkU2+ELl268M5bq70nTRjO/vpYVq95Na3nyNSxbahcxKCB/du9SSLptnFmVkksAJp8FVhvZrvNrBp4D0hNFGaYE05wUZeQtYqKikJ/0oQl28cWNBQO5rO2cQD7nHMH7Gdba23lsl5DQwMbN/4p6jKyVn19PW/88e2oy0iLbB9b0GYw2/msbRxAl3iLuNa2tdZW7nOOOeaLbKhcFLAcOOCAA3jr7f/u0L7H9f033tvytw4fu6SkF0d/8ajEcmNjI1v//j433XQ9N910fadrTZfyfmVJ/Q3DVlBQQE1tLT26d0+sq6vbxZa/bqWhYZ+3b2fnbH+mT59KkJfMndXRsaVqXJC6sQUNhfXA+cCz8fcUmsfia8BdzrmuwIFAOfBOewfcuvX9jOwlOXbMCJ568qHEckNDA9eMv5Hf/PaZdJUXWDa9p9CkV69DWLrkGQYOiL3C7NatK9u2VTHiW5dQU1Ob2C9XxxZ2L8mOCPryYR6wyzn3CjATmOycm+Kcu8DMPgJmEesvuQL4TzPbFfA8kWoKhKKiIiCzAyFbVVVVM/yci9j0xluJdWcMHczCl5/moIM69mlSpsrWsQVtG9cAfL/F6r802z4bmJ1EXZFTIISn6cmzZPHvGTSwPwBDh3yVhS8/zXkjLg7lI8R0aW9smUg3L7Vi8OABCoSQVVVVc843v8PGTW8m1g05/RSe+V323wuSbWNTKLTi9df/xHNzFwAKhDC1fPJs376Dn941M+KqUiObxqZQaEVDQwPjLp/Ik0/NVSCErOlye8XKdZw74mIqKzdFXVLKZMvYgn76kPOagkHCV129nWHDL4y6jLTIhrHpSkFEPAoFEfEoFETEo1AQEY9CQUQ8CgUR8SgURMSjUBARj0JBRDwKBRHxKBRExKNQEBGPQkFEPAoFEfEoFETEo1AQEU+6eknOAoYQawQDMDLeLUpEMlzQb14aBXQ1s9PifR/uA0Y22z4AOMfM/plsgSISrqCh4PWSdM4leknGryLKgEedc72BX5nZnKQrlYyz64O1oZ2rsNcXQjtf16POCOU8mSpoKLTaSzLeOq4H8ABwP1AIrHTObTSzt1o5TkKybeM6I9vaq3VU2OMq7PWF0M5VUFgc2vnC/Btm4mMxHb0ka4Gfm1ktgHNuBbH3HtoMhUxtG5dNwh5X2FcK+6r+L5Rzhfk3zKW2ceuB8wBa6SV5PLDOOVfonCsi9lLjjYDnEZGQBb1SmAcMi/eSLACucM5NAd4zs/nOuaeASmAP8LiZ/Tk15YpIuqWrl+S9wL1J1CUiEdHNSyLiUSiIiEdt4+JKSnpx+mmD2L59B2vXbYi6nJTK5bHlqijnTFcKQGlpCcuXPcf8Fx9n1cp5/ODG66IuKWVyeWy5Kuo5y/tQKC0tYdnSZ/lK/y8l1k2751ZunHJthFWlRi6PLVdlwpzldSi0NgFN7p12G1Mmt/yAJXvk8thyVabMWd6GQlsT0GT6vbczedKEEKtKjVweW67KpDnLy1AoLCxsdwKazJg+lUmTxodQVWp05MHVJNvGlqsybc7yLhRKS0twx/f1JmDV6lf48MOPE8sLFiyltrYusXzf9Du44YZrQq0ziNbCLlfGlqsycc7yKhSaErl7926JdatWv8L5F1zK7t31iXVr1lYyctQ4byLun/FjJk68OtR6O2N/YZcLY8tVmTpneRUKkyeNb3UCmv+xm6xYuY7RY66gru6zbXf+5EccdVSfUGrtrMmTxrcadrkwtlyVqXOWV6Ew9Y7pPPvsfKDtCWiyvGINo8dcSV1dHTU1tYwcNY4PPvgorHI7Zeod0/n00yog98aWqzJ1zvLqjsaGhgYuufQ6hgw5pd0JaLJs+WrGjL2SPXv2snLV+hCqDKahoYG//m0r9898hJ/Pmp1TY8tVmTpneRUKEJuIDz/6R4cmoMnSZavTWFFq3X3PrE7tn01jy1WZNmd59fJBRNqnUBARj0JBRDwKBRHxKBRExJNUKDjnBjvnVrWy/nzn3OvOuVedc7qHViSLBA4F59wPgceAri3WFwEzgeHAmcB455xulRPJEslcKWwBxrSyvpzYV71vM7N6YB2Q3324RLJIQWNjY+Bfds4dC/zezE5ttm4ocL2ZXRRf/gnwdzN7rK1jffLPfzVu3fp+4Fo6o7xfGZv/8q637sQvl3PggcUA/O/7H/Dxx5+EUksqtTYuSN/YBpzkUnKcjigoLKZxX337O6bAG29ZKOeB8Ods0MD+Be3tk447Glu2lOsJVLX3S1G3jdvy7gaOPfZoAGbNeoz7Zz4SSi2ptL8WZOkam9rGJS/MOeto27h0hMJmoMw5VwrsBL4GzEjDeUQkDVIWCs65i4GDzOzReAu5JcTes5hjZuFEvIgkLalQMLP/AU6N//x0s/ULgAVJVSYikdDNSyLiUSiIiEehICIehYKIeBQKIuLJu69j25++ZYOjLiFtcnlsuSrKOdOVgoh4FAoi4lEoiIhHoSAiHoWCiHgUCiLiUSiIiEehICIehYKIeBQKIuJRKIiIR6EgIh6Fgoh4kvpXks65wcA0MzurxfopwFVA05fVTzCz8L5MX0QCCxwK8bZxlwI1rWweAFxmZpuCHl9EopGOtnEAA4GbnXPrnHM3J3EOEQlZytvGxddPBR4k1i1qHvCwmb3U1rEaG/Y1Nu7bE7iWzsi3FmS5IFfHFva4Imkb55wrAH5mZtXx5ZeBk4G2Q2HfntDaguVbC7JckKtjC3NcUbaNOxh4xzlXTuz9hrOBOWk4j4ikQbraxt0CrAR2AxVmtjBV5xGR9EpX27gngCeSqkxEIqGbl0TEo1AQEY9CQUQ8agaTB0pKenH6aYPYvn0Ha9dtiLoc6YAo50xXCjmutLSE5cueY/6Lj7Nq5Tx+cON1UZck7Yh6zhQKOay0tIRlS5/lK/2/lFg37Z5buXHKtRFWJW3JhDlTKOSo1h5cTe6ddhtTJn8/gqqkLZkyZwqFHNTWg6vJ9HtvZ/KkCSFWJW3JpDlTKOSYjjy4msyYPpVJk8aHUJW0JdPmTKGQQwoLCz/34Fq1+hU+/PDjxPKCBUupra1LLN83/Q5uuOGaUOuUz2TinCkUckRpaQnu+L6fe3Cdf8Gl7N792T8TX7O2kpGjxnkPsvtn/JiJE68OtV7J3DlTKOSIyZPG0717t8Ry04Or+QOpyYqV6xg95grq6j7bdudPfsRRR/UJpVaJydQ5UyjkiKl3TOfTT6uAth9cTZZXrGH0mCupq6ujpqaWkaPG8cEHH4VVbrsKCtr9LpCsl6lzplDIEQ0NDfz1b1u59bZ72n1wNVm2fDVjxl7JyFHjWLlqfQhVdkzv3ofz+mtLGD7szKhLSatMnTPd5pxj7r5nVqf2X7psdZoqCaZ378OpWDaX8vIyXnh+DmPGXplxNaZaps2ZrhQkY/TpcwQrlj9PeXkZAN26dWPC+Msirir/KBQkI/TpcwQVy+bSr99xiXWLF6/gu5foluywKRQkcq0FwqJFFYweeyX19eF867Z8RqEgkWotEBYuXM6Yb1+lQIhIoDcanXNFxL6h+VjgQOBOM5vfbPv5wO3AXmCOmc1OvlTJNUce2ZuKZXNxrm9i3ebN73L3tAc48cR+3r7du3dj4MCTkj6n2RZ27mytqZk0Cfrpw/eAf5nZpc65Q4E/AvMhERgzgVOIfcX7eufcAjPLnA/BJXJdunT5XCAAlJeXsXb1i63+zmuVi5M+79lfH8vqNa8mfZxcFvTlw3PAbc2W9zb7uRx4z8y2mVk9sA44I+B5JEcVFRV9LhAkMwS6UjCznQDOuZ7AXODWZpsPBqqbLe8ADmnvmAWFRRT2+kKQcjqtoLA4tHN1tCtPKpT3K2v1fEce2Tvx88SJV3PRRSNDq6ktNbW19OjePdRzPvzQNHZk0MuHTJyzZLpOH02sT+RDzXs+EOsf2bPZck+gqr3jqW1c8vbXgmzLuxs49tijAZg16zHun/lIaDW1paSkF0uXPMOAk09MrNu8+V0u+u74z73u/8O83zBq9OVJn/Ojjz5h9+7dSR8nVcKcs7S2jXPO9QaWAv9hZhUtNm8GypxzpcBO4GvAjCDnkdy2bVsVw4ZfyNIlzzBwQOxNxPLyMh78xT2M+NYl1NTUJvatr9/D1q3vR1VqXgn6nsItQAlwm3NuVfy/S5xz481sDzAFWAK8SuzTh3D+tyxZp6qqmuHnXMSmN95KrDtj6GAWvvw0Bx3UI8LK8lfQ9xRuAG5oY/sCYEHQoiS/NAXDksW/Z9DA/gAMHfJVFr78NOeNuFgfIYZMNy9JRqiqquacb36HjZveTKwbcvopPPO7X0ZYVX5SKEjGaBkM27fv4Kd3zYy4qvyjUJCM0vRSYsXKdZw74mIqKzdFXVLe0fcpSMaprt7OsOEXRl1G3tKVgoh4FAoi4tHLhzzQt2xw1CVIJ0U5Z7pSEBGPQkFEPAoFEfEoFETEo1AQEY9CQUQ8CgUR8SgURMSjUBARj0JBRDwKBRHxKBRExJOutnFTgKuAT+KrJpiZJVeqiIQh5W3j4gYAl5mZvjZHJMsEDYXniHWGarK3xfaBwM3OuT7Ay2Z2d8DziEjIChobGwP/crxt3HxgdvMuUc65qcCDxLpFzQMeNrOX2jpWY8O+xsZ9ewLX0hkFhcU07su9Nudhj+uNt8J7RVjer4zNf3k3lHMNOMmFch4If866FHUtaG+flLeNc84VAD8zs+r48svAyUDboZCjbePCFPa4MqElXjrs+mBtKOeBcOeso/1T09E27mDgHedcObFW9GcTe1NSRLJA0CuF5m3jmlrSzwZ6mNmjzrlbgJXAbqDCzBYmX6qIhCFdbeOeAJ4IWpSIREc3L4mIR6EgIh6Fgoh4FAoi4lEoiIhHoSAiHoWCiHgUCiLiUSiIiEehICIehYKIeBQKIuJRKIiIR6EgIh6Fgoh4FAoi4lEoiIhHoSAiHoWCiHiCfptzIbEvanXAPuAKM9vSbPv5wO3EmsTMMbPZKahVREIQ9ErhfAAzG0LsyX9/04Z4n8mZwHDgTGB8vFOUiGSBQKFgZn8AxscXjwE+bra5HHjPzLaZWT2wDjgjqSpFJDSBO0SZ2V7n3G+B0cC3m206GKhutrwDOCToeUQkXIFDAcDMxjnnbgI2OOdOMLMaYv0jezbbrSdQ1d6xCgqLOtzWKlkFhcWhnStMYY9rQ+Wi0M5V3q8stPOF+TfMxMdi0DcaLwW+GO8mXQs0EHvDEWAzUOacKwV2Al8DZrR3TPWSTJ56SaZGvveSDPpG4wvAyc65NcASYBIwxjk33sz2AFPi618l9ulD7j0DRXJU0LZxNcCFbWxfACwIWpSIREc3L4mIR6EgIh6Fgoh4FAoi4lEoiIhHoSAiHoWCiHgUCiLiUSiIiEehICIehYKIeBQKIuJRKIiIR6EgIh6Fgoh4FAoi4lEoiIhHoSAiHoWCiHgUCiLiSVcvySnAVcAn8VUTzMySrFVEQhC0GUyil6Rz7ixivSRHNts+ALjMzDYlV56IhC0dvSQBBgI3O+fWOeduTqI+EQlZQWNjY+Bfbt5L0syWNls/FXiQWAu5ecDDZvZSO4f7BNgauBgRac8xwOHt7ZRUKADE28xvAE4wsxrnXAFwsJlVx7dfBxxqZj9N6kQiEop09JI8GHjHOVcO1ABnA3NSUKuIhCDQlYJzrgfwa6APUATcA/QADjKzR+OhMRHYDVSY2dTUlSwi6ZT0ywcRyS26eUlEPAoFEfEEvXkpKznnugAPAf2Jvd9xtZm9F21VqeOcGwxMM7Ozoq4lFZxzRcTepD4WOBC408zmR1pUirR3V3CU8u1KYRTQ1cxOA34E3BdxPSnjnPsh8BjQNepaUuh7wL/M7AzgXOAXEdeTSom7goHbid0VnBHyLRSGAosBzKwSGBRtOSm1BRgTdREp9hxwW7PlvVEVkmoduCs4MvkWCgcD1c2W9znncuIllJk9D+yJuo5UMrOdZrbDOdcTmAvcGnVNqWRme+N3BT9AbHwZId9CYTvQs9lyFzPLmf/75CLn3NHASuAJM3s66npSzczGAccDs+P3/0Qu30JhPXAegHPuVODtaMuRtjjnegNLgZvMLKfuinXOXdrsHwu2vCs4Ujlx6dwJ84BhzrlXgALgiojrkbbdApQAtznnmt5bONfM6iKsKVVeAH7tnFtD7K7gSWa2K+KaAN3RKCIt5NvLBxFph0JBRDwKBRHxKBRExKNQEBGPQkFEPAoFEfEoFETE8//AoHmczalKEgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# OPTIMAL POLICY VIA VALUE ITERATION\n",
    "\n",
    "Policy_VALIT = np.array([np.argmax(pol_opt2[row,:]) for row in range(grid.state_size)])\n",
    "\n",
    "\n",
    "grid.draw_deterministic_policy(Policy_VALIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterating over different p value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld(object):\n",
    "    def __init__(self):\n",
    "        \n",
    "        ### Attributes defining the Gridworld #######\n",
    "        # Shape of the gridworld\n",
    "        self.shape = (4,4)\n",
    "        \n",
    "        # Locations of the obstacles\n",
    "        self.obstacle_locs = [(1,2),(2,0),(3,0),(3,1),(3,3)]\n",
    "        \n",
    "        # Locations for the absorbing states\n",
    "        self.absorbing_locs = [(0,1),(3,2)]\n",
    "        \n",
    "        # Rewards for each of the absorbing states \n",
    "        self.special_rewards = [10, -100] #corresponds to each of the absorbing_locs\n",
    "        \n",
    "        # Reward for all the other states\n",
    "        self.default_reward = -1\n",
    "        \n",
    "        # Starting location\n",
    "        self.starting_loc = (0,0)\n",
    "        \n",
    "        # Action names\n",
    "        self.action_names = ['N','E','S','W']\n",
    "        \n",
    "        # Number of actions\n",
    "        self.action_size = len(self.action_names)\n",
    "        \n",
    "        \n",
    "        # Randomizing action results: [1 0 0 0] to no Noise in the action results.\n",
    "        self.action_randomizing_array = [1, 0, 0, 0]\n",
    "        \n",
    "        ############################################\n",
    "    \n",
    "    \n",
    "    \n",
    "        #### Internal State  ####\n",
    "        \n",
    "    \n",
    "        # Get attributes defining the world\n",
    "        state_size, T, R, pi, absorbing, locs = self.build_grid_world()\n",
    "        \n",
    "        # Number of valid states in the gridworld (there are 11 of them)\n",
    "        self.state_size = state_size\n",
    "        \n",
    "        # Transition operator (3D tensor)\n",
    "        self.T = T\n",
    "        \n",
    "        # Reward function (3D tensor)\n",
    "        self.R = R\n",
    "        \n",
    "        \n",
    "        # Probability function for stochasticity (2D tensor)\n",
    "        self.pi = pi\n",
    "        \n",
    "        # Absorbing states\n",
    "        self.absorbing = absorbing\n",
    "        \n",
    "        # The locations of the valid states\n",
    "        self.locs = locs\n",
    "        \n",
    "        # Number of the starting state\n",
    "        self.starting_state = self.loc_to_state(self.starting_loc, locs);\n",
    "        \n",
    "        # Locating the initial state\n",
    "        self.initial = np.zeros((1,len(locs)));\n",
    "        self.initial[0,self.starting_state] = 1\n",
    "        \n",
    "        \n",
    "        # Placing the walls on a bitmap\n",
    "        self.walls = np.zeros(self.shape);\n",
    "        for ob in self.obstacle_locs:\n",
    "            self.walls[ob]=1\n",
    "            \n",
    "        # Placing the absorbers on a grid for illustration\n",
    "        self.absorbers = np.zeros(self.shape)\n",
    "        for ab in self.absorbing_locs:\n",
    "            self.absorbers[ab] = -1\n",
    "        \n",
    "        # Placing the rewarders on a grid for illustration\n",
    "        self.rewarders = np.zeros(self.shape)\n",
    "        for i, rew in enumerate(self.absorbing_locs):\n",
    "            self.rewarders[rew] = self.special_rewards[i]\n",
    "        \n",
    "        #Illustrating the grid world\n",
    "        self.paint_maps()\n",
    "        ################################\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ####### Getters ###########\n",
    "    \n",
    "    def get_transition_matrix(self):\n",
    "        return self.T\n",
    "    \n",
    "    def get_reward_matrix(self):\n",
    "        return self.R\n",
    "    \n",
    "    def get_probability_matrix(self):\n",
    "        return self.pi\n",
    "    \n",
    "    \n",
    "    ########################\n",
    "    \n",
    "    ####### Methods #########\n",
    "    \n",
    "    \n",
    "    def value_iteration(self, discount = 0.3, threshold = 0.0001):\n",
    "        V = np.zeros(self.state_size)\n",
    "        \n",
    "        T = self.get_transition_matrix()\n",
    "        R = self.get_reward_matrix()\n",
    "        \n",
    "        ####\n",
    "        pi = self.get_probability_matrix()\n",
    "        \n",
    "        epochs = 0\n",
    "        while True:\n",
    "            epochs+=1\n",
    "            delta = 0\n",
    "\n",
    "            for state_idx in range(self.state_size):\n",
    "                if(self.absorbing[0,state_idx]):\n",
    "                    continue \n",
    "                    \n",
    "                v = V[state_idx]\n",
    "\n",
    "                Q = np.zeros(4)\n",
    "                for state_idx_prime in range(self.state_size):\n",
    "                    Q +=  pi[state_idx,:]*T[state_idx_prime,state_idx,:] * (R[state_idx_prime,state_idx, :] + discount * V[state_idx_prime])\n",
    "                    ########### added P\n",
    "                    \n",
    "                V[state_idx]= np.max(Q)\n",
    "                delta = max(delta,np.abs(v - V[state_idx]))\n",
    "                \n",
    "            if(delta<threshold):\n",
    "                optimal_policy = np.zeros((self.state_size, self.action_size))\n",
    "                for state_idx in range(self.state_size):\n",
    "                    Q = np.zeros(4)\n",
    "                    for state_idx_prime in range(self.state_size):\n",
    "                        Q += pi[state_idx,:]*T[state_idx_prime,state_idx,:] * (R[state_idx_prime,state_idx, :] + discount * V[state_idx_prime])\n",
    "                    \n",
    "                    optimal_policy[state_idx, np.argmax(Q)]=1\n",
    "\n",
    "                \n",
    "                return optimal_policy,epochs\n",
    "\n",
    "\n",
    "    \n",
    "    def policy_iteration(self, discount=0.3, threshold = 0.0001):\n",
    "        policy= np.zeros((self.state_size, self.action_size))\n",
    "        policy[:,0] = 1\n",
    "        \n",
    "        T = self.get_transition_matrix()\n",
    "        R = self.get_reward_matrix()\n",
    "        \n",
    "        ########\n",
    "        pi = self.get_probability_matrix()\n",
    "        \n",
    "        epochs =0\n",
    "        while True: \n",
    "            V, epochs_eval = self.policy_evaluation(policy, threshold, discount)\n",
    "            \n",
    "            epochs+=epochs_eval\n",
    "            #Policy iteration\n",
    "            policy_stable = True\n",
    "            \n",
    "            for state_idx in range(policy.shape[0]):\n",
    "                if(self.absorbing[0,state_idx]):\n",
    "                    continue \n",
    "                    \n",
    "                old_action = np.argmax(policy[state_idx,:])\n",
    "                \n",
    "                Q = np.zeros(4)\n",
    "                for state_idx_prime in range(policy.shape[0]):\n",
    "                    Q += pi[state_idx,:] * T[state_idx_prime,state_idx,:] * (R[state_idx_prime,state_idx, :] + discount * V[state_idx_prime])\n",
    "                   \n",
    "                ####### Added P, 2D vector consisting of pi(s,a)\n",
    "                \n",
    "                new_policy = np.zeros(4)\n",
    "                new_policy[np.argmax(Q)]=1\n",
    "                policy[state_idx] = new_policy\n",
    "                \n",
    "                if(old_action !=np.argmax(policy[state_idx])):\n",
    "                    policy_stable = False\n",
    "            \n",
    "            if(policy_stable):\n",
    "                return V, policy,epochs\n",
    "                \n",
    "                \n",
    "                \n",
    "        \n",
    "    \n",
    "    def policy_evaluation(self, policy, threshold, discount):\n",
    "        \n",
    "        # Make sure delta is bigger than the threshold to start with\n",
    "        delta= 2*threshold\n",
    "        \n",
    "        #Get the reward and transition matrices\n",
    "        R = self.get_reward_matrix()\n",
    "        T = self.get_transition_matrix()\n",
    "        \n",
    "        \n",
    "        ###\n",
    "        pi = self.get_probability_matrix()\n",
    "\n",
    "        \n",
    "        # The value is initialised at 0\n",
    "        V = np.zeros(policy.shape[0])\n",
    "        # Make a deep copy of the value array to hold the update during the evaluation\n",
    "        Vnew = np.copy(V)\n",
    "        \n",
    "        epoch = 0\n",
    "        # While the Value has not yet converged do:\n",
    "        while delta>threshold:\n",
    "            epoch += 1\n",
    "            for state_idx in range(policy.shape[0]):\n",
    "                # If it is one of the absorbing states, ignore\n",
    "                if(self.absorbing[0,state_idx]):\n",
    "                    continue   \n",
    "                \n",
    "                # Accumulator variable for the Value of a state\n",
    "                tmpV = 0\n",
    "                for action_idx in range(policy.shape[1]):\n",
    "                    # Accumulator variable for the State-Action Value\n",
    "                    tmpQ = 0\n",
    "                    for state_idx_prime in range(policy.shape[0]):\n",
    "                        tmpQ = tmpQ + pi[state_idx,action_idx]*T[state_idx_prime,state_idx,action_idx] * (R[state_idx_prime,state_idx, action_idx] + discount * V[state_idx_prime])\n",
    "                    \n",
    "                    tmpV += policy[state_idx,action_idx] * tmpQ\n",
    "                    \n",
    "                # Update the value of the state\n",
    "                Vnew[state_idx] = tmpV\n",
    "            \n",
    "            # After updating the values of all states, update the delta\n",
    "            delta =  max(abs(Vnew-V))\n",
    "            # and save the new value into the old\n",
    "            V=np.copy(Vnew)\n",
    "            \n",
    "        return V, epoch\n",
    "    \n",
    "    def draw_deterministic_policy(self, Policy):\n",
    "        # Draw a deterministic policy\n",
    "        # The policy needs to be a np array of 11 values between 0 and 3 with\n",
    "        # 0 -> N, 1->E, 2->S, 3->W\n",
    "        plt.figure()\n",
    "        plt.imshow(self.walls+ self.rewarders + self.absorbers)\n",
    "        plt.imshow(self.walls)\n",
    "        \n",
    "        #plt.hold('on')\n",
    "        for state, action in enumerate(Policy):\n",
    "            if(self.absorbing[0,state]):\n",
    "                continue\n",
    "            arrows = [r\"$\\uparrow$\",r\"$\\rightarrow$\", r\"$\\downarrow$\", r\"$\\leftarrow$\"]\n",
    "            action_arrow = arrows[action]\n",
    "            location = self.locs[state]\n",
    "            plt.text(location[1], location[0], action_arrow, ha='center', va='center', color='w', size='40')\n",
    "    \n",
    "        plt.show()\n",
    "    ##########################\n",
    "    \n",
    "    \n",
    "    ########### Internal Helper Functions #####################\n",
    "    def paint_maps(self):\n",
    "        plt.figure()\n",
    "        plt.subplot(1,3,1)\n",
    "        plt.imshow(self.walls)\n",
    "        plt.subplot(1,3,2)\n",
    "        plt.imshow(self.absorbers)\n",
    "        plt.subplot(1,3,3)\n",
    "        plt.imshow(self.rewarders) \n",
    "        plt.show()\n",
    "        \n",
    "    def build_grid_world(self):\n",
    "        # Get the locations of all the valid states, the neighbours of each state (by state number),\n",
    "        # and the absorbing states (array of 0's with ones in the absorbing states)\n",
    "        locations, neighbours, absorbing_states = self.get_topology()\n",
    "        \n",
    "        # Get the number of states\n",
    "        S = len(locations)\n",
    "        \n",
    "        # Initialise the transition matrix\n",
    "        T = np.zeros((S,S,4))\n",
    "        \n",
    "        ## Initialise the probability matrix\n",
    "        pi = np.zeros((S,4))  ### add\n",
    "        \n",
    "        ##probability matrix is based on action-state pairs\n",
    "        ##if the action and outcome is the samethen p=0.3\n",
    "        \n",
    "        ###change here for diff p!\n",
    "        p_constant = 0.8\n",
    "        \n",
    "        for action in range(4):\n",
    "            for effect in range(4):\n",
    "                \n",
    "                # Randomize the outcome of taking an action. is it random tho......\n",
    "                \n",
    "                # outcome = \n",
    "                \n",
    "                outcome = (action+effect+1) % 4\n",
    "                if outcome == 0:\n",
    "                    outcome = 3\n",
    "                else:\n",
    "                    outcome -= 1\n",
    "    \n",
    "                # Fill the probability and transition matrix\n",
    "                #prob = self.action_randomizing_array[effect] \n",
    "                \n",
    "                for prior_state in range(S):\n",
    "                    post_state = neighbours[prior_state, outcome]\n",
    "                    post_state = int(post_state)\n",
    "                    \n",
    "                    if action == outcome: \n",
    "                        pi[prior_state,action] = p_constant\n",
    "                        prob = p_constant\n",
    "                        \n",
    "                    else:\n",
    "                        pi[prior_state,action] = (1-p_constant)/3\n",
    "                        prob = (1-p_constant)/3\n",
    "                        \n",
    "                    T[post_state,prior_state,action] = T[post_state,prior_state,action]+prob\n",
    "                \n",
    "    \n",
    "        # Build the reward matrix\n",
    "        R = self.default_reward*np.ones((S,S,4))\n",
    "        for i, sr in enumerate(self.special_rewards):\n",
    "            post_state = self.loc_to_state(self.absorbing_locs[i],locations)\n",
    "            R[post_state,:,:]= sr\n",
    "        \n",
    "        return S, T, R, pi, absorbing_states,locations\n",
    "    \n",
    "    def get_topology(self):\n",
    "        height = self.shape[0]\n",
    "        width = self.shape[1]\n",
    "        \n",
    "        index = 1 \n",
    "        locs = []\n",
    "        neighbour_locs = []\n",
    "        \n",
    "        for i in range(height):\n",
    "            for j in range(width):\n",
    "                # Get the locaiton of each state\n",
    "                loc = (i,j)\n",
    "                \n",
    "                #And append it to the valid state locations if it is a valid state (ie not absorbing)\n",
    "                if(self.is_location(loc)):\n",
    "                    locs.append(loc)\n",
    "                    \n",
    "                    # Get an array with the neighbours of each state, in terms of locations\n",
    "                    local_neighbours = [self.get_neighbour(loc,direction) for direction in ['nr','ea','so', 'we']]\n",
    "                    neighbour_locs.append(local_neighbours)\n",
    "                \n",
    "        # translate neighbour lists from locations to states\n",
    "        num_states = len(locs)\n",
    "        state_neighbours = np.zeros((num_states,4))\n",
    "        \n",
    "        for state in range(num_states):\n",
    "            for direction in range(4):\n",
    "                # Find neighbour location\n",
    "                nloc = neighbour_locs[state][direction]\n",
    "                \n",
    "                # Turn location into a state number\n",
    "                nstate = self.loc_to_state(nloc,locs)\n",
    "      \n",
    "                # Insert into neighbour matrix\n",
    "                state_neighbours[state,direction] = nstate;\n",
    "                \n",
    "    \n",
    "        # Translate absorbing locations into absorbing state indices\n",
    "        absorbing = np.zeros((1,num_states))\n",
    "        for a in self.absorbing_locs:\n",
    "            absorbing_state = self.loc_to_state(a,locs)\n",
    "            absorbing[0,absorbing_state] =1\n",
    "        \n",
    "        return locs, state_neighbours, absorbing \n",
    "\n",
    "    def loc_to_state(self,loc,locs):\n",
    "        #takes list of locations and gives index corresponding to input loc\n",
    "        return locs.index(tuple(loc))\n",
    "        #return locs.index(loc)\n",
    "\n",
    "\n",
    "    def is_location(self, loc):\n",
    "        # It is a valid location if it is in grid and not obstacle\n",
    "        if(loc[0]<0 or loc[1]<0 or loc[0]>self.shape[0]-1 or loc[1]>self.shape[1]-1):\n",
    "            return False\n",
    "        elif(loc in self.obstacle_locs):\n",
    "            return False\n",
    "        else:\n",
    "             return True\n",
    "            \n",
    "    def get_neighbour(self,loc,direction):\n",
    "        #Find the valid neighbours (ie that are in the grif and not obstacle)\n",
    "        i = loc[0]\n",
    "        j = loc[1]\n",
    "        \n",
    "        nr = (i-1,j)\n",
    "        ea = (i,j+1)\n",
    "        so = (i+1,j)\n",
    "        we = (i,j-1)\n",
    "        \n",
    "        # If the neighbour is a valid location, accept it, otherwise, stay put\n",
    "        if(direction == 'nr' and self.is_location(nr)):\n",
    "            return nr\n",
    "        elif(direction == 'ea' and self.is_location(ea)):\n",
    "            return ea\n",
    "        elif(direction == 'so' and self.is_location(so)):\n",
    "            return so\n",
    "        elif(direction == 'we' and self.is_location(we)):\n",
    "            return we\n",
    "        else:\n",
    "            #default is to return to the same location\n",
    "            return loc\n",
    "        \n",
    "###########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Policy is : [[0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]]\n",
      "The value of that policy is :[ 0.11751083  0.          0.11750625 -0.06709    -0.06615792  0.11654208\n",
      " -0.06806333 -0.07546667 -1.72605417 -0.07639875  0.        ]\n",
      "It took 3 epochs\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The value of the optimal policy using policy iteration is [ 0.52131312  0.          0.52131207 -0.05857237 -0.0577857   0.5205381\n",
      " -0.06787633 -0.0591737  -0.50838279 -0.06861352  0.        ]:\n",
      "The optimal policy using policy iteration is [[0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "The number of epochs for convergence are 12\n",
      "The optimal policy using value iteration is [[0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "The number of epocs for convergence is 3\n"
     ]
    }
   ],
   "source": [
    "grid = GridWorld()\n",
    "\n",
    "### Question 1 : Change the policy here:\n",
    "Policy= np.zeros((grid.state_size, grid.action_size))\n",
    "Policy = Policy + 0.25\n",
    "print(\"The Policy is : {}\".format(Policy))\n",
    "\n",
    "val, epochs = grid.policy_evaluation(Policy,0.001,0.3)\n",
    "print(\"The value of that policy is :{}\".format(val))\n",
    "print(\"It took {} epochs\".format(epochs))\n",
    "\n",
    "\n",
    "gamma_range = [0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]\n",
    "epochs_needed = []\n",
    "for gamma in gamma_range:\n",
    "    val, epochs = grid.policy_evaluation(Policy,0.001,gamma)\n",
    "    epochs_needed.append(epochs)\n",
    "    \n",
    "plt.figure()\n",
    "plt.plot(gamma_range,epochs_needed)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "V_opt, pol_opt, epochs = grid.policy_iteration()\n",
    "print(\"The value of the optimal policy using policy iteration is {}:\".format(V_opt))\n",
    "print(\"The optimal policy using policy iteration is {}\".format(pol_opt))\n",
    "print(\"The number of epochs for convergence are {}\".format(epochs))\n",
    "\n",
    "\n",
    "pol_opt2, epochs = grid.value_iteration()\n",
    "print(\"The optimal policy using value iteration is {}\".format(pol_opt2))\n",
    "print(\"The number of epocs for convergence is {}\".format(epochs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# OPTIMAL POLICY VIA VALUE ITERATION\n",
    "\n",
    "Policy_VALIT = np.array([np.argmax(pol_opt2[row,:]) for row in range(grid.state_size)])\n",
    "\n",
    "\n",
    "grid.draw_deterministic_policy(Policy_VALIT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## iterating over different gamma vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld(object):\n",
    "    def __init__(self):\n",
    "        \n",
    "        ### Attributes defining the Gridworld #######\n",
    "        # Shape of the gridworld\n",
    "        self.shape = (4,4)\n",
    "        \n",
    "        # Locations of the obstacles\n",
    "        self.obstacle_locs = [(1,2),(2,0),(3,0),(3,1),(3,3)]\n",
    "        \n",
    "        # Locations for the absorbing states\n",
    "        self.absorbing_locs = [(0,1),(3,2)]\n",
    "        \n",
    "        # Rewards for each of the absorbing states \n",
    "        self.special_rewards = [10, -100] #corresponds to each of the absorbing_locs\n",
    "        \n",
    "        # Reward for all the other states\n",
    "        self.default_reward = -1\n",
    "        \n",
    "        # Starting location\n",
    "        self.starting_loc = (0,0)\n",
    "        \n",
    "        # Action names\n",
    "        self.action_names = ['N','E','S','W']\n",
    "        \n",
    "        # Number of actions\n",
    "        self.action_size = len(self.action_names)\n",
    "        \n",
    "        \n",
    "        # Randomizing action results: [1 0 0 0] to no Noise in the action results.\n",
    "        self.action_randomizing_array = [1, 0, 0, 0]\n",
    "        \n",
    "        ############################################\n",
    "    \n",
    "    \n",
    "    \n",
    "        #### Internal State  ####\n",
    "        \n",
    "    \n",
    "        # Get attributes defining the world\n",
    "        state_size, T, R, pi, absorbing, locs = self.build_grid_world()\n",
    "        \n",
    "        # Number of valid states in the gridworld (there are 11 of them)\n",
    "        self.state_size = state_size\n",
    "        \n",
    "        # Transition operator (3D tensor)\n",
    "        self.T = T\n",
    "        \n",
    "        # Reward function (3D tensor)\n",
    "        self.R = R\n",
    "        \n",
    "        \n",
    "        # Probability function for stochasticity (2D tensor)\n",
    "        self.pi = pi\n",
    "        \n",
    "        # Absorbing states\n",
    "        self.absorbing = absorbing\n",
    "        \n",
    "        # The locations of the valid states\n",
    "        self.locs = locs\n",
    "        \n",
    "        # Number of the starting state\n",
    "        self.starting_state = self.loc_to_state(self.starting_loc, locs);\n",
    "        \n",
    "        # Locating the initial state\n",
    "        self.initial = np.zeros((1,len(locs)));\n",
    "        self.initial[0,self.starting_state] = 1\n",
    "        \n",
    "        \n",
    "        # Placing the walls on a bitmap\n",
    "        self.walls = np.zeros(self.shape);\n",
    "        for ob in self.obstacle_locs:\n",
    "            self.walls[ob]=1\n",
    "            \n",
    "        # Placing the absorbers on a grid for illustration\n",
    "        self.absorbers = np.zeros(self.shape)\n",
    "        for ab in self.absorbing_locs:\n",
    "            self.absorbers[ab] = -1\n",
    "        \n",
    "        # Placing the rewarders on a grid for illustration\n",
    "        self.rewarders = np.zeros(self.shape)\n",
    "        for i, rew in enumerate(self.absorbing_locs):\n",
    "            self.rewarders[rew] = self.special_rewards[i]\n",
    "        \n",
    "        #Illustrating the grid world\n",
    "        self.paint_maps()\n",
    "        ################################\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ####### Getters ###########\n",
    "    \n",
    "    def get_transition_matrix(self):\n",
    "        return self.T\n",
    "    \n",
    "    def get_reward_matrix(self):\n",
    "        return self.R\n",
    "    \n",
    "    def get_probability_matrix(self):\n",
    "        return self.pi\n",
    "    \n",
    "    \n",
    "    ########################\n",
    "    \n",
    "    ####### Methods #########\n",
    "    \n",
    "    ##change gamma\n",
    "    def value_iteration(self, discount = 0.3, threshold = 0.0001):\n",
    "        V = np.zeros(self.state_size)\n",
    "        \n",
    "        T = self.get_transition_matrix()\n",
    "        R = self.get_reward_matrix()\n",
    "        \n",
    "        ####\n",
    "        pi = self.get_probability_matrix()\n",
    "        \n",
    "        epochs = 0\n",
    "        while True:\n",
    "            epochs+=1\n",
    "            delta = 0\n",
    "\n",
    "            for state_idx in range(self.state_size):\n",
    "                if(self.absorbing[0,state_idx]):\n",
    "                    continue \n",
    "                    \n",
    "                v = V[state_idx]\n",
    "\n",
    "                Q = np.zeros(4)\n",
    "                for state_idx_prime in range(self.state_size):\n",
    "                    Q +=  pi[state_idx,:]*T[state_idx_prime,state_idx,:] * (R[state_idx_prime,state_idx, :] + discount * V[state_idx_prime])\n",
    "                    ########### added P\n",
    "                    \n",
    "                V[state_idx]= np.max(Q)\n",
    "                delta = max(delta,np.abs(v - V[state_idx]))\n",
    "                \n",
    "            if(delta<threshold):\n",
    "                optimal_policy = np.zeros((self.state_size, self.action_size))\n",
    "                for state_idx in range(self.state_size):\n",
    "                    Q = np.zeros(4)\n",
    "                    for state_idx_prime in range(self.state_size):\n",
    "                        Q += pi[state_idx,:]*T[state_idx_prime,state_idx,:] * (R[state_idx_prime,state_idx, :] + discount * V[state_idx_prime])\n",
    "                    \n",
    "                    optimal_policy[state_idx, np.argmax(Q)]=1\n",
    "\n",
    "                \n",
    "                return optimal_policy,epochs\n",
    "\n",
    "    ##change gamma here\n",
    "    \n",
    "    def policy_iteration(self, discount=0.3, threshold = 0.0001):\n",
    "        policy= np.zeros((self.state_size, self.action_size))\n",
    "        policy[:,0] = 1\n",
    "        \n",
    "        T = self.get_transition_matrix()\n",
    "        R = self.get_reward_matrix()\n",
    "        \n",
    "        ########\n",
    "        pi = self.get_probability_matrix()\n",
    "        \n",
    "        epochs =0\n",
    "        while True: \n",
    "            V, epochs_eval = self.policy_evaluation(policy, threshold, discount)\n",
    "            \n",
    "            epochs+=epochs_eval\n",
    "            #Policy iteration\n",
    "            policy_stable = True\n",
    "            \n",
    "            for state_idx in range(policy.shape[0]):\n",
    "                if(self.absorbing[0,state_idx]):\n",
    "                    continue \n",
    "                    \n",
    "                old_action = np.argmax(policy[state_idx,:])\n",
    "                \n",
    "                Q = np.zeros(4)\n",
    "                for state_idx_prime in range(policy.shape[0]):\n",
    "                    Q += pi[state_idx,:] * T[state_idx_prime,state_idx,:] * (R[state_idx_prime,state_idx, :] + discount * V[state_idx_prime])\n",
    "                   \n",
    "                ####### Added P, 2D vector consisting of pi(s,a)\n",
    "                \n",
    "                new_policy = np.zeros(4)\n",
    "                new_policy[np.argmax(Q)]=1\n",
    "                policy[state_idx] = new_policy\n",
    "                \n",
    "                if(old_action !=np.argmax(policy[state_idx])):\n",
    "                    policy_stable = False\n",
    "            \n",
    "            if(policy_stable):\n",
    "                return V, policy,epochs\n",
    "                \n",
    "                \n",
    "                \n",
    "        \n",
    "    \n",
    "    def policy_evaluation(self, policy, threshold, discount):\n",
    "        \n",
    "        # Make sure delta is bigger than the threshold to start with\n",
    "        delta= 2*threshold\n",
    "        \n",
    "        #Get the reward and transition matrices\n",
    "        R = self.get_reward_matrix()\n",
    "        T = self.get_transition_matrix()\n",
    "        \n",
    "        \n",
    "        ###\n",
    "        pi = self.get_probability_matrix()\n",
    "\n",
    "        \n",
    "        # The value is initialised at 0\n",
    "        V = np.zeros(policy.shape[0])\n",
    "        # Make a deep copy of the value array to hold the update during the evaluation\n",
    "        Vnew = np.copy(V)\n",
    "        \n",
    "        epoch = 0\n",
    "        # While the Value has not yet converged do:\n",
    "        while delta>threshold:\n",
    "            epoch += 1\n",
    "            for state_idx in range(policy.shape[0]):\n",
    "                # If it is one of the absorbing states, ignore\n",
    "                if(self.absorbing[0,state_idx]):\n",
    "                    continue   \n",
    "                \n",
    "                # Accumulator variable for the Value of a state\n",
    "                tmpV = 0\n",
    "                for action_idx in range(policy.shape[1]):\n",
    "                    # Accumulator variable for the State-Action Value\n",
    "                    tmpQ = 0\n",
    "                    for state_idx_prime in range(policy.shape[0]):\n",
    "                        tmpQ = tmpQ + pi[state_idx,action_idx]*T[state_idx_prime,state_idx,action_idx] * (R[state_idx_prime,state_idx, action_idx] + discount * V[state_idx_prime])\n",
    "                    \n",
    "                    tmpV += policy[state_idx,action_idx] * tmpQ\n",
    "                    \n",
    "                # Update the value of the state\n",
    "                Vnew[state_idx] = tmpV\n",
    "            \n",
    "            # After updating the values of all states, update the delta\n",
    "            delta =  max(abs(Vnew-V))\n",
    "            # and save the new value into the old\n",
    "            V=np.copy(Vnew)\n",
    "            \n",
    "        return V, epoch\n",
    "    \n",
    "    def draw_deterministic_policy(self, Policy):\n",
    "        # Draw a deterministic policy\n",
    "        # The policy needs to be a np array of 11 values between 0 and 3 with\n",
    "        # 0 -> N, 1->E, 2->S, 3->W\n",
    "        plt.figure()\n",
    "        plt.imshow(self.walls+ self.rewarders + self.absorbers)\n",
    "        plt.imshow(self.walls)\n",
    "        \n",
    "        #plt.hold('on')\n",
    "        for state, action in enumerate(Policy):\n",
    "            if(self.absorbing[0,state]):\n",
    "                continue\n",
    "            arrows = [r\"$\\uparrow$\",r\"$\\rightarrow$\", r\"$\\downarrow$\", r\"$\\leftarrow$\"]\n",
    "            action_arrow = arrows[action]\n",
    "            location = self.locs[state]\n",
    "            plt.text(location[1], location[0], action_arrow, ha='center', va='center', color='w', size='40')\n",
    "    \n",
    "        plt.show()\n",
    "    ##########################\n",
    "    \n",
    "    \n",
    "    ########### Internal Helper Functions #####################\n",
    "    def paint_maps(self):\n",
    "        plt.figure()\n",
    "        plt.subplot(1,3,1)\n",
    "        plt.imshow(self.walls)\n",
    "        plt.subplot(1,3,2)\n",
    "        plt.imshow(self.absorbers)\n",
    "        plt.subplot(1,3,3)\n",
    "        plt.imshow(self.rewarders) \n",
    "        plt.show()\n",
    "        \n",
    "    def build_grid_world(self):\n",
    "        # Get the locations of all the valid states, the neighbours of each state (by state number),\n",
    "        # and the absorbing states (array of 0's with ones in the absorbing states)\n",
    "        locations, neighbours, absorbing_states = self.get_topology()\n",
    "        \n",
    "        # Get the number of states\n",
    "        S = len(locations)\n",
    "        \n",
    "        # Initialise the transition matrix\n",
    "        T = np.zeros((S,S,4))\n",
    "        \n",
    "        ## Initialise the probability matrix\n",
    "        pi = np.zeros((S,4))  ### add\n",
    "        \n",
    "        ##probability matrix is based on action-state pairs\n",
    "        ##if the action and outcome is the samethen p=0.3\n",
    "        \n",
    "        ###change here for diff p\n",
    "        p_constant = 0.3\n",
    "        \n",
    "        for action in range(4):\n",
    "            for effect in range(4):\n",
    "                \n",
    "                # Randomize the outcome of taking an action. is it random tho......\n",
    "                \n",
    "                # outcome = \n",
    "                \n",
    "                outcome = (action+effect+1) % 4\n",
    "                if outcome == 0:\n",
    "                    outcome = 3\n",
    "                else:\n",
    "                    outcome -= 1\n",
    "    \n",
    "                # Fill the probability and transition matrix\n",
    "                #prob = self.action_randomizing_array[effect] \n",
    "                \n",
    "                for prior_state in range(S):\n",
    "                    post_state = neighbours[prior_state, outcome]\n",
    "                    post_state = int(post_state)\n",
    "                    \n",
    "                    if action == outcome: \n",
    "                        pi[prior_state,action] = p_constant\n",
    "                        prob = p_constant\n",
    "                        \n",
    "                    else:\n",
    "                        pi[prior_state,action] = (1-p_constant)/3\n",
    "                        prob = (1-p_constant)/3\n",
    "                        \n",
    "                    T[post_state,prior_state,action] = T[post_state,prior_state,action]+prob\n",
    "                \n",
    "    \n",
    "        # Build the reward matrix\n",
    "        R = self.default_reward*np.ones((S,S,4))\n",
    "        for i, sr in enumerate(self.special_rewards):\n",
    "            post_state = self.loc_to_state(self.absorbing_locs[i],locations)\n",
    "            R[post_state,:,:]= sr\n",
    "        \n",
    "        return S, T, R, pi, absorbing_states,locations\n",
    "    \n",
    "    def get_topology(self):\n",
    "        height = self.shape[0]\n",
    "        width = self.shape[1]\n",
    "        \n",
    "        index = 1 \n",
    "        locs = []\n",
    "        neighbour_locs = []\n",
    "        \n",
    "        for i in range(height):\n",
    "            for j in range(width):\n",
    "                # Get the locaiton of each state\n",
    "                loc = (i,j)\n",
    "                \n",
    "                #And append it to the valid state locations if it is a valid state (ie not absorbing)\n",
    "                if(self.is_location(loc)):\n",
    "                    locs.append(loc)\n",
    "                    \n",
    "                    # Get an array with the neighbours of each state, in terms of locations\n",
    "                    local_neighbours = [self.get_neighbour(loc,direction) for direction in ['nr','ea','so', 'we']]\n",
    "                    neighbour_locs.append(local_neighbours)\n",
    "                \n",
    "        # translate neighbour lists from locations to states\n",
    "        num_states = len(locs)\n",
    "        state_neighbours = np.zeros((num_states,4))\n",
    "        \n",
    "        for state in range(num_states):\n",
    "            for direction in range(4):\n",
    "                # Find neighbour location\n",
    "                nloc = neighbour_locs[state][direction]\n",
    "                \n",
    "                # Turn location into a state number\n",
    "                nstate = self.loc_to_state(nloc,locs)\n",
    "      \n",
    "                # Insert into neighbour matrix\n",
    "                state_neighbours[state,direction] = nstate;\n",
    "                \n",
    "    \n",
    "        # Translate absorbing locations into absorbing state indices\n",
    "        absorbing = np.zeros((1,num_states))\n",
    "        for a in self.absorbing_locs:\n",
    "            absorbing_state = self.loc_to_state(a,locs)\n",
    "            absorbing[0,absorbing_state] =1\n",
    "        \n",
    "        return locs, state_neighbours, absorbing \n",
    "\n",
    "    def loc_to_state(self,loc,locs):\n",
    "        #takes list of locations and gives index corresponding to input loc\n",
    "        return locs.index(tuple(loc))\n",
    "        #return locs.index(loc)\n",
    "\n",
    "\n",
    "    def is_location(self, loc):\n",
    "        # It is a valid location if it is in grid and not obstacle\n",
    "        if(loc[0]<0 or loc[1]<0 or loc[0]>self.shape[0]-1 or loc[1]>self.shape[1]-1):\n",
    "            return False\n",
    "        elif(loc in self.obstacle_locs):\n",
    "            return False\n",
    "        else:\n",
    "             return True\n",
    "            \n",
    "    def get_neighbour(self,loc,direction):\n",
    "        #Find the valid neighbours (ie that are in the grif and not obstacle)\n",
    "        i = loc[0]\n",
    "        j = loc[1]\n",
    "        \n",
    "        nr = (i-1,j)\n",
    "        ea = (i,j+1)\n",
    "        so = (i+1,j)\n",
    "        we = (i,j-1)\n",
    "        \n",
    "        # If the neighbour is a valid location, accept it, otherwise, stay put\n",
    "        if(direction == 'nr' and self.is_location(nr)):\n",
    "            return nr\n",
    "        elif(direction == 'ea' and self.is_location(ea)):\n",
    "            return ea\n",
    "        elif(direction == 'so' and self.is_location(so)):\n",
    "            return so\n",
    "        elif(direction == 'we' and self.is_location(we)):\n",
    "            return we\n",
    "        else:\n",
    "            #default is to return to the same location\n",
    "            return loc\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Policy is : [[0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]]\n",
      "The value of that policy is :[ 0.41902897  0.          0.41881401 -0.23877617 -0.22684287  0.40541948\n",
      " -0.25259373 -0.34555087 -6.12785596 -0.35748073  0.        ]\n",
      "It took 4 epochs\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The value of the optimal policy using policy iteration is [ 0.55107186  0.          0.55084705 -0.23351059 -0.22019077  0.53650379\n",
      " -0.25208023 -0.326305   -5.72937394 -0.34342427  0.        ]:\n",
      "The optimal policy using policy iteration is [[0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "The number of epochs for convergence are 15\n",
      "The optimal policy using value iteration is [[0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "The number of epocs for convergence is 5\n"
     ]
    }
   ],
   "source": [
    "grid = GridWorld()\n",
    "\n",
    "### Question 1 : Change the policy here:\n",
    "Policy= np.zeros((grid.state_size, grid.action_size))\n",
    "Policy = Policy + 0.25\n",
    "print(\"The Policy is : {}\".format(Policy))\n",
    "\n",
    "\n",
    "##change gamma here\n",
    "val, epochs = grid.policy_evaluation(Policy,0.001,0.3)\n",
    "print(\"The value of that policy is :{}\".format(val))\n",
    "print(\"It took {} epochs\".format(epochs))\n",
    "\n",
    "\n",
    "gamma_range = [0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]\n",
    "epochs_needed = []\n",
    "for gamma in gamma_range:\n",
    "    val, epochs = grid.policy_evaluation(Policy,0.001,gamma)\n",
    "    epochs_needed.append(epochs)\n",
    "    \n",
    "plt.figure()\n",
    "plt.plot(gamma_range,epochs_needed)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "V_opt, pol_opt, epochs = grid.policy_iteration()\n",
    "print(\"The value of the optimal policy using policy iteration is {}:\".format(V_opt))\n",
    "print(\"The optimal policy using policy iteration is {}\".format(pol_opt))\n",
    "print(\"The number of epochs for convergence are {}\".format(epochs))\n",
    "\n",
    "\n",
    "pol_opt2, epochs = grid.value_iteration()\n",
    "print(\"The optimal policy using value iteration is {}\".format(pol_opt2))\n",
    "print(\"The number of epocs for convergence is {}\".format(epochs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# OPTIMAL POLICY VIA POLICY ITERATION\n",
    "Policy_POLIT = np.array([np.argmax(pol_opt[row,:]) for row in range(grid.state_size)])\n",
    "\n",
    "\n",
    "grid.draw_deterministic_policy(Policy_POLIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
